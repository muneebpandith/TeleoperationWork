{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cardinality=20\n",
    "output_cardinality=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0738c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection as sk\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81495b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning as pl\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "940b1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc51aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2683d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pl.seed_everything(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac4eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.filenames = os.listdir(DATA_PATH)\n",
    "        self.filenames = [filename for filename in self.filenames if filename.endswith(\".csv\")]\n",
    "        self.len_filenames = len(self.filenames)\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.data = []\n",
    "        for i in range(len(self.filenames)):\n",
    "            self.data.append(self.read_data(i))\n",
    "        return self.data\n",
    "    \n",
    "    def read_data(self,i):\n",
    "        return pd.read_csv(self.data_path+\"/\"+self.filenames[i])\n",
    "    \n",
    "\n",
    "    def process_data(self, scaler, input_cardinality=20, output_cardinality=10, cols=['ThetaXHG']):\n",
    "        self.datasamples = self.get_data()\n",
    "        X = []\n",
    "        y = []\n",
    "        X_scaled = []\n",
    "        y_scaled = []\n",
    "        for i, datasample in enumerate(self.datasamples):            \n",
    "            #for j in range(datasample[cols].shape[0]-output_cardinality):\n",
    "            #    print('Input' +str(j)+ ' to '+str(j+input_cardinality) + 'Output: '+str(j+input_cardinality)+ ' to '+str(j+input_cardinality+output_cardinality))\n",
    "            #Append to X (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto input_cardinality]]\n",
    "            #Append to Y (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto output_cardinality]]\n",
    "            #start of Y will be ahead of end of X by 1\n",
    "            \n",
    "            for j in range(datasample.shape[0]-output_cardinality-input_cardinality+1):\n",
    "                X.append(datasample[cols].iloc[j:j+input_cardinality].to_numpy())\n",
    "                y.append(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy())\n",
    "                X_scaled.append(scaler.transform(datasample[cols].iloc[j:j+input_cardinality].to_numpy()))\n",
    "                y_scaled.append(scaler.transform(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy()))\n",
    "        \n",
    "        #return self.datasamples, X, y\n",
    "        #print(np.array(X).shape,np.array(y).shape,np.array(X_scaled).shape, np.array(y_scaled).shape)\n",
    "        return self.datasamples, np.array(X), np.array(y), np.array(X_scaled), np.array(y_scaled)\n",
    "    \n",
    "    \n",
    "    def train_test_split(self, X, y, split={'train':0.8,'val':0.2, 'test':0.5}):\n",
    "        #print(X)\n",
    "        self.X_train, self.X_val_test, self.y_train, self.y_val_test = sk.train_test_split(X, y, test_size=split['val'] , random_state=43)\n",
    "        self.X_test, self.X_val, self.y_test, self.y_val = sk.train_test_split(self.X_val_test, self.y_val_test, test_size=split['test'] , random_state=43)\n",
    "        \n",
    "        return (self.X_train, self.y_train), (self.X_val, self.X_test), (self.X_test, self.y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getscaler(self,cols):\n",
    "        self.datasamples = self.get_data()       \n",
    "        for i, sample in enumerate(self.datasamples):\n",
    "            if i ==0:\n",
    "                features = pd.DataFrame(sample[cols])\n",
    "            else:\n",
    "                features = pd.DataFrame.append(features, sample[cols]) #sample[cols]\n",
    "   \n",
    "        #scaling\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        scaler = scaler.fit(features)\n",
    "        features_scaled = pd.DataFrame(scaler.transform(features), index = features.index, columns = cols)\n",
    "        return scaler\n",
    "    \n",
    "        \"\"\"        \n",
    "        #convert back to datasamples\n",
    "        #save to X\n",
    "        #save to X_scaled\n",
    "        \n",
    "        \n",
    "        #Split treain test and validation\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            self.datasamples = []\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        return features, features_scaled\n",
    "        \"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5167df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329ba98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASET(Dataset):\n",
    "    #convert to pytorch dataset\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, output = self.X[idx], self.y[idx]\n",
    "        return (torch.from_numpy(sequence.reshape(-1)), torch.from_numpy(output.reshape(-1)))\n",
    "        #return dict(sequence=torch.tensor(sequence.reshape(-1),dtype=torch.float64), label=torch.tensor(output.reshape(-1),dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4722168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASETLOADER():\n",
    "    def __init__(self, data, batchsize, bs_val):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batchsize = batchsize\n",
    "        self.train_dataset = THETADATASET(self.data.X_train, self.data.y_train)\n",
    "        self.val_dataset = THETADATASET(self.data.X_val, self.data.y_val)\n",
    "        self.test_dataset = THETADATASET(self.data.X_test, self.data.y_test)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size = self.batchsize, shuffle= False, num_workers=0)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size = bs_val, shuffle= False, num_workers=0)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size = 1, shuffle= False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c884b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847ddc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders={}\n",
    "#dataloaders['train'], dataloaders['val'] = data_loaded.train_dataloader, data_loaded.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor f, o in tqdm(dataloaders['train']):\\n    #print(f[0].shape)\\n    break\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for f, o in tqdm(dataloaders['train']):\n",
    "    #print(f[0].shape)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(a,first['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(DATA_LOADED, model, criterion, optimizer, num_epochs=25):\n",
    "    #train_model(DATA_LOADED, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 0\n",
    "    dataloaders ={}\n",
    "    dataloaders['train'], dataloaders['val'] = DATA_LOADED.train_dataloader, DATA_LOADED.val_dataloader\n",
    "\n",
    "    #TR_ACCURACY=[]\n",
    "    TR_LOSS=[]\n",
    "    #VAL_ACCURACY=[]\n",
    "    VAL_LOSS=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        model.reset_hidden_states()\n",
    "        #model.reset_hidden_states()\n",
    "        #print(model.hidden)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            #running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #if phase == 'train':\n",
    "            #    scheduler.step()\n",
    "\n",
    "            #epoch_loss = running_loss / DATA_LOADED.dataset_sizes[phase]\n",
    "            epoch_loss = running_loss / dataset_size[phase]\n",
    "            \n",
    "            #epoch_acc = running_corrects.double() / DATA.dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "\n",
    "            if phase == 'train':\n",
    "                #  print(\"Training\")\n",
    "                #  TR_ACCURACY.append(epoch_acc)\n",
    "                TR_LOSS.append(epoch_loss)\n",
    "            else:\n",
    "                print(\"Valuation\")\n",
    "                #VAL_ACCURACY.append(epoch_acc)\n",
    "                VAL_LOSS.append(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return TR_LOSS, VAL_LOSS, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHALLEABLELSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def reset_hidden_states(self):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        #lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        #self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        #lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= ht[-1]\n",
    "        #lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin0_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def reset_hidden_states(self,bs):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, bs, self.lstm_units).double(), torch.zeros(1*self.num_layers, bs, self.lstm_units).double())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= h.detach()[-1]\n",
    "        lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        #lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin1_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "batch_size = 16\n",
    "bs_val = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "Lookahead1.ipynb\n",
      "Lookahead2.ipynb\n",
      "Lookahead3.ipynb\n",
      "Lookahead4.ipynb\n",
      "Lookahead5.ipynb\n",
      "Lookahead6.ipynb\n",
      "Lookahead7.ipynb\n",
      "Lookahead8.ipynb\n",
      "Lookahead9.ipynb\n",
      "Lookahead10.ipynb\n",
      "Lookahead15.ipynb\n",
      "Lookahead20.ipynb\n",
      "readme.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../DATASET/TRANSORMER_DATA\"\n",
    "data = DATA(DATA_PATH)\n",
    "cols=['ThetaXHG']\n",
    "scaler = data.getscaler(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, X, y, X_scaled, y_scaled = data.process_data(scaler, input_cardinality=input_cardinality, output_cardinality=output_cardinality, cols=['ThetaXHG'])\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = data.train_test_split(X_scaled,y_scaled,  split={'train':0.8,'val':0.2, 'test':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1290, 1, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "net = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "net = net.to(device)\n",
    "net = net.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first['sequence'].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaded = THETADATASETLOADER(data, batchsize=batch_size, bs_val = bs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size={}\n",
    "dataset_size['train'] =X_train.shape[0]\n",
    "dataset_size['val'] =X_val.shape[0]\n",
    "dataset_size['test'] =X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_LOSS = []\n",
    "VAL_LOSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.125"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "722/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in range(epochs):\\n    net.reset_hidden_states(bs=batch_size)\\n    it = iter(data_loaded.train_dataloader)\\n    it_val = iter(data_loaded.train_dataloader)\\n    print('Epoch'+str(i))\\n    c = 0\\n    tr_loss = 0.0\\n    val_loss = 0.0\\n    \\n    while c < int(X_train.shape[0]/batch_size): \\n        with torch.set_grad_enabled(True):\\n            #print(c)\\n            c = c+1\\n            item = next(it)\\n            outputs = net(item[0])\\n            tr_loss = tr_loss + criterion(outputs, item[1])\\n    \\n    TR_LOSS.append(tr_loss)\\n    print(tr_loss)\\n    optimizer.zero_grad()\\n    tr_loss.backward()\\n    optimizer.step()\\n    \\n    net.reset_hidden_states(bs=bs_val)\\n    #print(net.hidden[0].shape)\\n    while c < int(X_val.shape[0]/bs_val):\\n        net.eval()\\n        with torch.set_grad_enabled(False):\\n            #print(c)\\n            c = c+1\\n            item = next(it_val)\\n            outputs = net(item[0])\\n            val_loss = val_loss + criterion(outputs, item[1])\\n    VAL_LOSS.append(val_loss)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "\"\"\"for i in range(epochs):\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    print('Epoch'+str(i))\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            outputs = net(item[0])\n",
    "            tr_loss = tr_loss + criterion(outputs, item[1])\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            outputs = net(item[0])\n",
    "            val_loss = val_loss + criterion(outputs, item[1])\n",
    "    VAL_LOSS.append(val_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.4141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch0 >> Training Loss: tensor(10.4141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(3.6199, dtype=torch.float64) <<\n",
      "tensor(28.4300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch1 >> Training Loss: tensor(28.4300, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.8719, dtype=torch.float64) <<\n",
      "tensor(5.9024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch2 >> Training Loss: tensor(5.9024, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(1.5216, dtype=torch.float64) <<\n",
      "tensor(10.8595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch3 >> Training Loss: tensor(10.8595, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(1.4224, dtype=torch.float64) <<\n",
      "tensor(10.0652, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch4 >> Training Loss: tensor(10.0652, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(1.0484, dtype=torch.float64) <<\n",
      "tensor(7.1657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch5 >> Training Loss: tensor(7.1657, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.9291, dtype=torch.float64) <<\n",
      "tensor(6.3420, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch6 >> Training Loss: tensor(6.3420, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(1.0251, dtype=torch.float64) <<\n",
      "tensor(7.2562, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch7 >> Training Loss: tensor(7.2562, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.9514, dtype=torch.float64) <<\n",
      "tensor(6.7635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch8 >> Training Loss: tensor(6.7635, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.7344, dtype=torch.float64) <<\n",
      "tensor(5.0708, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch9 >> Training Loss: tensor(5.0708, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.6183, dtype=torch.float64) <<\n",
      "tensor(4.1855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch10 >> Training Loss: tensor(4.1855, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.6038, dtype=torch.float64) <<\n",
      "tensor(4.1489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch11 >> Training Loss: tensor(4.1489, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.5286, dtype=torch.float64) <<\n",
      "tensor(3.6846, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch12 >> Training Loss: tensor(3.6846, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2658, dtype=torch.float64) <<\n",
      "tensor(1.7906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch13 >> Training Loss: tensor(1.7906, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1878, dtype=torch.float64) <<\n",
      "tensor(1.4452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch14 >> Training Loss: tensor(1.4452, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1486, dtype=torch.float64) <<\n",
      "tensor(1.2018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch15 >> Training Loss: tensor(1.2018, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0585, dtype=torch.float64) <<\n",
      "tensor(0.4945, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch16 >> Training Loss: tensor(0.4945, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1885, dtype=torch.float64) <<\n",
      "tensor(1.5672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch17 >> Training Loss: tensor(1.5672, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0634, dtype=torch.float64) <<\n",
      "tensor(0.5357, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch18 >> Training Loss: tensor(0.5357, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0867, dtype=torch.float64) <<\n",
      "tensor(0.6934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch19 >> Training Loss: tensor(0.6934, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0929, dtype=torch.float64) <<\n",
      "tensor(0.7561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch20 >> Training Loss: tensor(0.7561, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0202, dtype=torch.float64) <<\n",
      "tensor(0.1584, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch21 >> Training Loss: tensor(0.1584, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0476, dtype=torch.float64) <<\n",
      "tensor(0.3400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch22 >> Training Loss: tensor(0.3400, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0880, dtype=torch.float64) <<\n",
      "tensor(0.6380, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch23 >> Training Loss: tensor(0.6380, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0733, dtype=torch.float64) <<\n",
      "tensor(0.5260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch24 >> Training Loss: tensor(0.5260, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0439, dtype=torch.float64) <<\n",
      "tensor(0.3240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch25 >> Training Loss: tensor(0.3240, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0507, dtype=torch.float64) <<\n",
      "tensor(0.4154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch26 >> Training Loss: tensor(0.4154, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0624, dtype=torch.float64) <<\n",
      "tensor(0.5229, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch27 >> Training Loss: tensor(0.5229, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0382, dtype=torch.float64) <<\n",
      "tensor(0.3099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch28 >> Training Loss: tensor(0.3099, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0237, dtype=torch.float64) <<\n",
      "tensor(0.1617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch29 >> Training Loss: tensor(0.1617, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0427, dtype=torch.float64) <<\n",
      "tensor(0.2911, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch30 >> Training Loss: tensor(0.2911, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0501, dtype=torch.float64) <<\n",
      "tensor(0.3441, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch31 >> Training Loss: tensor(0.3441, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0307, dtype=torch.float64) <<\n",
      "tensor(0.1958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch32 >> Training Loss: tensor(0.1958, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0257, dtype=torch.float64) <<\n",
      "tensor(0.1688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch33 >> Training Loss: tensor(0.1688, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0364, dtype=torch.float64) <<\n",
      "tensor(0.2669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch34 >> Training Loss: tensor(0.2669, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0235, dtype=torch.float64) <<\n",
      "tensor(0.1711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch35 >> Training Loss: tensor(0.1711, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0105, dtype=torch.float64) <<\n",
      "tensor(0.0685, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch36 >> Training Loss: tensor(0.0685, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0182, dtype=torch.float64) <<\n",
      "tensor(0.1297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch37 >> Training Loss: tensor(0.1297, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0208, dtype=torch.float64) <<\n",
      "tensor(0.1509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch38 >> Training Loss: tensor(0.1509, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0116, dtype=torch.float64) <<\n",
      "tensor(0.0816, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch39 >> Training Loss: tensor(0.0816, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0110, dtype=torch.float64) <<\n",
      "tensor(0.0829, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch40 >> Training Loss: tensor(0.0829, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0173, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1357, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch41 >> Training Loss: tensor(0.1357, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0125, dtype=torch.float64) <<\n",
      "tensor(0.0962, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch42 >> Training Loss: tensor(0.0962, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0071, dtype=torch.float64) <<\n",
      "tensor(0.0515, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch43 >> Training Loss: tensor(0.0515, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0112, dtype=torch.float64) <<\n",
      "tensor(0.0857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch44 >> Training Loss: tensor(0.0857, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0113, dtype=torch.float64) <<\n",
      "tensor(0.0893, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch45 >> Training Loss: tensor(0.0893, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0062, dtype=torch.float64) <<\n",
      "tensor(0.0491, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch46 >> Training Loss: tensor(0.0491, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0087, dtype=torch.float64) <<\n",
      "tensor(0.0678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch47 >> Training Loss: tensor(0.0678, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0114, dtype=torch.float64) <<\n",
      "tensor(0.0883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch48 >> Training Loss: tensor(0.0883, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0068, dtype=torch.float64) <<\n",
      "tensor(0.0530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch49 >> Training Loss: tensor(0.0530, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0066, dtype=torch.float64) <<\n",
      "tensor(0.0519, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch50 >> Training Loss: tensor(0.0519, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0084, dtype=torch.float64) <<\n",
      "tensor(0.0655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch51 >> Training Loss: tensor(0.0655, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0057, dtype=torch.float64) <<\n",
      "tensor(0.0427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch52 >> Training Loss: tensor(0.0427, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0051, dtype=torch.float64) <<\n",
      "tensor(0.0371, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch53 >> Training Loss: tensor(0.0371, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0074, dtype=torch.float64) <<\n",
      "tensor(0.0542, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch54 >> Training Loss: tensor(0.0542, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0061, dtype=torch.float64) <<\n",
      "tensor(0.0436, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch55 >> Training Loss: tensor(0.0436, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0050, dtype=torch.float64) <<\n",
      "tensor(0.0335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch56 >> Training Loss: tensor(0.0335, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0064, dtype=torch.float64) <<\n",
      "tensor(0.0433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch57 >> Training Loss: tensor(0.0433, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0058, dtype=torch.float64) <<\n",
      "tensor(0.0373, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch58 >> Training Loss: tensor(0.0373, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0045, dtype=torch.float64) <<\n",
      "tensor(0.0262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch59 >> Training Loss: tensor(0.0262, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0058, dtype=torch.float64) <<\n",
      "tensor(0.0345, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch60 >> Training Loss: tensor(0.0345, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0061, dtype=torch.float64) <<\n",
      "tensor(0.0348, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch61 >> Training Loss: tensor(0.0348, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0055, dtype=torch.float64) <<\n",
      "tensor(0.0275, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch62 >> Training Loss: tensor(0.0275, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0063, dtype=torch.float64) <<\n",
      "tensor(0.0333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch63 >> Training Loss: tensor(0.0333, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0063, dtype=torch.float64) <<\n",
      "tensor(0.0328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch64 >> Training Loss: tensor(0.0328, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0054, dtype=torch.float64) <<\n",
      "tensor(0.0257, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch65 >> Training Loss: tensor(0.0257, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0057, dtype=torch.float64) <<\n",
      "tensor(0.0292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch66 >> Training Loss: tensor(0.0292, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0057, dtype=torch.float64) <<\n",
      "tensor(0.0298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch67 >> Training Loss: tensor(0.0298, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0052, dtype=torch.float64) <<\n",
      "tensor(0.0255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch68 >> Training Loss: tensor(0.0255, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0056, dtype=torch.float64) <<\n",
      "tensor(0.0285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch69 >> Training Loss: tensor(0.0285, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0056, dtype=torch.float64) <<\n",
      "tensor(0.0286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch70 >> Training Loss: tensor(0.0286, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0050, dtype=torch.float64) <<\n",
      "tensor(0.0244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch71 >> Training Loss: tensor(0.0244, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0052, dtype=torch.float64) <<\n",
      "tensor(0.0258, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch72 >> Training Loss: tensor(0.0258, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0052, dtype=torch.float64) <<\n",
      "tensor(0.0256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch73 >> Training Loss: tensor(0.0256, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0050, dtype=torch.float64) <<\n",
      "tensor(0.0230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch74 >> Training Loss: tensor(0.0230, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0052, dtype=torch.float64) <<\n",
      "tensor(0.0249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch75 >> Training Loss: tensor(0.0249, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0051, dtype=torch.float64) <<\n",
      "tensor(0.0246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch76 >> Training Loss: tensor(0.0246, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0048, dtype=torch.float64) <<\n",
      "tensor(0.0227, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch77 >> Training Loss: tensor(0.0227, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0049, dtype=torch.float64) <<\n",
      "tensor(0.0239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch78 >> Training Loss: tensor(0.0239, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0047, dtype=torch.float64) <<\n",
      "tensor(0.0230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch79 >> Training Loss: tensor(0.0230, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch80 >> Training Loss: tensor(0.0220, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0045, dtype=torch.float64) <<\n",
      "tensor(0.0233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch81 >> Training Loss: tensor(0.0233, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch82 >> Training Loss: tensor(0.0225, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch83 >> Training Loss: tensor(0.0222, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0229, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch84 >> Training Loss: tensor(0.0229, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch85 >> Training Loss: tensor(0.0218, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch86 >> Training Loss: tensor(0.0219, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch87 >> Training Loss: tensor(0.0222, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch88 >> Training Loss: tensor(0.0215, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch89 >> Training Loss: tensor(0.0219, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch90 >> Training Loss: tensor(0.0218, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0212, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch91 >> Training Loss: tensor(0.0212, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch92 >> Training Loss: tensor(0.0215, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0212, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch93 >> Training Loss: tensor(0.0212, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0210, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch94 >> Training Loss: tensor(0.0210, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch95 >> Training Loss: tensor(0.0213, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch96 >> Training Loss: tensor(0.0209, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch97 >> Training Loss: tensor(0.0209, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch98 >> Training Loss: tensor(0.0209, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch99 >> Training Loss: tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0045, dtype=torch.float64) <<\n",
      "tensor(0.0208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch100 >> Training Loss: tensor(0.0208, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0045, dtype=torch.float64) <<\n",
      "tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch101 >> Training Loss: tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0045, dtype=torch.float64) <<\n",
      "tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch102 >> Training Loss: tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0045, dtype=torch.float64) <<\n",
      "tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch103 >> Training Loss: tensor(0.0206, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch104 >> Training Loss: tensor(0.0204, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch105 >> Training Loss: tensor(0.0204, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch106 >> Training Loss: tensor(0.0204, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0044, dtype=torch.float64) <<\n",
      "tensor(0.0203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch107 >> Training Loss: tensor(0.0203, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch108 >> Training Loss: tensor(0.0203, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch109 >> Training Loss: tensor(0.0202, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch110 >> Training Loss: tensor(0.0201, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch111 >> Training Loss: tensor(0.0201, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch112 >> Training Loss: tensor(0.0200, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0043, dtype=torch.float64) <<\n",
      "tensor(0.0200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch113 >> Training Loss: tensor(0.0200, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0042, dtype=torch.float64) <<\n",
      "tensor(0.0199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch114 >> Training Loss: tensor(0.0199, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0042, dtype=torch.float64) <<\n",
      "tensor(0.0199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch115 >> Training Loss: tensor(0.0199, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0042, dtype=torch.float64) <<\n",
      "tensor(0.0199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch116 >> Training Loss: tensor(0.0199, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0041, dtype=torch.float64) <<\n",
      "tensor(0.0198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch117 >> Training Loss: tensor(0.0198, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0041, dtype=torch.float64) <<\n",
      "tensor(0.0198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch118 >> Training Loss: tensor(0.0198, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0041, dtype=torch.float64) <<\n",
      "tensor(0.0197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch119 >> Training Loss: tensor(0.0197, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0041, dtype=torch.float64) <<\n",
      "tensor(0.0197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch120 >> Training Loss: tensor(0.0197, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0041, dtype=torch.float64) <<\n",
      "tensor(0.0197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch121 >> Training Loss: tensor(0.0197, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch122 >> Training Loss: tensor(0.0196, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch123 >> Training Loss: tensor(0.0196, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch124 >> Training Loss: tensor(0.0195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch125 >> Training Loss: tensor(0.0195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch126 >> Training Loss: tensor(0.0194, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch127 >> Training Loss: tensor(0.0194, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch128 >> Training Loss: tensor(0.0194, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch129 >> Training Loss: tensor(0.0193, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0040, dtype=torch.float64) <<\n",
      "tensor(0.0193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch130 >> Training Loss: tensor(0.0193, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch131 >> Training Loss: tensor(0.0192, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch132 >> Training Loss: tensor(0.0192, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch133 >> Training Loss: tensor(0.0192, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch134 >> Training Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch135 >> Training Loss: tensor(0.0191, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch136 >> Training Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch137 >> Training Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch138 >> Training Loss: tensor(0.0190, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0039, dtype=torch.float64) <<\n",
      "tensor(0.0189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch139 >> Training Loss: tensor(0.0189, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0038, dtype=torch.float64) <<\n",
      "tensor(0.0189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch140 >> Training Loss: tensor(0.0189, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0038, dtype=torch.float64) <<\n",
      "tensor(0.0189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch141 >> Training Loss: tensor(0.0189, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0038, dtype=torch.float64) <<\n",
      "tensor(0.0188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch142 >> Training Loss: tensor(0.0188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0038, dtype=torch.float64) <<\n",
      "tensor(0.0188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch143 >> Training Loss: tensor(0.0188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0038, dtype=torch.float64) <<\n",
      "tensor(0.0188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch144 >> Training Loss: tensor(0.0188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0038, dtype=torch.float64) <<\n",
      "tensor(0.0187, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch145 >> Training Loss: tensor(0.0187, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0038, dtype=torch.float64) <<\n",
      "tensor(0.0187, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch146 >> Training Loss: tensor(0.0187, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0037, dtype=torch.float64) <<\n",
      "tensor(0.0186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch147 >> Training Loss: tensor(0.0186, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0037, dtype=torch.float64) <<\n",
      "tensor(0.0186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch148 >> Training Loss: tensor(0.0186, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0037, dtype=torch.float64) <<\n",
      "tensor(0.0186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch149 >> Training Loss: tensor(0.0186, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0037, dtype=torch.float64) <<\n",
      "tensor(0.0185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch150 >> Training Loss: tensor(0.0185, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0037, dtype=torch.float64) <<\n",
      "tensor(0.0185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch151 >> Training Loss: tensor(0.0185, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0037, dtype=torch.float64) <<\n",
      "tensor(0.0185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch152 >> Training Loss: tensor(0.0185, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch153 >> Training Loss: tensor(0.0184, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch154 >> Training Loss: tensor(0.0184, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch155 >> Training Loss: tensor(0.0184, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch156 >> Training Loss: tensor(0.0183, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch157 >> Training Loss: tensor(0.0183, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch158 >> Training Loss: tensor(0.0183, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch159 >> Training Loss: tensor(0.0182, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0036, dtype=torch.float64) <<\n",
      "tensor(0.0182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch160 >> Training Loss: tensor(0.0182, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n",
      "tensor(0.0182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch161 >> Training Loss: tensor(0.0182, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n",
      "tensor(0.0181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch162 >> Training Loss: tensor(0.0181, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n",
      "tensor(0.0181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch163 >> Training Loss: tensor(0.0181, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch164 >> Training Loss: tensor(0.0181, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n",
      "tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch165 >> Training Loss: tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n",
      "tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch166 >> Training Loss: tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n",
      "tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch167 >> Training Loss: tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0035, dtype=torch.float64) <<\n",
      "tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch168 >> Training Loss: tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch169 >> Training Loss: tensor(0.0179, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch170 >> Training Loss: tensor(0.0179, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0179, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch171 >> Training Loss: tensor(0.0179, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch172 >> Training Loss: tensor(0.0178, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch173 >> Training Loss: tensor(0.0178, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch174 >> Training Loss: tensor(0.0178, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch175 >> Training Loss: tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch176 >> Training Loss: tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0034, dtype=torch.float64) <<\n",
      "tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch177 >> Training Loss: tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch178 >> Training Loss: tensor(0.0177, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch179 >> Training Loss: tensor(0.0176, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch180 >> Training Loss: tensor(0.0176, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch181 >> Training Loss: tensor(0.0176, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch182 >> Training Loss: tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch183 >> Training Loss: tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch184 >> Training Loss: tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch185 >> Training Loss: tensor(0.0175, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0033, dtype=torch.float64) <<\n",
      "tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch186 >> Training Loss: tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch187 >> Training Loss: tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch188 >> Training Loss: tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch189 >> Training Loss: tensor(0.0174, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch190 >> Training Loss: tensor(0.0173, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch191 >> Training Loss: tensor(0.0173, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch192 >> Training Loss: tensor(0.0173, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch193 >> Training Loss: tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch194 >> Training Loss: tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch195 >> Training Loss: tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0032, dtype=torch.float64) <<\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch196 >> Training Loss: tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch197 >> Training Loss: tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch198 >> Training Loss: tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch199 >> Training Loss: tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch200 >> Training Loss: tensor(0.0171, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch201 >> Training Loss: tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch202 >> Training Loss: tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch203 >> Training Loss: tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch204 >> Training Loss: tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch205 >> Training Loss: tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch206 >> Training Loss: tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0031, dtype=torch.float64) <<\n",
      "tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch207 >> Training Loss: tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch208 >> Training Loss: tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch209 >> Training Loss: tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch210 >> Training Loss: tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch211 >> Training Loss: tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch212 >> Training Loss: tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch213 >> Training Loss: tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch214 >> Training Loss: tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch215 >> Training Loss: tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch216 >> Training Loss: tensor(0.0167, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch217 >> Training Loss: tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch218 >> Training Loss: tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0030, dtype=torch.float64) <<\n",
      "tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch219 >> Training Loss: tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch220 >> Training Loss: tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch221 >> Training Loss: tensor(0.0166, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch222 >> Training Loss: tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch223 >> Training Loss: tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch224 >> Training Loss: tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch225 >> Training Loss: tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch226 >> Training Loss: tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch227 >> Training Loss: tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch228 >> Training Loss: tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch229 >> Training Loss: tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch230 >> Training Loss: tensor(0.0164, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch231 >> Training Loss: tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0029, dtype=torch.float64) <<\n",
      "tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch232 >> Training Loss: tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch233 >> Training Loss: tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch234 >> Training Loss: tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch235 >> Training Loss: tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch236 >> Training Loss: tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch237 >> Training Loss: tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch238 >> Training Loss: tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch239 >> Training Loss: tensor(0.0162, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch240 >> Training Loss: tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch241 >> Training Loss: tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch242 >> Training Loss: tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch243 >> Training Loss: tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch244 >> Training Loss: tensor(0.0161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch245 >> Training Loss: tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch246 >> Training Loss: tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0028, dtype=torch.float64) <<\n",
      "tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch247 >> Training Loss: tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch248 >> Training Loss: tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch249 >> Training Loss: tensor(0.0160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch250 >> Training Loss: tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch251 >> Training Loss: tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch252 >> Training Loss: tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch253 >> Training Loss: tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch254 >> Training Loss: tensor(0.0159, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch255 >> Training Loss: tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch256 >> Training Loss: tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch257 >> Training Loss: tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch258 >> Training Loss: tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch259 >> Training Loss: tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch260 >> Training Loss: tensor(0.0158, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch261 >> Training Loss: tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch262 >> Training Loss: tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch263 >> Training Loss: tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch264 >> Training Loss: tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0027, dtype=torch.float64) <<\n",
      "tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch265 >> Training Loss: tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch266 >> Training Loss: tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch267 >> Training Loss: tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch268 >> Training Loss: tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch269 >> Training Loss: tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch270 >> Training Loss: tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch271 >> Training Loss: tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch272 >> Training Loss: tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch273 >> Training Loss: tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch274 >> Training Loss: tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch275 >> Training Loss: tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch276 >> Training Loss: tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch277 >> Training Loss: tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch278 >> Training Loss: tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch279 >> Training Loss: tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch280 >> Training Loss: tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch281 >> Training Loss: tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch282 >> Training Loss: tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch283 >> Training Loss: tensor(0.0154, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch284 >> Training Loss: tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch285 >> Training Loss: tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0026, dtype=torch.float64) <<\n",
      "tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch286 >> Training Loss: tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch287 >> Training Loss: tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch288 >> Training Loss: tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch289 >> Training Loss: tensor(0.0153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch290 >> Training Loss: tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch291 >> Training Loss: tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch292 >> Training Loss: tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch293 >> Training Loss: tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch294 >> Training Loss: tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch295 >> Training Loss: tensor(0.0152, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch296 >> Training Loss: tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch297 >> Training Loss: tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch298 >> Training Loss: tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch299 >> Training Loss: tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch300 >> Training Loss: tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch301 >> Training Loss: tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch302 >> Training Loss: tensor(0.0151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch303 >> Training Loss: tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch304 >> Training Loss: tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch305 >> Training Loss: tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch306 >> Training Loss: tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch307 >> Training Loss: tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch308 >> Training Loss: tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch309 >> Training Loss: tensor(0.0150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch310 >> Training Loss: tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch311 >> Training Loss: tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch312 >> Training Loss: tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch313 >> Training Loss: tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0025, dtype=torch.float64) <<\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch314 >> Training Loss: tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch315 >> Training Loss: tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch316 >> Training Loss: tensor(0.0149, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch317 >> Training Loss: tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch318 >> Training Loss: tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch319 >> Training Loss: tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch320 >> Training Loss: tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch321 >> Training Loss: tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch322 >> Training Loss: tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch323 >> Training Loss: tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch324 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch325 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch326 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch327 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch328 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch329 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch330 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch331 >> Training Loss: tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch332 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch333 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch334 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch335 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch336 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch337 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch338 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch339 >> Training Loss: tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch340 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch341 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch342 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch343 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch344 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch345 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch346 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch347 >> Training Loss: tensor(0.0145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch348 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch349 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch350 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch351 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch352 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch353 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch354 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0024, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch355 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch356 >> Training Loss: tensor(0.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch357 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch358 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch359 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch360 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch361 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch362 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch363 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch364 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch365 >> Training Loss: tensor(0.0143, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch366 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch367 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch368 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch369 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch370 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch371 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch372 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch373 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch374 >> Training Loss: tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch375 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch376 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch377 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch378 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch379 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch380 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch381 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch382 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch383 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch384 >> Training Loss: tensor(0.0141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch385 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch386 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch387 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch388 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch389 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch390 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch391 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch392 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch393 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch394 >> Training Loss: tensor(0.0140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch395 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch396 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch397 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch398 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch399 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch400 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch401 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch402 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch403 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch404 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch405 >> Training Loss: tensor(0.0139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch406 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch407 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch408 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch409 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch410 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch411 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch412 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch413 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch414 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch415 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch416 >> Training Loss: tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch417 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch418 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch419 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch420 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch421 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch422 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch423 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch424 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch425 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch426 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch427 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch428 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch429 >> Training Loss: tensor(0.0137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch430 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch431 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch432 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch433 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch434 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch435 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch436 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch437 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch438 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch439 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch440 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch441 >> Training Loss: tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0023, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch442 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch443 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch444 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch445 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch446 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch447 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch448 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch449 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch450 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch451 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch452 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch453 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch454 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch455 >> Training Loss: tensor(0.0135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch456 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch457 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch458 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch459 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch460 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch461 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch462 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch463 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch464 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch465 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch466 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch467 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch468 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch469 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch470 >> Training Loss: tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch471 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch472 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch473 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch474 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch475 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch476 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch477 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch478 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch479 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch480 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch481 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch482 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch483 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch484 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch485 >> Training Loss: tensor(0.0133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch486 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch487 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch488 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch489 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch490 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch491 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch492 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch493 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch494 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch495 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch496 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch497 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch498 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch499 >> Training Loss: tensor(0.0132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0022, dtype=torch.float64) <<\n"
     ]
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "TR_LOSS = []\n",
    "VAL_LOSS = []\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    #print(net.hidden[0].shape)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            tr_loss = tr_loss + criterion(outputs, labels)\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    c = 0\n",
    "    \n",
    "    #net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            val_loss = val_loss + criterion(outputs, labels)\n",
    "            #print(val_loss)\n",
    "    VAL_LOSS.append(val_loss)\n",
    "    print('Epoch'+str(i)+ ' >> Training Loss: '+str(tr_loss)+ ' Validation Loss: '+str(val_loss)+ ' <<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOSS = []\n",
    "for t in TR_LOSS:\n",
    "    cc = t.detach().numpy()\n",
    "    TRT_LOSS.append(cc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(10.4140715)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUYUlEQVR4nO3dfWxdd33H8c/Xduw4idM4iROcB5o+ZNACa1pZpaXVVKBA6ViBDTGiipWtUkCDqQymqe0kVjRNgAQtFKaKdK3aaaUDRKt2pQKyNIKBRsApoUmbQkIVaNIkdto8OE9+ut/9cc+1r33s+Ob6Xp/79X2/JOs83nu+P+fmk19+55x7zN0FAIinIesCAADlIcABICgCHACCIsABICgCHACCaprJgy1dutTXrFkzk4cEgPC2bdt22N07xq+f0QBfs2aNuru7Z/KQABCemf1+ovUMoQBAUAQ4AARFgANAUAQ4AARFgANAUAQ4AARFgANAUGEC/JkXD+mVo6ezLgMAakaYAP+bh7p10zd+mnUZAFAzwgS4JB0+MZB1CQBQM0IFOABgFAEOAEER4AAQVIgA58HLAJAWJMCzrgAAak+MAM+6AACoQTECnC44AKTECPCsCwCAGhQiwHP0wAEgJUSAk98AkBYiwAEAaSECnB44AKSFCHDGwAEgLUSAE98AkBYjwOmBA0BKjADPugAAqEExAjyXdQUAUHtiBDh9cABIiRHg5DcApEwZ4Ga22sy2mNkLZva8md2WrL/LzPab2fbk58ZqFUl+A0BaUwn7DEn6rLs/a2ZtkraZ2aZk2z3u/uXqlZfHdeAAkDZlgLv7AUkHkvk+M9slaWW1Cxtbw0weDQBiOKcxcDNbI+lySVuTVZ8ys+fM7EEza5/kNRvMrNvMunt7e8sqkpOYAJBWcoCb2QJJ35P0aXc/Luk+SRdJWqd8D/0rE73O3Te6e5e7d3V0dJRXJfkNACklBbiZzVE+vB9x98ckyd0Pufuwu+ck3S/pymoVmSPAASCllKtQTNIDkna5+91F6zuLdvugpJ2VLy+PIRQASCvlKpRrJH1U0g4z256su1PSejNbp/wAx15JH69CfZI4iQkAEynlKpSfSrIJNj1d+XImqWGmDgQAgYS4EzPHIDgApIQIcABAWogAZwwcANJiBDij4ACQEiLAGQIHgLQQAc4j1QAgLUaAZ10AANSgGAFOggNASpAAJ8EBYLwYAZ51AQBQg2IEOAkOACkxApw+OACkhAjwXC7rCgCg9oQIcHrgAJAWI8DJbwBICRHgAIC0EAGeowsOACkhApz8BoC0GAGedQEAUINiBDhdcABIiRHgWRcAADUoRoDTAweAlCABnnUFAFB7YgR41gUAQA2KEeAkOACkhAhwbuQBgLQpA9zMVpvZFjN7wcyeN7PbkvWLzWyTme1Opu3VKpL8BoC0UnrgQ5I+6+6XSrpK0ifN7FJJt0va7O5rJW1OlquCbyMEgLQpA9zdD7j7s8l8n6RdklZKer+kh5PdHpb0gSrVyFlMAJjAOY2Bm9kaSZdL2ippubsfSDYdlLR8ktdsMLNuM+vu7e0tq8icF96rrJcDwKxUcoCb2QJJ35P0aXc/XrzN83faTNhPdveN7t7l7l0dHR1lFckQCgCklRTgZjZH+fB+xN0fS1YfMrPOZHunpJ7qlMhJTACYSClXoZikByTtcve7izY9KemWZP4WSU9Uvrw88hsA0ppK2OcaSR+VtMPMtifr7pT0RUnfMbNbJf1e0oerUqFGrwNnCBwARk0Z4O7+U02ene+sbDmTFTEjRwGAUELciclJTABIixHg5DcApIQI8NHrwBkFB4CCEAHOAx0AIC1GgGddAADUoBgBToIDQEqQAOc6cAAYL0aAZ10AANSgGAFOggNASowApw8OACkhApzvAweAtBABznXgAJAWIsABAGkhApwOOACkhQjw0e8DZxAcAApCBDg9cABIixHgWRcAADUoRoDTBQeAlCABnswwBA4AI2IEOIMoAJASI8DJbwBIiRHgWRcAADUoRIDn+D5wAEgJEeAMoQBAWowAz7oAAKhBUwa4mT1oZj1mtrNo3V1mtt/Mtic/N1a1SrrgAJBSSg/8IUk3TLD+Hndfl/w8XdmyxuL7wAEgbcoAd/efSHptBmo5Ww1ZHh4AatJ0xsA/ZWbPJUMs7RWraALENwCklRvg90m6SNI6SQckfWWyHc1sg5l1m1l3b29vWQejAw4AaWUFuLsfcvdhd89Jul/SlWfZd6O7d7l7V0dHR1lF8n3gAJBWVoCbWWfR4gcl7ZxsXwBAdTRNtYOZPSrpOklLzWyfpH+WdJ2ZrVN+eHqvpI9Xr0SGUABgIlMGuLuvn2D1A1WoZfIaOI0JACkh7sTkOnAASAsR4AyhAEBajABnCAUAUmIEOPkNAClBApzvAweA8YIEeNYVAEDtiRHgWRcAADUoRoCT4ACQEiPA6YMDQEqIAM+R3wCQEiLAC2Mo5DgAjAoR4IXgZiwcAEbFCPAkuBkLB4BRIQK88EAHeuAAMCpEgPu4KQAgSoCT4ACQEiPA5WOmAIAoAe5jpwCAMAHOdeAAMF6QAC9MiXAAKIgR4OOmAIAgAc514ACQFiLAi4P74LEz2RUCADUkRIAXu+oLm7MuAQBqQogA5+QlAKSFCPDx3wee4wvCAWDqADezB82sx8x2Fq1bbGabzGx3Mm2vZpHj78B89eRANQ8HACGU0gN/SNIN49bdLmmzu6+VtDlZrpqPve0CXXvx0pFlTmQCQAkB7u4/kfTauNXvl/RwMv+wpA9UtqyxLl62QFdduHhk+ZVjp6t5OAAIodwx8OXufiCZPyhp+WQ7mtkGM+s2s+7e3t4yDyeZ2ch835mhst8HAGaLaZ/E9PwlIpOeVXT3je7e5e5dHR0d0z2cJKl/aLgi7wMAkZUb4IfMrFOSkmlP5UqaWFEHXP2DuWofDgBqXrkB/qSkW5L5WyQ9UZlyJmcaTfD+IQIcAEq5jPBRSf8n6Q1mts/MbpX0RUnvMrPdkq5PlququAc+QIADgJqm2sHd10+y6Z0VruWsivKbMXAAUJA7MaVxY+D0wAEgUICPGQOnBw4AYQK8GGPgABAowIeLvpGQIRQAiBTgRd9AyHXgABA0wAeGCXAACBPgQ8U9cE5iAkCcAM8xhAIAY4QJ8LE9cAIcAMIEeK7oKhQuIwSAQAE+NMwYOAAUCxPgOa4DB4AxwgT4UG40tM8MDmtPz4kMqwGA7IUJ8MJ14POaG3Xk1KCuv/vH+vlLr2ZcFQBkJ1yAL2qdM7Lu2T8cyaocAMhcmAAvXEa4sr11ZF3P8f6sygGAzIUJ8MKNPCsWjQb47p6+rMoBgMyFCfCRHnhRgPf20QMHUL/CBHjhMsLiIZQTZ4ayKgcAMhcmwAs38iyZ3zyy7kQ/AQ6gfoUJ8EIPvLGhQfeuv1zXX7JcJweG5UU3+ABAPQkT4IUx8MYG6abLVuiK8xdpOOfclQmgboUJ8LlNjZKklmS6oKVJktTHODiAOtWUdQGl+tcPvlmXdC7U1RcukTQa4Cf7h9TR1pJlaQCQiTABvmRBi267fu3I8vwkwDmRCaBehRlCGa+4Bw4A9WhaPXAz2yupT9KwpCF376pEUaVYQA8cQJ2rxBDK2939cAXe55wwhAKg3oUdQmmbS4ADqG/TDXCX9CMz22ZmGybawcw2mFm3mXX39vZO83Cj2ufl78g83DdQsfcEgEimG+DXuvsVkt4r6ZNm9ifjd3D3je7e5e5dHR0d0zzcqOamBi2e36xDfWcq9p4AEMm0Atzd9yfTHkmPS7qyEkWVallbC98JDqBulR3gZjbfzNoK85LeLWlnpQorRUdbi3rpgQOoU9O5CmW5pMfNrPA+33L3H1SkqlILWDhXuw/xcGMA9ansAHf3lyRdVsFaztnyhS06ePyMfrbnsK65eGmWpQDAjAt7GaEk3fzW89XS1KBv/uSlrEsBgBkXOsBXLGrVn122Qi+8cjzrUgBgxsUJ8Eke3PCmFQt1+ES/ejiZCaDOxAjw7/+D9OU/mnDT2mVtkqSXek/OZEUAkLkYAd7QJA1N3MNekNxSf2qAW+oB1JcYAT5n7qQBPr85/4Sek/3DM1kRAGQuRoA3zZWGB6Rc+vmX81rogQOoT0ECPHlk2nD6tnl64ADqVZAAn5ufDp5ObZrXnO+Bnx4kwAHUlyABnvTAh9I98OamBs1pNB6tBqDuBAnwpAc+yYnM1jmNOjVADxxAfQkS4JP3wKX849XogQOoN0ECvDU/naQHPq+ZHjiA+hMkwEvogXMZIYA6EyTAC2Pg6atQJHrgAOpTsACfuAc+r7mJG3kA1J0gAV4YQpl4DPy81jk8nR5A3QkS4EkPfNd/T/i1sutWL9LB42f020N9uvPxHTpykjAHMPvFCPA5SYDv+K60+fOpzVdftESS9Pff3q5vbf2DvrFlz0xWBwCZiBHghR64JD33ndTmtcsWaPXiVj2fPJmnf4gTmgBmvyAB3jI6f3y/dGzfmM1mppsuWzHDRQFAtoIE+Nyxy/e8SXrqM1JutKf9phXnjcwPp791FgBmnRgB3tA0Ov+GP81Pux+QXnxqZPWq9taR+WOnB+STPEMTAGaLGAFuJr3vq9LfbpU+/B/SZ16UFq6SfnbvyLXhq9rnjez+9I6DuuJfNunAsdPad+RURkUDQHXFCHBJ6vpradkbpcYmaWGn9PY7pP3d0lfeKO3fpvZ5c8bsfuTUoG6+f6uu/dIW7dx/TD/YeYCbfQDMKnECfLx1N0t/+Uj+BOePPiczG9nU0ZY/6fnS4fyT6j+y8ef6xH8+q3s379GJ/iEdPcV14gDis+mMFZvZDZK+JqlR0r+7+xfPtn9XV5d3d3eXfbwJbfmC9OMvSX+3Ta81LdOc5mbNa2nWRXc+LUlavbhVL792Wm0tTerrH1Jjg+l1C+fqz69YqUs6F+rGt3RWth4AqDAz2+buXePXN020c4lv2Cjp3yS9S9I+Sb80syfd/YXyyyzDuvXS1vukr1+hxZLUvka6coPW2ZAubtivu95zjXb1tWpB2yL9xaMvq8OPqvfoIt3/zFGdUYtWnDdXK9tb9ZaVi3Ts9KCuvKBd/UM5rWpv1asnBnRJ50K1NDWoqTH/5J85jQ1qbmzQ/JamkevNC71/U364Pj9vKvpPQWq9jay3onmN+Z8EAJxN2T1wM7ta0l3u/p5k+Q5JcvcvTPaaqvTAJenV30mPf0La94tzetlxa1NjblADmqNB5R+OPKhGDXuDciWOLk3223NNHsRn21bMJl2YYt8plHr8ain+nVlGtZzzp55/VyumOr/K2v8DOvnuL+stb3tvWa+teA9c0kpJLxct75P01gkOvEHSBkl6/etfP43DncWSi6SPfV869rI0PJC/Pnzv/0rnXyPlhqS+g9KZo9LxV6S210kneiTPad7RfRq0Zi3UkAYHB9TYYDrad1JzG12nBobU3Nig04PDyrmUy7ncXTlJwznX0LCrqSjjxwRC0T+KPj4qxmwb+8JJ/zHw9B6pff3s7zH1Uarzsiq+0TTUQA01UMJUApRYNam/txWw8Lz2ir/ndAK8JO6+UdJGKd8Dr9qBmprzQV7wujdP/RKN/gIak2lHMm1LppX/lQNAZUznKpT9klYXLa9K1gEAZsB0AvyXktaa2QVm1izpI5KerExZAICplD2E4u5DZvYpST9UfgTiQXd/vmKVAQDOalpj4O7+tKSnK1QLAOAcxL0TEwDqHAEOAEER4AAQFAEOAEFN68uszvlgZr2Sfl/my5dKOlzBciKgzfWBNteH6bT5fHfvGL9yRgN8Osyse6LvApjNaHN9oM31oRptZggFAIIiwAEgqEgBvjHrAjJAm+sDba4PFW9zmDFwAMBYkXrgAIAiBDgABBUiwM3sBjP7jZntMbPbs66nUszsQTPrMbOdResWm9kmM9udTNuT9WZm9ya/g+fM7IrsKi+Pma02sy1m9oKZPW9mtyXrZ22bJcnM5prZL8zs10m7P5+sv8DMtibt+3bytcwys5ZkeU+yfU2mDSiTmTWa2a/M7KlkeVa3V5LMbK+Z7TCz7WbWnayr2ue75gO86OHJ75V0qaT1ZnZptlVVzEOSbhi37nZJm919raTNybKUb//a5GeDpPtmqMZKGpL0WXe/VNJVkj6Z/FnO5jZLUr+kd7j7ZZLWSbrBzK6S9CVJ97j7xZKOSLo12f9WSUeS9fck+0V0m6RdRcuzvb0Fb3f3dUXXfFfv8+3uNf0j6WpJPyxavkPSHVnXVcH2rZG0s2j5N5I6k/lOSb9J5r8paf1E+0X9kfSEpHfVWZvnSXpW+efHHpbUlKwf+Zwr/x37VyfzTcl+lnXt59jOVUlYvUPSU8o/dXjWtreo3XslLR23rmqf75rvgWvihyevzKiWmbDc3Q8k8wclLU/mZ9XvIflv8uWStqoO2pwMJ2yX1CNpk6TfSTrq7kPJLsVtG2l3sv2YpCUzWvD0fVXSP0rKJctLNLvbW+CSfmRm25IHuktV/HxX/aHGKJ+7u5nNuus8zWyBpO9J+rS7HzezkW2ztc3uPixpnZktkvS4pDdmW1H1mNn7JPW4+zYzuy7jcmbate6+38yWSdpkZi8Wb6z05ztCD7zeHp58yMw6JSmZ9iTrZ8XvwczmKB/ej7j7Y8nqWd3mYu5+VNIW5YcQFplZoRNV3LaRdifbz5P06sxWOi3XSLrJzPZK+i/lh1G+ptnb3hHuvj+Z9ij/D/WVquLnO0KA19vDk5+UdEsyf4vy48SF9X+VnLm+StKxov+WhWD5rvYDkna5+91Fm2ZtmyXJzDqSnrfMrFX5cf9dygf5h5Ldxre78Pv4kKRnPBkkjcDd73D3Ve6+Rvm/r8+4+82ape0tMLP5ZtZWmJf0bkk7Vc3Pd9aD/iWeGLhR0m+VHzf8p6zrqWC7HpV0QNKg8uNftyo/9rdZ0m5J/yNpcbKvKX81zu8k7ZDUlXX9ZbT3WuXHCJ+TtD35uXE2tzlpxx9L+lXS7p2SPpesv1DSLyTtkfRdSS3J+rnJ8p5k+4VZt2Eabb9O0lP10N6kfb9Ofp4vZFU1P9/cSg8AQUUYQgEATIAAB4CgCHAACIoAB4CgCHAACIoAB4CgCHAACOr/AcJih1VQs5BVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(TRT_LOSS)\n",
    "plt.plot(VAL_LOSS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0298, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "accumulated_test_loss = 0.0\n",
    "net.reset_hidden_states(bs=1)\n",
    "it_test = iter(data_loaded.test_dataloader)\n",
    "c=0\n",
    "while c < int(X_test.shape[0]/1):\n",
    "    #net.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        #print(c)\n",
    "        c = c+1\n",
    "        item = next(it_test)\n",
    "        #print(item[0])\n",
    "        #break\n",
    "        outputs = net(item[0])\n",
    "        accumulated_test_loss = accumulated_test_loss + criterion(outputs, item[1])        \n",
    "print(accumulated_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('RESULTS_LOSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_array = []\n",
    "for z in TR_LOSS:\n",
    "    tr_loss_array.append(z.tolist())\n",
    "training_loss = pd.DataFrame(tr_loss_array, columns=['TR_LOSS'])\n",
    "training_loss.to_csv('../RESULTS_LOSS/TR_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_array = []\n",
    "for z in VAL_LOSS:\n",
    "    val_loss_array.append(z.tolist())\n",
    "val_loss = pd.DataFrame(val_loss_array, columns=['VAL_LOSS'])\n",
    "val_loss.to_csv('../RESULTS_LOSS/VAL_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_array = [accumulated_test_loss]\n",
    "\n",
    "test2_loss = pd.DataFrame(test_loss_array, columns=['TEST_LOSS'])\n",
    "test2_loss.to_csv('../RESULTS_LOSS/TEST_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE SCALER\n",
    "import joblib\n",
    "scaler_filename = \"../SCALER_DUMPS/min_max_scaler_lookahead1.save\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "# And now to load...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE MODEL\n",
    "model_ckp_path = \"../MODEL_CHECKPOINTS/model_lookahead1.pth\" \n",
    "torch.save(net.state_dict(), model_ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net2 = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "#net2.load_state_dict(torch.load(model_ckp_path))\n",
    "#net2.eval()\n",
    "#scaler = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMNET(\n",
       "  (lstm1): LSTM(1, 4, batch_first=True, dropout=0.6)\n",
       "  (linear0): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (linear1): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (ll): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2211],\n",
      "        [0.2254],\n",
      "        [0.2131],\n",
      "        [0.2215],\n",
      "        [0.2328],\n",
      "        [0.2430],\n",
      "        [0.2842],\n",
      "        [0.3061],\n",
      "        [0.3191],\n",
      "        [0.3230],\n",
      "        [0.3347],\n",
      "        [0.3666],\n",
      "        [0.3719],\n",
      "        [0.4202],\n",
      "        [0.4908],\n",
      "        [0.5670],\n",
      "        [0.6127],\n",
      "        [0.6311],\n",
      "        [0.6816],\n",
      "        [0.7165]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 20, 4), got [1, 16, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MUNEEB~1\\AppData\\Local\\Temp/ipykernel_9572/4090673582.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(inp_scaled.reshape(-1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(type(inp_scaled))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnet2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\MUNEEB~1\\AppData\\Local\\Temp/ipykernel_9572/3358299725.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#print(self.hidden.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#print(self.hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m#c.detach_()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m--> 607\u001b[1;33m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[0;32m    608\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n\u001b[0;32m    609\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    221\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 20, 4), got [1, 16, 4]"
     ]
    }
   ],
   "source": [
    "inp = torch.Tensor([[0.2457, 0.2482, 0.2410, 0.2459, 0.2525, 0.2585, 0.2825, 0.2953, 0.3029,0.3052, 0.3120, 0.3306, 0.3337, 0.3619, 0.4031, 0.4476, 0.4743, 0.4850, 0.5145, 0.5349]])\n",
    "#print()\n",
    "inp_scaled = torch.from_numpy(scaler.transform(inp.reshape(20,1)))\n",
    "print(inp_scaled)\n",
    "#print(inp_scaled.reshape(-1))\n",
    "#print(type(inp_scaled))\n",
    "net2(inp_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
