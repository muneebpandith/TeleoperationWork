{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e847bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cardinality=20\n",
    "output_cardinality=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0738c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection as sk\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81495b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning as pl\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ead879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f09a04d4fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940b1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc51aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2683d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pl.seed_everything(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac4eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.filenames = os.listdir(DATA_PATH)\n",
    "        self.filenames = [filename for filename in self.filenames if filename.endswith(\".csv\")]\n",
    "        self.len_filenames = len(self.filenames)\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.data = []\n",
    "        for i in range(len(self.filenames)):\n",
    "            self.data.append(self.read_data(i))\n",
    "        return self.data\n",
    "    \n",
    "    def read_data(self,i):\n",
    "        return pd.read_csv(self.data_path+\"/\"+self.filenames[i])\n",
    "    \n",
    "\n",
    "    def process_data(self, scaler, input_cardinality=20, output_cardinality=10, cols=['ThetaXHG']):\n",
    "        self.datasamples = self.get_data()\n",
    "        X = []\n",
    "        y = []\n",
    "        X_scaled = []\n",
    "        y_scaled = []\n",
    "        for i, datasample in enumerate(self.datasamples):            \n",
    "            #for j in range(datasample[cols].shape[0]-output_cardinality):\n",
    "            #    print('Input' +str(j)+ ' to '+str(j+input_cardinality) + 'Output: '+str(j+input_cardinality)+ ' to '+str(j+input_cardinality+output_cardinality))\n",
    "            #Append to X (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto input_cardinality]]\n",
    "            #Append to Y (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto output_cardinality]]\n",
    "            #start of Y will be ahead of end of X by 1\n",
    "            \n",
    "            for j in range(datasample.shape[0]-output_cardinality-input_cardinality+1):\n",
    "                X.append(datasample[cols].iloc[j:j+input_cardinality].to_numpy())\n",
    "                y.append(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy())\n",
    "                X_scaled.append(scaler.transform(datasample[cols].iloc[j:j+input_cardinality].to_numpy()))\n",
    "                y_scaled.append(scaler.transform(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy()))\n",
    "        \n",
    "        #return self.datasamples, X, y\n",
    "        #print(np.array(X).shape,np.array(y).shape,np.array(X_scaled).shape, np.array(y_scaled).shape)\n",
    "        return self.datasamples, np.array(X), np.array(y), np.array(X_scaled), np.array(y_scaled)\n",
    "    \n",
    "    \n",
    "    def train_test_split(self, X, y, split={'train':0.8,'val':0.2, 'test':0.5}):\n",
    "        #print(X)\n",
    "        self.X_train, self.X_val_test, self.y_train, self.y_val_test = sk.train_test_split(X, y, test_size=split['val'] , random_state=43)\n",
    "        self.X_test, self.X_val, self.y_test, self.y_val = sk.train_test_split(self.X_val_test, self.y_val_test, test_size=split['test'] , random_state=43)\n",
    "        \n",
    "        return (self.X_train, self.y_train), (self.X_val, self.X_test), (self.X_test, self.y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getscaler(self,cols):\n",
    "        self.datasamples = self.get_data()       \n",
    "        for i, sample in enumerate(self.datasamples):\n",
    "            if i ==0:\n",
    "                features = pd.DataFrame(sample[cols])\n",
    "            else:\n",
    "                features = pd.DataFrame.append(features, sample[cols]) #sample[cols]\n",
    "   \n",
    "        #scaling\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        scaler = scaler.fit(features)\n",
    "        features_scaled = pd.DataFrame(scaler.transform(features), index = features.index, columns = cols)\n",
    "        return scaler\n",
    "    \n",
    "        \"\"\"        \n",
    "        #convert back to datasamples\n",
    "        #save to X\n",
    "        #save to X_scaled\n",
    "        \n",
    "        \n",
    "        #Split treain test and validation\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            self.datasamples = []\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        return features, features_scaled\n",
    "        \"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5167df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329ba98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASET(Dataset):\n",
    "    #convert to pytorch dataset\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, output = self.X[idx], self.y[idx]\n",
    "        return (torch.from_numpy(sequence.reshape(-1)), torch.from_numpy(output.reshape(-1)))\n",
    "        #return dict(sequence=torch.tensor(sequence.reshape(-1),dtype=torch.float64), label=torch.tensor(output.reshape(-1),dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d8d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4722168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASETLOADER():\n",
    "    def __init__(self, data, batchsize, bs_val):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batchsize = batchsize\n",
    "        self.train_dataset = THETADATASET(self.data.X_train, self.data.y_train)\n",
    "        self.val_dataset = THETADATASET(self.data.X_val, self.data.y_val)\n",
    "        self.test_dataset = THETADATASET(self.data.X_test, self.data.y_test)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size = self.batchsize, shuffle= False, num_workers=0, worker_init_fn=seed_worker,generator=g)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size = bs_val, shuffle= False, num_workers=0,  worker_init_fn=seed_worker,generator=g)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size = 1, shuffle= False, num_workers=0,  worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c884b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1aa2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847ddc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446a3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders={}\n",
    "#dataloaders['train'], dataloaders['val'] = data_loaded.train_dataloader, data_loaded.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5411a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor f, o in tqdm(dataloaders['train']):\\n    #print(f[0].shape)\\n    break\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for f, o in tqdm(dataloaders['train']):\n",
    "    #print(f[0].shape)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3389745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(a,first['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5a811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea08fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc39c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(DATA_LOADED, model, criterion, optimizer, num_epochs=25):\n",
    "    #train_model(DATA_LOADED, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 0\n",
    "    dataloaders ={}\n",
    "    dataloaders['train'], dataloaders['val'] = DATA_LOADED.train_dataloader, DATA_LOADED.val_dataloader\n",
    "\n",
    "    #TR_ACCURACY=[]\n",
    "    TR_LOSS=[]\n",
    "    #VAL_ACCURACY=[]\n",
    "    VAL_LOSS=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        model.reset_hidden_states()\n",
    "        #model.reset_hidden_states()\n",
    "        #print(model.hidden)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            #running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #if phase == 'train':\n",
    "            #    scheduler.step()\n",
    "\n",
    "            #epoch_loss = running_loss / DATA_LOADED.dataset_sizes[phase]\n",
    "            epoch_loss = running_loss / dataset_size[phase]\n",
    "            \n",
    "            #epoch_acc = running_corrects.double() / DATA.dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "\n",
    "            if phase == 'train':\n",
    "                #  print(\"Training\")\n",
    "                #  TR_ACCURACY.append(epoch_acc)\n",
    "                TR_LOSS.append(epoch_loss)\n",
    "            else:\n",
    "                print(\"Valuation\")\n",
    "                #VAL_ACCURACY.append(epoch_acc)\n",
    "                VAL_LOSS.append(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return TR_LOSS, VAL_LOSS, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c88e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHALLEABLELSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def reset_hidden_states(self):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        #lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        #self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        #lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= ht[-1]\n",
    "        #lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin0_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63a3a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def reset_hidden_states(self,bs):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, bs, self.lstm_units).double(), torch.zeros(1*self.num_layers, bs, self.lstm_units).double())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= h.detach()[-1]\n",
    "        lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        #lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin1_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4360dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd04f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd86f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2414e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "batch_size = 16\n",
    "bs_val = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c141d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookahead10.ipynb  Lookahead2.ipynb  Lookahead6.ipynb  prediction.ipynb\r\n",
      "Lookahead15.ipynb  Lookahead3.ipynb  Lookahead7.ipynb  readme.md\r\n",
      "Lookahead1.ipynb   Lookahead4.ipynb  Lookahead8.ipynb\r\n",
      "Lookahead20.ipynb  Lookahead5.ipynb  Lookahead9.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3f936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../DATASET/TRANSORMER_DATA\"\n",
    "data = DATA(DATA_PATH)\n",
    "cols=['ThetaXHG']\n",
    "scaler = data.getscaler(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "957024ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, X, y, X_scaled, y_scaled = data.process_data(scaler, input_cardinality=input_cardinality, output_cardinality=output_cardinality, cols=['ThetaXHG'])\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = data.train_test_split(X_scaled,y_scaled,  split={'train':0.8,'val':0.2, 'test':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3a02b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924, 10, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "850ef43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toor/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "net = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "net = net.to(device)\n",
    "net = net.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebb4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "598b88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first['sequence'].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0cffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaded = THETADATASETLOADER(data, batchsize=batch_size, bs_val = bs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccc3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ffaa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size={}\n",
    "dataset_size['train'] =X_train.shape[0]\n",
    "dataset_size['val'] =X_val.shape[0]\n",
    "dataset_size['test'] =X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3db471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_LOSS = []\n",
    "VAL_LOSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d89481f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "722/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09609bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in range(epochs):\\n    net.reset_hidden_states(bs=batch_size)\\n    it = iter(data_loaded.train_dataloader)\\n    it_val = iter(data_loaded.train_dataloader)\\n    print('Epoch'+str(i))\\n    c = 0\\n    tr_loss = 0.0\\n    val_loss = 0.0\\n    \\n    while c < int(X_train.shape[0]/batch_size): \\n        with torch.set_grad_enabled(True):\\n            #print(c)\\n            c = c+1\\n            item = next(it)\\n            outputs = net(item[0])\\n            tr_loss = tr_loss + criterion(outputs, item[1])\\n    \\n    TR_LOSS.append(tr_loss)\\n    print(tr_loss)\\n    optimizer.zero_grad()\\n    tr_loss.backward()\\n    optimizer.step()\\n    \\n    net.reset_hidden_states(bs=bs_val)\\n    #print(net.hidden[0].shape)\\n    while c < int(X_val.shape[0]/bs_val):\\n        net.eval()\\n        with torch.set_grad_enabled(False):\\n            #print(c)\\n            c = c+1\\n            item = next(it_val)\\n            outputs = net(item[0])\\n            val_loss = val_loss + criterion(outputs, item[1])\\n    VAL_LOSS.append(val_loss)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "\"\"\"for i in range(epochs):\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    print('Epoch'+str(i))\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            outputs = net(item[0])\n",
    "            tr_loss = tr_loss + criterion(outputs, item[1])\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            outputs = net(item[0])\n",
    "            val_loss = val_loss + criterion(outputs, item[1])\n",
    "    VAL_LOSS.append(val_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "651ccb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.8766, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch0 >> Training Loss: tensor(14.8766, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.7740, dtype=torch.float64) <<\n",
      "tensor(8.8932, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch1 >> Training Loss: tensor(8.8932, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.5146, dtype=torch.float64) <<\n",
      "tensor(6.4588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch2 >> Training Loss: tensor(6.4588, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4959, dtype=torch.float64) <<\n",
      "tensor(5.6699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch3 >> Training Loss: tensor(5.6699, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.5931, dtype=torch.float64) <<\n",
      "tensor(5.9238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch4 >> Training Loss: tensor(5.9238, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.5969, dtype=torch.float64) <<\n",
      "tensor(5.8367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch5 >> Training Loss: tensor(5.8367, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4779, dtype=torch.float64) <<\n",
      "tensor(5.0074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch6 >> Training Loss: tensor(5.0074, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4072, dtype=torch.float64) <<\n",
      "tensor(4.6413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch7 >> Training Loss: tensor(4.6413, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3485, dtype=torch.float64) <<\n",
      "tensor(4.1400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch8 >> Training Loss: tensor(4.1400, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2973, dtype=torch.float64) <<\n",
      "tensor(3.4464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch9 >> Training Loss: tensor(3.4464, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3093, dtype=torch.float64) <<\n",
      "tensor(3.3119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch10 >> Training Loss: tensor(3.3119, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2294, dtype=torch.float64) <<\n",
      "tensor(2.6132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch11 >> Training Loss: tensor(2.6132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1846, dtype=torch.float64) <<\n",
      "tensor(2.2786, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch12 >> Training Loss: tensor(2.2786, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1673, dtype=torch.float64) <<\n",
      "tensor(2.0511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch13 >> Training Loss: tensor(2.0511, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1871, dtype=torch.float64) <<\n",
      "tensor(2.2844, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch14 >> Training Loss: tensor(2.2844, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1502, dtype=torch.float64) <<\n",
      "tensor(1.8103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch15 >> Training Loss: tensor(1.8103, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1414, dtype=torch.float64) <<\n",
      "tensor(1.6133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch16 >> Training Loss: tensor(1.6133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1023, dtype=torch.float64) <<\n",
      "tensor(1.1447, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch17 >> Training Loss: tensor(1.1447, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1087, dtype=torch.float64) <<\n",
      "tensor(1.0958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch18 >> Training Loss: tensor(1.0958, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0873, dtype=torch.float64) <<\n",
      "tensor(0.8981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch19 >> Training Loss: tensor(0.8981, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0732, dtype=torch.float64) <<\n",
      "tensor(0.8624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch20 >> Training Loss: tensor(0.8624, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0682, dtype=torch.float64) <<\n",
      "tensor(0.8454, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch21 >> Training Loss: tensor(0.8454, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0641, dtype=torch.float64) <<\n",
      "tensor(0.7290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch22 >> Training Loss: tensor(0.7290, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0762, dtype=torch.float64) <<\n",
      "tensor(0.7756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch23 >> Training Loss: tensor(0.7756, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0628, dtype=torch.float64) <<\n",
      "tensor(0.6929, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch24 >> Training Loss: tensor(0.6929, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0594, dtype=torch.float64) <<\n",
      "tensor(0.7272, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch25 >> Training Loss: tensor(0.7272, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0583, dtype=torch.float64) <<\n",
      "tensor(0.7018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch26 >> Training Loss: tensor(0.7018, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0673, dtype=torch.float64) <<\n",
      "tensor(0.7443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch27 >> Training Loss: tensor(0.7443, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0623, dtype=torch.float64) <<\n",
      "tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch28 >> Training Loss: tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0608, dtype=torch.float64) <<\n",
      "tensor(0.7592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch29 >> Training Loss: tensor(0.7592, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0550, dtype=torch.float64) <<\n",
      "tensor(0.6869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch30 >> Training Loss: tensor(0.6869, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0602, dtype=torch.float64) <<\n",
      "tensor(0.6931, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch31 >> Training Loss: tensor(0.6931, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0480, dtype=torch.float64) <<\n",
      "tensor(0.6068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch32 >> Training Loss: tensor(0.6068, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0463, dtype=torch.float64) <<\n",
      "tensor(0.6203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch33 >> Training Loss: tensor(0.6203, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0468, dtype=torch.float64) <<\n",
      "tensor(0.5756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch34 >> Training Loss: tensor(0.5756, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0512, dtype=torch.float64) <<\n",
      "tensor(0.5925, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch35 >> Training Loss: tensor(0.5925, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0425, dtype=torch.float64) <<\n",
      "tensor(0.5656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch36 >> Training Loss: tensor(0.5656, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0409, dtype=torch.float64) <<\n",
      "tensor(0.5543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch37 >> Training Loss: tensor(0.5543, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0444, dtype=torch.float64) <<\n",
      "tensor(0.5300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch38 >> Training Loss: tensor(0.5300, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0407, dtype=torch.float64) <<\n",
      "tensor(0.5013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch39 >> Training Loss: tensor(0.5013, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0364, dtype=torch.float64) <<\n",
      "tensor(0.5011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch40 >> Training Loss: tensor(0.5011, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0367, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch41 >> Training Loss: tensor(0.4823, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0416, dtype=torch.float64) <<\n",
      "tensor(0.5063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch42 >> Training Loss: tensor(0.5063, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0371, dtype=torch.float64) <<\n",
      "tensor(0.4979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch43 >> Training Loss: tensor(0.4979, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0369, dtype=torch.float64) <<\n",
      "tensor(0.5038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch44 >> Training Loss: tensor(0.5038, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0396, dtype=torch.float64) <<\n",
      "tensor(0.5007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch45 >> Training Loss: tensor(0.5007, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0364, dtype=torch.float64) <<\n",
      "tensor(0.4826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch46 >> Training Loss: tensor(0.4826, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0350, dtype=torch.float64) <<\n",
      "tensor(0.4872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch47 >> Training Loss: tensor(0.4872, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0366, dtype=torch.float64) <<\n",
      "tensor(0.4767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch48 >> Training Loss: tensor(0.4767, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0379, dtype=torch.float64) <<\n",
      "tensor(0.4813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch49 >> Training Loss: tensor(0.4813, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0354, dtype=torch.float64) <<\n",
      "tensor(0.4837, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch50 >> Training Loss: tensor(0.4837, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0360, dtype=torch.float64) <<\n",
      "tensor(0.4778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch51 >> Training Loss: tensor(0.4778, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0387, dtype=torch.float64) <<\n",
      "tensor(0.4814, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch52 >> Training Loss: tensor(0.4814, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0353, dtype=torch.float64) <<\n",
      "tensor(0.4716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch53 >> Training Loss: tensor(0.4716, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0347, dtype=torch.float64) <<\n",
      "tensor(0.4688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch54 >> Training Loss: tensor(0.4688, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0369, dtype=torch.float64) <<\n",
      "tensor(0.4667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch55 >> Training Loss: tensor(0.4667, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0347, dtype=torch.float64) <<\n",
      "tensor(0.4597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch56 >> Training Loss: tensor(0.4597, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0338, dtype=torch.float64) <<\n",
      "tensor(0.4620, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch57 >> Training Loss: tensor(0.4620, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0356, dtype=torch.float64) <<\n",
      "tensor(0.4608, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch58 >> Training Loss: tensor(0.4608, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0344, dtype=torch.float64) <<\n",
      "tensor(0.4573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch59 >> Training Loss: tensor(0.4573, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch60 >> Training Loss: tensor(0.4585, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0349, dtype=torch.float64) <<\n",
      "tensor(0.4552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch61 >> Training Loss: tensor(0.4552, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0338, dtype=torch.float64) <<\n",
      "tensor(0.4508, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch62 >> Training Loss: tensor(0.4508, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0330, dtype=torch.float64) <<\n",
      "tensor(0.4514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch63 >> Training Loss: tensor(0.4514, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0347, dtype=torch.float64) <<\n",
      "tensor(0.4506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch64 >> Training Loss: tensor(0.4506, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0337, dtype=torch.float64) <<\n",
      "tensor(0.4490, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch65 >> Training Loss: tensor(0.4490, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0332, dtype=torch.float64) <<\n",
      "tensor(0.4500, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch66 >> Training Loss: tensor(0.4500, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0348, dtype=torch.float64) <<\n",
      "tensor(0.4496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch67 >> Training Loss: tensor(0.4496, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4467, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch68 >> Training Loss: tensor(0.4467, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0331, dtype=torch.float64) <<\n",
      "tensor(0.4456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch69 >> Training Loss: tensor(0.4456, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0343, dtype=torch.float64) <<\n",
      "tensor(0.4458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch70 >> Training Loss: tensor(0.4458, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0328, dtype=torch.float64) <<\n",
      "tensor(0.4447, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch71 >> Training Loss: tensor(0.4447, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0332, dtype=torch.float64) <<\n",
      "tensor(0.4434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch72 >> Training Loss: tensor(0.4434, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0335, dtype=torch.float64) <<\n",
      "tensor(0.4431, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch73 >> Training Loss: tensor(0.4431, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0324, dtype=torch.float64) <<\n",
      "tensor(0.4427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch74 >> Training Loss: tensor(0.4427, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0335, dtype=torch.float64) <<\n",
      "tensor(0.4412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch75 >> Training Loss: tensor(0.4412, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4394, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch76 >> Training Loss: tensor(0.4394, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch77 >> Training Loss: tensor(0.4384, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0332, dtype=torch.float64) <<\n",
      "tensor(0.4381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch78 >> Training Loss: tensor(0.4381, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0321, dtype=torch.float64) <<\n",
      "tensor(0.4376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch79 >> Training Loss: tensor(0.4376, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0332, dtype=torch.float64) <<\n",
      "tensor(0.4365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch80 >> Training Loss: tensor(0.4365, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0322, dtype=torch.float64) <<\n",
      "tensor(0.4350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch81 >> Training Loss: tensor(0.4350, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch82 >> Training Loss: tensor(0.4336, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch83 >> Training Loss: tensor(0.4326, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0319, dtype=torch.float64) <<\n",
      "tensor(0.4319, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch84 >> Training Loss: tensor(0.4319, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0327, dtype=torch.float64) <<\n",
      "tensor(0.4313, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch85 >> Training Loss: tensor(0.4313, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4307, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch86 >> Training Loss: tensor(0.4307, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0330, dtype=torch.float64) <<\n",
      "tensor(0.4303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch87 >> Training Loss: tensor(0.4303, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch88 >> Training Loss: tensor(0.4305, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0341, dtype=torch.float64) <<\n",
      "tensor(0.4326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch89 >> Training Loss: tensor(0.4326, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0310, dtype=torch.float64) <<\n",
      "tensor(0.4399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch90 >> Training Loss: tensor(0.4399, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0397, dtype=torch.float64) <<\n",
      "tensor(0.4624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch91 >> Training Loss: tensor(0.4624, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0372, dtype=torch.float64) <<\n",
      "tensor(0.5352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch92 >> Training Loss: tensor(0.5352, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0744, dtype=torch.float64) <<\n",
      "tensor(0.7205, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch93 >> Training Loss: tensor(0.7205, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0911, dtype=torch.float64) <<\n",
      "tensor(1.1237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch94 >> Training Loss: tensor(1.1237, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1105, dtype=torch.float64) <<\n",
      "tensor(1.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch95 >> Training Loss: tensor(1.0144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0357, dtype=torch.float64) <<\n",
      "tensor(0.5166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch96 >> Training Loss: tensor(0.5166, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0387, dtype=torch.float64) <<\n",
      "tensor(0.5560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch97 >> Training Loss: tensor(0.5560, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0785, dtype=torch.float64) <<\n",
      "tensor(0.7536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch98 >> Training Loss: tensor(0.7536, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0314, dtype=torch.float64) <<\n",
      "tensor(0.4492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch99 >> Training Loss: tensor(0.4492, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0414, dtype=torch.float64) <<\n",
      "tensor(0.5889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch100 >> Training Loss: tensor(0.5889, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0508, dtype=torch.float64) <<\n",
      "tensor(0.5413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch101 >> Training Loss: tensor(0.5413, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0394, dtype=torch.float64) <<\n",
      "tensor(0.4627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch102 >> Training Loss: tensor(0.4627, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0399, dtype=torch.float64) <<\n",
      "tensor(0.5681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch103 >> Training Loss: tensor(0.5681, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0317, dtype=torch.float64) <<\n",
      "tensor(0.4311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch104 >> Training Loss: tensor(0.4311, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0517, dtype=torch.float64) <<\n",
      "tensor(0.5503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch105 >> Training Loss: tensor(0.5503, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0314, dtype=torch.float64) <<\n",
      "tensor(0.4367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch106 >> Training Loss: tensor(0.4367, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0355, dtype=torch.float64) <<\n",
      "tensor(0.5119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch107 >> Training Loss: tensor(0.5119, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0364, dtype=torch.float64) <<\n",
      "tensor(0.4475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch108 >> Training Loss: tensor(0.4475, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0420, dtype=torch.float64) <<\n",
      "tensor(0.4826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch109 >> Training Loss: tensor(0.4826, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0319, dtype=torch.float64) <<\n",
      "tensor(0.4567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch110 >> Training Loss: tensor(0.4567, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0321, dtype=torch.float64) <<\n",
      "tensor(0.4621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch111 >> Training Loss: tensor(0.4621, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0384, dtype=torch.float64) <<\n",
      "tensor(0.4591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch112 >> Training Loss: tensor(0.4591, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0365, dtype=torch.float64) <<\n",
      "tensor(0.4481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch113 >> Training Loss: tensor(0.4481, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0319, dtype=torch.float64) <<\n",
      "tensor(0.4589, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch114 >> Training Loss: tensor(0.4589, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0312, dtype=torch.float64) <<\n",
      "tensor(0.4398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch115 >> Training Loss: tensor(0.4398, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0381, dtype=torch.float64) <<\n",
      "tensor(0.4565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch116 >> Training Loss: tensor(0.4565, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0338, dtype=torch.float64) <<\n",
      "tensor(0.4335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch117 >> Training Loss: tensor(0.4335, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0316, dtype=torch.float64) <<\n",
      "tensor(0.4531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch118 >> Training Loss: tensor(0.4531, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0312, dtype=torch.float64) <<\n",
      "tensor(0.4298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch119 >> Training Loss: tensor(0.4298, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0369, dtype=torch.float64) <<\n",
      "tensor(0.4493, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch120 >> Training Loss: tensor(0.4493, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0321, dtype=torch.float64) <<\n",
      "tensor(0.4271, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch121 >> Training Loss: tensor(0.4271, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4447, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch122 >> Training Loss: tensor(0.4447, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0317, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch123 >> Training Loss: tensor(0.4256, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0356, dtype=torch.float64) <<\n",
      "tensor(0.4401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch124 >> Training Loss: tensor(0.4401, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0312, dtype=torch.float64) <<\n",
      "tensor(0.4248, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch125 >> Training Loss: tensor(0.4248, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0307, dtype=torch.float64) <<\n",
      "tensor(0.4349, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch126 >> Training Loss: tensor(0.4349, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch127 >> Training Loss: tensor(0.4246, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0339, dtype=torch.float64) <<\n",
      "tensor(0.4297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch128 >> Training Loss: tensor(0.4297, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0306, dtype=torch.float64) <<\n",
      "tensor(0.4246, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch129 >> Training Loss: tensor(0.4246, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4245, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch130 >> Training Loss: tensor(0.4245, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0331, dtype=torch.float64) <<\n",
      "tensor(0.4244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch131 >> Training Loss: tensor(0.4244, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch132 >> Training Loss: tensor(0.4198, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0303, dtype=torch.float64) <<\n",
      "tensor(0.4231, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch133 >> Training Loss: tensor(0.4231, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch134 >> Training Loss: tensor(0.4161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch135 >> Training Loss: tensor(0.4204, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch136 >> Training Loss: tensor(0.4142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0300, dtype=torch.float64) <<\n",
      "tensor(0.4155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch137 >> Training Loss: tensor(0.4155, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0318, dtype=torch.float64) <<\n",
      "tensor(0.4135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch138 >> Training Loss: tensor(0.4135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch139 >> Training Loss: tensor(0.4102, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0297, dtype=torch.float64) <<\n",
      "tensor(0.4116, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch140 >> Training Loss: tensor(0.4116, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0306, dtype=torch.float64) <<\n",
      "tensor(0.4067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch141 >> Training Loss: tensor(0.4067, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0312, dtype=torch.float64) <<\n",
      "tensor(0.4068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch142 >> Training Loss: tensor(0.4068, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0294, dtype=torch.float64) <<\n",
      "tensor(0.4055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch143 >> Training Loss: tensor(0.4055, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.4013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch144 >> Training Loss: tensor(0.4013, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch145 >> Training Loss: tensor(0.4016, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0290, dtype=torch.float64) <<\n",
      "tensor(0.3998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch146 >> Training Loss: tensor(0.3998, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0297, dtype=torch.float64) <<\n",
      "tensor(0.3957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch147 >> Training Loss: tensor(0.3957, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.3944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch148 >> Training Loss: tensor(0.3944, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0285, dtype=torch.float64) <<\n",
      "tensor(0.3940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch149 >> Training Loss: tensor(0.3940, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0301, dtype=torch.float64) <<\n",
      "tensor(0.3916, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch150 >> Training Loss: tensor(0.3916, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0283, dtype=torch.float64) <<\n",
      "tensor(0.3879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch151 >> Training Loss: tensor(0.3879, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0288, dtype=torch.float64) <<\n",
      "tensor(0.3845, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch152 >> Training Loss: tensor(0.3845, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0284, dtype=torch.float64) <<\n",
      "tensor(0.3819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch153 >> Training Loss: tensor(0.3819, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0278, dtype=torch.float64) <<\n",
      "tensor(0.3798, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch154 >> Training Loss: tensor(0.3798, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0290, dtype=torch.float64) <<\n",
      "tensor(0.3793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch155 >> Training Loss: tensor(0.3793, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0271, dtype=torch.float64) <<\n",
      "tensor(0.3859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch156 >> Training Loss: tensor(0.3859, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0388, dtype=torch.float64) <<\n",
      "tensor(0.4375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch157 >> Training Loss: tensor(0.4375, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0659, dtype=torch.float64) <<\n",
      "tensor(0.8160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch158 >> Training Loss: tensor(0.8160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2816, dtype=torch.float64) <<\n",
      "tensor(2.4579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch159 >> Training Loss: tensor(2.4579, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4767, dtype=torch.float64) <<\n",
      "tensor(4.8022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch160 >> Training Loss: tensor(4.8022, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0298, dtype=torch.float64) <<\n",
      "tensor(0.4160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch161 >> Training Loss: tensor(0.4160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3546, dtype=torch.float64) <<\n",
      "tensor(3.0727, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch162 >> Training Loss: tensor(3.0727, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1069, dtype=torch.float64) <<\n",
      "tensor(0.9789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch163 >> Training Loss: tensor(0.9789, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch164 >> Training Loss: tensor(1.0934, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1532, dtype=torch.float64) <<\n",
      "tensor(1.9161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch165 >> Training Loss: tensor(1.9161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0929, dtype=torch.float64) <<\n",
      "tensor(1.2382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch166 >> Training Loss: tensor(1.2382, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.9156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch167 >> Training Loss: tensor(0.9156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1566, dtype=torch.float64) <<\n",
      "tensor(1.3984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch168 >> Training Loss: tensor(1.3984, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1576, dtype=torch.float64) <<\n",
      "tensor(1.3883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch169 >> Training Loss: tensor(1.3883, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0784, dtype=torch.float64) <<\n",
      "tensor(0.7705, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch170 >> Training Loss: tensor(0.7705, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0504, dtype=torch.float64) <<\n",
      "tensor(0.6482, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch171 >> Training Loss: tensor(0.6482, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0783, dtype=torch.float64) <<\n",
      "tensor(0.9668, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch172 >> Training Loss: tensor(0.9668, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0630, dtype=torch.float64) <<\n",
      "tensor(0.7961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch173 >> Training Loss: tensor(0.7961, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0391, dtype=torch.float64) <<\n",
      "tensor(0.5292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch174 >> Training Loss: tensor(0.5292, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0760, dtype=torch.float64) <<\n",
      "tensor(0.8644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch175 >> Training Loss: tensor(0.8644, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0790, dtype=torch.float64) <<\n",
      "tensor(0.8975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch176 >> Training Loss: tensor(0.8975, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0431, dtype=torch.float64) <<\n",
      "tensor(0.5700, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch177 >> Training Loss: tensor(0.5700, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0480, dtype=torch.float64) <<\n",
      "tensor(0.6366, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch178 >> Training Loss: tensor(0.6366, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0572, dtype=torch.float64) <<\n",
      "tensor(0.7310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch179 >> Training Loss: tensor(0.7310, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0453, dtype=torch.float64) <<\n",
      "tensor(0.5866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch180 >> Training Loss: tensor(0.5866, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0456, dtype=torch.float64) <<\n",
      "tensor(0.5254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch181 >> Training Loss: tensor(0.5254, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0647, dtype=torch.float64) <<\n",
      "tensor(0.6464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch182 >> Training Loss: tensor(0.6464, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0720, dtype=torch.float64) <<\n",
      "tensor(0.6994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch183 >> Training Loss: tensor(0.6994, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0579, dtype=torch.float64) <<\n",
      "tensor(0.5990, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch184 >> Training Loss: tensor(0.5990, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0445, dtype=torch.float64) <<\n",
      "tensor(0.5287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch185 >> Training Loss: tensor(0.5287, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0442, dtype=torch.float64) <<\n",
      "tensor(0.5678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch186 >> Training Loss: tensor(0.5678, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0441, dtype=torch.float64) <<\n",
      "tensor(0.5768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch187 >> Training Loss: tensor(0.5768, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0381, dtype=torch.float64) <<\n",
      "tensor(0.5044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch188 >> Training Loss: tensor(0.5044, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0398, dtype=torch.float64) <<\n",
      "tensor(0.4975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch189 >> Training Loss: tensor(0.4975, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0483, dtype=torch.float64) <<\n",
      "tensor(0.5654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch190 >> Training Loss: tensor(0.5654, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0457, dtype=torch.float64) <<\n",
      "tensor(0.5455, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch191 >> Training Loss: tensor(0.5455, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0378, dtype=torch.float64) <<\n",
      "tensor(0.4863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch192 >> Training Loss: tensor(0.4863, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0377, dtype=torch.float64) <<\n",
      "tensor(0.5023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch193 >> Training Loss: tensor(0.5023, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0390, dtype=torch.float64) <<\n",
      "tensor(0.5175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch194 >> Training Loss: tensor(0.5175, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0372, dtype=torch.float64) <<\n",
      "tensor(0.4824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch195 >> Training Loss: tensor(0.4824, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0392, dtype=torch.float64) <<\n",
      "tensor(0.4709, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch196 >> Training Loss: tensor(0.4709, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0447, dtype=torch.float64) <<\n",
      "tensor(0.4995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch197 >> Training Loss: tensor(0.4995, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0450, dtype=torch.float64) <<\n",
      "tensor(0.5013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch198 >> Training Loss: tensor(0.5013, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0398, dtype=torch.float64) <<\n",
      "tensor(0.4736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch199 >> Training Loss: tensor(0.4736, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0366, dtype=torch.float64) <<\n",
      "tensor(0.4698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch200 >> Training Loss: tensor(0.4698, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0363, dtype=torch.float64) <<\n",
      "tensor(0.4818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch201 >> Training Loss: tensor(0.4818, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0354, dtype=torch.float64) <<\n",
      "tensor(0.4704, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch202 >> Training Loss: tensor(0.4704, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0354, dtype=torch.float64) <<\n",
      "tensor(0.4559, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch203 >> Training Loss: tensor(0.4559, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0381, dtype=torch.float64) <<\n",
      "tensor(0.4674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch204 >> Training Loss: tensor(0.4674, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0390, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch205 >> Training Loss: tensor(0.4732, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0364, dtype=torch.float64) <<\n",
      "tensor(0.4589, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch206 >> Training Loss: tensor(0.4589, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0348, dtype=torch.float64) <<\n",
      "tensor(0.4571, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch207 >> Training Loss: tensor(0.4571, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0348, dtype=torch.float64) <<\n",
      "tensor(0.4636, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch208 >> Training Loss: tensor(0.4636, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0346, dtype=torch.float64) <<\n",
      "tensor(0.4560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch209 >> Training Loss: tensor(0.4560, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0354, dtype=torch.float64) <<\n",
      "tensor(0.4492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch210 >> Training Loss: tensor(0.4492, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0374, dtype=torch.float64) <<\n",
      "tensor(0.4552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch211 >> Training Loss: tensor(0.4552, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0377, dtype=torch.float64) <<\n",
      "tensor(0.4565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch212 >> Training Loss: tensor(0.4565, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0359, dtype=torch.float64) <<\n",
      "tensor(0.4499, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch213 >> Training Loss: tensor(0.4499, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0344, dtype=torch.float64) <<\n",
      "tensor(0.4498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch214 >> Training Loss: tensor(0.4498, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0340, dtype=torch.float64) <<\n",
      "tensor(0.4524, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch215 >> Training Loss: tensor(0.4524, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0339, dtype=torch.float64) <<\n",
      "tensor(0.4478, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch216 >> Training Loss: tensor(0.4478, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0345, dtype=torch.float64) <<\n",
      "tensor(0.4445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch217 >> Training Loss: tensor(0.4445, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0356, dtype=torch.float64) <<\n",
      "tensor(0.4475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch218 >> Training Loss: tensor(0.4475, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0354, dtype=torch.float64) <<\n",
      "tensor(0.4471, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch219 >> Training Loss: tensor(0.4471, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0343, dtype=torch.float64) <<\n",
      "tensor(0.4438, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch220 >> Training Loss: tensor(0.4438, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0335, dtype=torch.float64) <<\n",
      "tensor(0.4449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch221 >> Training Loss: tensor(0.4449, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch222 >> Training Loss: tensor(0.4453, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0336, dtype=torch.float64) <<\n",
      "tensor(0.4422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch223 >> Training Loss: tensor(0.4422, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0344, dtype=torch.float64) <<\n",
      "tensor(0.4415, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch224 >> Training Loss: tensor(0.4415, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0350, dtype=torch.float64) <<\n",
      "tensor(0.4427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch225 >> Training Loss: tensor(0.4427, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0346, dtype=torch.float64) <<\n",
      "tensor(0.4414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch226 >> Training Loss: tensor(0.4414, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0338, dtype=torch.float64) <<\n",
      "tensor(0.4403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch227 >> Training Loss: tensor(0.4403, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0333, dtype=torch.float64) <<\n",
      "tensor(0.4412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch228 >> Training Loss: tensor(0.4412, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0333, dtype=torch.float64) <<\n",
      "tensor(0.4406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch229 >> Training Loss: tensor(0.4406, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0336, dtype=torch.float64) <<\n",
      "tensor(0.4390, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch230 >> Training Loss: tensor(0.4390, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0341, dtype=torch.float64) <<\n",
      "tensor(0.4392, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch231 >> Training Loss: tensor(0.4392, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0342, dtype=torch.float64) <<\n",
      "tensor(0.4392, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch232 >> Training Loss: tensor(0.4392, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0337, dtype=torch.float64) <<\n",
      "tensor(0.4381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch233 >> Training Loss: tensor(0.4381, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0331, dtype=torch.float64) <<\n",
      "tensor(0.4380, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch234 >> Training Loss: tensor(0.4380, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch235 >> Training Loss: tensor(0.4382, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0331, dtype=torch.float64) <<\n",
      "tensor(0.4374, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch236 >> Training Loss: tensor(0.4374, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4369, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch237 >> Training Loss: tensor(0.4369, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0337, dtype=torch.float64) <<\n",
      "tensor(0.4371, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch238 >> Training Loss: tensor(0.4371, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0335, dtype=torch.float64) <<\n",
      "tensor(0.4365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch239 >> Training Loss: tensor(0.4365, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0332, dtype=torch.float64) <<\n",
      "tensor(0.4360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch240 >> Training Loss: tensor(0.4360, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4361, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch241 >> Training Loss: tensor(0.4361, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4358, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch242 >> Training Loss: tensor(0.4358, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0332, dtype=torch.float64) <<\n",
      "tensor(0.4353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch243 >> Training Loss: tensor(0.4353, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch244 >> Training Loss: tensor(0.4353, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch245 >> Training Loss: tensor(0.4350, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0331, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4346, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch246 >> Training Loss: tensor(0.4346, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0328, dtype=torch.float64) <<\n",
      "tensor(0.4344, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch247 >> Training Loss: tensor(0.4344, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0327, dtype=torch.float64) <<\n",
      "tensor(0.4343, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch248 >> Training Loss: tensor(0.4343, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0328, dtype=torch.float64) <<\n",
      "tensor(0.4339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch249 >> Training Loss: tensor(0.4339, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0330, dtype=torch.float64) <<\n",
      "tensor(0.4338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch250 >> Training Loss: tensor(0.4338, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0331, dtype=torch.float64) <<\n",
      "tensor(0.4336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch251 >> Training Loss: tensor(0.4336, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch252 >> Training Loss: tensor(0.4333, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0327, dtype=torch.float64) <<\n",
      "tensor(0.4331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch253 >> Training Loss: tensor(0.4331, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0326, dtype=torch.float64) <<\n",
      "tensor(0.4329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch254 >> Training Loss: tensor(0.4329, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0327, dtype=torch.float64) <<\n",
      "tensor(0.4326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch255 >> Training Loss: tensor(0.4326, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0328, dtype=torch.float64) <<\n",
      "tensor(0.4324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch256 >> Training Loss: tensor(0.4324, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch257 >> Training Loss: tensor(0.4323, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0328, dtype=torch.float64) <<\n",
      "tensor(0.4320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch258 >> Training Loss: tensor(0.4320, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0326, dtype=torch.float64) <<\n",
      "tensor(0.4318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch259 >> Training Loss: tensor(0.4318, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch260 >> Training Loss: tensor(0.4316, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4313, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch261 >> Training Loss: tensor(0.4313, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0326, dtype=torch.float64) <<\n",
      "tensor(0.4311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch262 >> Training Loss: tensor(0.4311, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0326, dtype=torch.float64) <<\n",
      "tensor(0.4309, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch263 >> Training Loss: tensor(0.4309, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0326, dtype=torch.float64) <<\n",
      "tensor(0.4307, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch264 >> Training Loss: tensor(0.4307, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0324, dtype=torch.float64) <<\n",
      "tensor(0.4304, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch265 >> Training Loss: tensor(0.4304, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0324, dtype=torch.float64) <<\n",
      "tensor(0.4302, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch266 >> Training Loss: tensor(0.4302, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0324, dtype=torch.float64) <<\n",
      "tensor(0.4300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch267 >> Training Loss: tensor(0.4300, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0324, dtype=torch.float64) <<\n",
      "tensor(0.4297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch268 >> Training Loss: tensor(0.4297, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch269 >> Training Loss: tensor(0.4295, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0324, dtype=torch.float64) <<\n",
      "tensor(0.4292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch270 >> Training Loss: tensor(0.4292, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0323, dtype=torch.float64) <<\n",
      "tensor(0.4290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch271 >> Training Loss: tensor(0.4290, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0322, dtype=torch.float64) <<\n",
      "tensor(0.4287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch272 >> Training Loss: tensor(0.4287, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0322, dtype=torch.float64) <<\n",
      "tensor(0.4285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch273 >> Training Loss: tensor(0.4285, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0322, dtype=torch.float64) <<\n",
      "tensor(0.4282, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch274 >> Training Loss: tensor(0.4282, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0322, dtype=torch.float64) <<\n",
      "tensor(0.4279, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch275 >> Training Loss: tensor(0.4279, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0322, dtype=torch.float64) <<\n",
      "tensor(0.4276, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch276 >> Training Loss: tensor(0.4276, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0321, dtype=torch.float64) <<\n",
      "tensor(0.4273, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch277 >> Training Loss: tensor(0.4273, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4270, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch278 >> Training Loss: tensor(0.4270, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4267, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch279 >> Training Loss: tensor(0.4267, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch280 >> Training Loss: tensor(0.4263, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch281 >> Training Loss: tensor(0.4260, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch282 >> Training Loss: tensor(0.4256, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0319, dtype=torch.float64) <<\n",
      "tensor(0.4252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch283 >> Training Loss: tensor(0.4252, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0318, dtype=torch.float64) <<\n",
      "tensor(0.4248, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch284 >> Training Loss: tensor(0.4248, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0318, dtype=torch.float64) <<\n",
      "tensor(0.4244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch285 >> Training Loss: tensor(0.4244, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0318, dtype=torch.float64) <<\n",
      "tensor(0.4240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch286 >> Training Loss: tensor(0.4240, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0318, dtype=torch.float64) <<\n",
      "tensor(0.4235, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch287 >> Training Loss: tensor(0.4235, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0317, dtype=torch.float64) <<\n",
      "tensor(0.4230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch288 >> Training Loss: tensor(0.4230, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0316, dtype=torch.float64) <<\n",
      "tensor(0.4225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch289 >> Training Loss: tensor(0.4225, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0316, dtype=torch.float64) <<\n",
      "tensor(0.4220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch290 >> Training Loss: tensor(0.4220, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0316, dtype=torch.float64) <<\n",
      "tensor(0.4214, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch291 >> Training Loss: tensor(0.4214, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch292 >> Training Loss: tensor(0.4208, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch293 >> Training Loss: tensor(0.4202, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0314, dtype=torch.float64) <<\n",
      "tensor(0.4195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch294 >> Training Loss: tensor(0.4195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch295 >> Training Loss: tensor(0.4188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch296 >> Training Loss: tensor(0.4181, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch297 >> Training Loss: tensor(0.4173, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0312, dtype=torch.float64) <<\n",
      "tensor(0.4165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch298 >> Training Loss: tensor(0.4165, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch299 >> Training Loss: tensor(0.4156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0310, dtype=torch.float64) <<\n",
      "tensor(0.4147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch300 >> Training Loss: tensor(0.4147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0310, dtype=torch.float64) <<\n",
      "tensor(0.4137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch301 >> Training Loss: tensor(0.4137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch302 >> Training Loss: tensor(0.4127, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4117, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch303 >> Training Loss: tensor(0.4117, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch304 >> Training Loss: tensor(0.4106, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0307, dtype=torch.float64) <<\n",
      "tensor(0.4094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch305 >> Training Loss: tensor(0.4094, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0306, dtype=torch.float64) <<\n",
      "tensor(0.4082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch306 >> Training Loss: tensor(0.4082, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch307 >> Training Loss: tensor(0.4069, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0304, dtype=torch.float64) <<\n",
      "tensor(0.4056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch308 >> Training Loss: tensor(0.4056, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0303, dtype=torch.float64) <<\n",
      "tensor(0.4042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch309 >> Training Loss: tensor(0.4042, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0302, dtype=torch.float64) <<\n",
      "tensor(0.4027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch310 >> Training Loss: tensor(0.4027, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0301, dtype=torch.float64) <<\n",
      "tensor(0.4011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch311 >> Training Loss: tensor(0.4011, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0300, dtype=torch.float64) <<\n",
      "tensor(0.3995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch312 >> Training Loss: tensor(0.3995, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.3977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch313 >> Training Loss: tensor(0.3977, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0298, dtype=torch.float64) <<\n",
      "tensor(0.3959, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch314 >> Training Loss: tensor(0.3959, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0296, dtype=torch.float64) <<\n",
      "tensor(0.3941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch315 >> Training Loss: tensor(0.3941, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0295, dtype=torch.float64) <<\n",
      "tensor(0.3922, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch316 >> Training Loss: tensor(0.3922, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0293, dtype=torch.float64) <<\n",
      "tensor(0.3903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch317 >> Training Loss: tensor(0.3903, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0293, dtype=torch.float64) <<\n",
      "tensor(0.3884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch318 >> Training Loss: tensor(0.3884, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0288, dtype=torch.float64) <<\n",
      "tensor(0.3868, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch319 >> Training Loss: tensor(0.3868, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.3903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch320 >> Training Loss: tensor(0.3903, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0353, dtype=torch.float64) <<\n",
      "tensor(0.4859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch321 >> Training Loss: tensor(0.4859, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1340, dtype=torch.float64) <<\n",
      "tensor(1.1890, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch322 >> Training Loss: tensor(1.1890, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1501, dtype=torch.float64) <<\n",
      "tensor(1.6543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch323 >> Training Loss: tensor(1.6543, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0538, dtype=torch.float64) <<\n",
      "tensor(0.7038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch324 >> Training Loss: tensor(0.7038, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0794, dtype=torch.float64) <<\n",
      "tensor(0.7631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch325 >> Training Loss: tensor(0.7631, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1301, dtype=torch.float64) <<\n",
      "tensor(1.1625, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch326 >> Training Loss: tensor(1.1625, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0588, dtype=torch.float64) <<\n",
      "tensor(0.6098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch327 >> Training Loss: tensor(0.6098, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0514, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch328 >> Training Loss: tensor(0.7076, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0774, dtype=torch.float64) <<\n",
      "tensor(1.0230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch329 >> Training Loss: tensor(1.0230, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0575, dtype=torch.float64) <<\n",
      "tensor(0.7768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch330 >> Training Loss: tensor(0.7768, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0424, dtype=torch.float64) <<\n",
      "tensor(0.5074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch331 >> Training Loss: tensor(0.5074, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0748, dtype=torch.float64) <<\n",
      "tensor(0.7257, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch332 >> Training Loss: tensor(0.7257, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0705, dtype=torch.float64) <<\n",
      "tensor(0.7078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch333 >> Training Loss: tensor(0.7078, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0338, dtype=torch.float64) <<\n",
      "tensor(0.4460, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch334 >> Training Loss: tensor(0.4460, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0465, dtype=torch.float64) <<\n",
      "tensor(0.6300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch335 >> Training Loss: tensor(0.6300, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0521, dtype=torch.float64) <<\n",
      "tensor(0.6886, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch336 >> Training Loss: tensor(0.6886, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0339, dtype=torch.float64) <<\n",
      "tensor(0.4726, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch337 >> Training Loss: tensor(0.4726, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0471, dtype=torch.float64) <<\n",
      "tensor(0.5339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch338 >> Training Loss: tensor(0.5339, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0581, dtype=torch.float64) <<\n",
      "tensor(0.6058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch339 >> Training Loss: tensor(0.6058, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0416, dtype=torch.float64) <<\n",
      "tensor(0.4772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch340 >> Training Loss: tensor(0.4772, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0356, dtype=torch.float64) <<\n",
      "tensor(0.4768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch341 >> Training Loss: tensor(0.4768, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0415, dtype=torch.float64) <<\n",
      "tensor(0.5670, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch342 >> Training Loss: tensor(0.5670, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0391, dtype=torch.float64) <<\n",
      "tensor(0.5329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch343 >> Training Loss: tensor(0.5329, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0353, dtype=torch.float64) <<\n",
      "tensor(0.4543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch344 >> Training Loss: tensor(0.4543, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0426, dtype=torch.float64) <<\n",
      "tensor(0.4852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch345 >> Training Loss: tensor(0.4852, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0459, dtype=torch.float64) <<\n",
      "tensor(0.5136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch346 >> Training Loss: tensor(0.5136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0366, dtype=torch.float64) <<\n",
      "tensor(0.4543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch347 >> Training Loss: tensor(0.4543, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0330, dtype=torch.float64) <<\n",
      "tensor(0.4555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch348 >> Training Loss: tensor(0.4555, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0359, dtype=torch.float64) <<\n",
      "tensor(0.5003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch349 >> Training Loss: tensor(0.5003, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0337, dtype=torch.float64) <<\n",
      "tensor(0.4696, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch350 >> Training Loss: tensor(0.4696, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0335, dtype=torch.float64) <<\n",
      "tensor(0.4388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch351 >> Training Loss: tensor(0.4388, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0392, dtype=torch.float64) <<\n",
      "tensor(0.4659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch352 >> Training Loss: tensor(0.4659, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0399, dtype=torch.float64) <<\n",
      "tensor(0.4668, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch353 >> Training Loss: tensor(0.4668, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0351, dtype=torch.float64) <<\n",
      "tensor(0.4399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch354 >> Training Loss: tensor(0.4399, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch355 >> Training Loss: tensor(0.4480, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0339, dtype=torch.float64) <<\n",
      "tensor(0.4629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch356 >> Training Loss: tensor(0.4629, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0330, dtype=torch.float64) <<\n",
      "tensor(0.4453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch357 >> Training Loss: tensor(0.4453, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0338, dtype=torch.float64) <<\n",
      "tensor(0.4338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch358 >> Training Loss: tensor(0.4338, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0366, dtype=torch.float64) <<\n",
      "tensor(0.4481, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch359 >> Training Loss: tensor(0.4481, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0360, dtype=torch.float64) <<\n",
      "tensor(0.4459, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch360 >> Training Loss: tensor(0.4459, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch361 >> Training Loss: tensor(0.4321, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch362 >> Training Loss: tensor(0.4387, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0321, dtype=torch.float64) <<\n",
      "tensor(0.4431, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch363 >> Training Loss: tensor(0.4431, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch364 >> Training Loss: tensor(0.4324, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0336, dtype=torch.float64) <<\n",
      "tensor(0.4316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch365 >> Training Loss: tensor(0.4316, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0353, dtype=torch.float64) <<\n",
      "tensor(0.4383, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch366 >> Training Loss: tensor(0.4383, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0344, dtype=torch.float64) <<\n",
      "tensor(0.4332, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch367 >> Training Loss: tensor(0.4332, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0326, dtype=torch.float64) <<\n",
      "tensor(0.4283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch368 >> Training Loss: tensor(0.4283, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch369 >> Training Loss: tensor(0.4328, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0319, dtype=torch.float64) <<\n",
      "tensor(0.4325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch370 >> Training Loss: tensor(0.4325, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0322, dtype=torch.float64) <<\n",
      "tensor(0.4274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch371 >> Training Loss: tensor(0.4274, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0334, dtype=torch.float64) <<\n",
      "tensor(0.4291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch372 >> Training Loss: tensor(0.4291, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0338, dtype=torch.float64) <<\n",
      "tensor(0.4309, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch373 >> Training Loss: tensor(0.4309, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0327, dtype=torch.float64) <<\n",
      "tensor(0.4267, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch374 >> Training Loss: tensor(0.4267, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0317, dtype=torch.float64) <<\n",
      "tensor(0.4262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch375 >> Training Loss: tensor(0.4262, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch376 >> Training Loss: tensor(0.4285, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0316, dtype=torch.float64) <<\n",
      "tensor(0.4265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch377 >> Training Loss: tensor(0.4265, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0323, dtype=torch.float64) <<\n",
      "tensor(0.4250, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch378 >> Training Loss: tensor(0.4250, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0331, dtype=torch.float64) <<\n",
      "tensor(0.4266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch379 >> Training Loss: tensor(0.4266, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0329, dtype=torch.float64) <<\n",
      "tensor(0.4257, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch380 >> Training Loss: tensor(0.4257, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0321, dtype=torch.float64) <<\n",
      "tensor(0.4238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch381 >> Training Loss: tensor(0.4238, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0316, dtype=torch.float64) <<\n",
      "tensor(0.4247, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch382 >> Training Loss: tensor(0.4247, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch383 >> Training Loss: tensor(0.4249, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0318, dtype=torch.float64) <<\n",
      "tensor(0.4233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch384 >> Training Loss: tensor(0.4233, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0323, dtype=torch.float64) <<\n",
      "tensor(0.4235, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch385 >> Training Loss: tensor(0.4235, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0325, dtype=torch.float64) <<\n",
      "tensor(0.4238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch386 >> Training Loss: tensor(0.4238, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4226, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch387 >> Training Loss: tensor(0.4226, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4223, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch388 >> Training Loss: tensor(0.4223, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch389 >> Training Loss: tensor(0.4228, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0314, dtype=torch.float64) <<\n",
      "tensor(0.4220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch390 >> Training Loss: tensor(0.4220, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0318, dtype=torch.float64) <<\n",
      "tensor(0.4215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch391 >> Training Loss: tensor(0.4215, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0321, dtype=torch.float64) <<\n",
      "tensor(0.4218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch392 >> Training Loss: tensor(0.4218, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0320, dtype=torch.float64) <<\n",
      "tensor(0.4213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch393 >> Training Loss: tensor(0.4213, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0316, dtype=torch.float64) <<\n",
      "tensor(0.4208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch394 >> Training Loss: tensor(0.4208, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4210, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch395 >> Training Loss: tensor(0.4210, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4207, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch396 >> Training Loss: tensor(0.4207, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch397 >> Training Loss: tensor(0.4202, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0317, dtype=torch.float64) <<\n",
      "tensor(0.4202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch398 >> Training Loss: tensor(0.4202, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0317, dtype=torch.float64) <<\n",
      "tensor(0.4200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch399 >> Training Loss: tensor(0.4200, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0314, dtype=torch.float64) <<\n",
      "tensor(0.4195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch400 >> Training Loss: tensor(0.4195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0312, dtype=torch.float64) <<\n",
      "tensor(0.4194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch401 >> Training Loss: tensor(0.4194, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch402 >> Training Loss: tensor(0.4193, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch403 >> Training Loss: tensor(0.4189, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch404 >> Training Loss: tensor(0.4188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0315, dtype=torch.float64) <<\n",
      "tensor(0.4186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch405 >> Training Loss: tensor(0.4186, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0314, dtype=torch.float64) <<\n",
      "tensor(0.4183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch406 >> Training Loss: tensor(0.4183, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch407 >> Training Loss: tensor(0.4181, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch408 >> Training Loss: tensor(0.4180, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch409 >> Training Loss: tensor(0.4177, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch410 >> Training Loss: tensor(0.4175, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0313, dtype=torch.float64) <<\n",
      "tensor(0.4174, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch411 >> Training Loss: tensor(0.4174, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0312, dtype=torch.float64) <<\n",
      "tensor(0.4171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch412 >> Training Loss: tensor(0.4171, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0310, dtype=torch.float64) <<\n",
      "tensor(0.4169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch413 >> Training Loss: tensor(0.4169, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch414 >> Training Loss: tensor(0.4168, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0310, dtype=torch.float64) <<\n",
      "tensor(0.4165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch415 >> Training Loss: tensor(0.4165, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch416 >> Training Loss: tensor(0.4163, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch417 >> Training Loss: tensor(0.4162, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0311, dtype=torch.float64) <<\n",
      "tensor(0.4159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch418 >> Training Loss: tensor(0.4159, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch419 >> Training Loss: tensor(0.4157, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch420 >> Training Loss: tensor(0.4156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch421 >> Training Loss: tensor(0.4153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch422 >> Training Loss: tensor(0.4151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0310, dtype=torch.float64) <<\n",
      "tensor(0.4150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch423 >> Training Loss: tensor(0.4150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0309, dtype=torch.float64) <<\n",
      "tensor(0.4148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch424 >> Training Loss: tensor(0.4148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch425 >> Training Loss: tensor(0.4146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0307, dtype=torch.float64) <<\n",
      "tensor(0.4144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch426 >> Training Loss: tensor(0.4144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch427 >> Training Loss: tensor(0.4142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch428 >> Training Loss: tensor(0.4140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch429 >> Training Loss: tensor(0.4138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0308, dtype=torch.float64) <<\n",
      "tensor(0.4136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch430 >> Training Loss: tensor(0.4136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0307, dtype=torch.float64) <<\n",
      "tensor(0.4134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch431 >> Training Loss: tensor(0.4134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0306, dtype=torch.float64) <<\n",
      "tensor(0.4132, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch432 >> Training Loss: tensor(0.4132, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0306, dtype=torch.float64) <<\n",
      "tensor(0.4130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch433 >> Training Loss: tensor(0.4130, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0307, dtype=torch.float64) <<\n",
      "tensor(0.4128, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch434 >> Training Loss: tensor(0.4128, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0307, dtype=torch.float64) <<\n",
      "tensor(0.4126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch435 >> Training Loss: tensor(0.4126, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0306, dtype=torch.float64) <<\n",
      "tensor(0.4124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch436 >> Training Loss: tensor(0.4124, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch437 >> Training Loss: tensor(0.4122, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch438 >> Training Loss: tensor(0.4120, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4117, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch439 >> Training Loss: tensor(0.4117, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4115, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch440 >> Training Loss: tensor(0.4115, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch441 >> Training Loss: tensor(0.4113, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0305, dtype=torch.float64) <<\n",
      "tensor(0.4111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch442 >> Training Loss: tensor(0.4111, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0304, dtype=torch.float64) <<\n",
      "tensor(0.4109, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch443 >> Training Loss: tensor(0.4109, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0304, dtype=torch.float64) <<\n",
      "tensor(0.4107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch444 >> Training Loss: tensor(0.4107, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0304, dtype=torch.float64) <<\n",
      "tensor(0.4105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch445 >> Training Loss: tensor(0.4105, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0304, dtype=torch.float64) <<\n",
      "tensor(0.4103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch446 >> Training Loss: tensor(0.4103, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0304, dtype=torch.float64) <<\n",
      "tensor(0.4101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch447 >> Training Loss: tensor(0.4101, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0303, dtype=torch.float64) <<\n",
      "tensor(0.4099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch448 >> Training Loss: tensor(0.4099, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0303, dtype=torch.float64) <<\n",
      "tensor(0.4096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch449 >> Training Loss: tensor(0.4096, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0303, dtype=torch.float64) <<\n",
      "tensor(0.4094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch450 >> Training Loss: tensor(0.4094, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0303, dtype=torch.float64) <<\n",
      "tensor(0.4092, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch451 >> Training Loss: tensor(0.4092, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0303, dtype=torch.float64) <<\n",
      "tensor(0.4090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch452 >> Training Loss: tensor(0.4090, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0302, dtype=torch.float64) <<\n",
      "tensor(0.4088, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch453 >> Training Loss: tensor(0.4088, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0302, dtype=torch.float64) <<\n",
      "tensor(0.4085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch454 >> Training Loss: tensor(0.4085, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0302, dtype=torch.float64) <<\n",
      "tensor(0.4083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch455 >> Training Loss: tensor(0.4083, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0302, dtype=torch.float64) <<\n",
      "tensor(0.4081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch456 >> Training Loss: tensor(0.4081, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0302, dtype=torch.float64) <<\n",
      "tensor(0.4079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch457 >> Training Loss: tensor(0.4079, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0301, dtype=torch.float64) <<\n",
      "tensor(0.4076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch458 >> Training Loss: tensor(0.4076, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0301, dtype=torch.float64) <<\n",
      "tensor(0.4074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch459 >> Training Loss: tensor(0.4074, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0301, dtype=torch.float64) <<\n",
      "tensor(0.4072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch460 >> Training Loss: tensor(0.4072, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0300, dtype=torch.float64) <<\n",
      "tensor(0.4070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch461 >> Training Loss: tensor(0.4070, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0300, dtype=torch.float64) <<\n",
      "tensor(0.4067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch462 >> Training Loss: tensor(0.4067, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0300, dtype=torch.float64) <<\n",
      "tensor(0.4065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch463 >> Training Loss: tensor(0.4065, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0300, dtype=torch.float64) <<\n",
      "tensor(0.4062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch464 >> Training Loss: tensor(0.4062, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0300, dtype=torch.float64) <<\n",
      "tensor(0.4060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch465 >> Training Loss: tensor(0.4060, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.4058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch466 >> Training Loss: tensor(0.4058, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.4055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch467 >> Training Loss: tensor(0.4055, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.4053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch468 >> Training Loss: tensor(0.4053, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.4050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch469 >> Training Loss: tensor(0.4050, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0299, dtype=torch.float64) <<\n",
      "tensor(0.4048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch470 >> Training Loss: tensor(0.4048, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0298, dtype=torch.float64) <<\n",
      "tensor(0.4045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch471 >> Training Loss: tensor(0.4045, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0298, dtype=torch.float64) <<\n",
      "tensor(0.4043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch472 >> Training Loss: tensor(0.4043, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0298, dtype=torch.float64) <<\n",
      "tensor(0.4040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch473 >> Training Loss: tensor(0.4040, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0298, dtype=torch.float64) <<\n",
      "tensor(0.4038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch474 >> Training Loss: tensor(0.4038, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0298, dtype=torch.float64) <<\n",
      "tensor(0.4035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch475 >> Training Loss: tensor(0.4035, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0297, dtype=torch.float64) <<\n",
      "tensor(0.4032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch476 >> Training Loss: tensor(0.4032, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0297, dtype=torch.float64) <<\n",
      "tensor(0.4030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch477 >> Training Loss: tensor(0.4030, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0297, dtype=torch.float64) <<\n",
      "tensor(0.4027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch478 >> Training Loss: tensor(0.4027, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0297, dtype=torch.float64) <<\n",
      "tensor(0.4024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch479 >> Training Loss: tensor(0.4024, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0296, dtype=torch.float64) <<\n",
      "tensor(0.4021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch480 >> Training Loss: tensor(0.4021, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0296, dtype=torch.float64) <<\n",
      "tensor(0.4019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch481 >> Training Loss: tensor(0.4019, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0296, dtype=torch.float64) <<\n",
      "tensor(0.4016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch482 >> Training Loss: tensor(0.4016, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0296, dtype=torch.float64) <<\n",
      "tensor(0.4013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch483 >> Training Loss: tensor(0.4013, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0295, dtype=torch.float64) <<\n",
      "tensor(0.4010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch484 >> Training Loss: tensor(0.4010, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0295, dtype=torch.float64) <<\n",
      "tensor(0.4007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch485 >> Training Loss: tensor(0.4007, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0295, dtype=torch.float64) <<\n",
      "tensor(0.4004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch486 >> Training Loss: tensor(0.4004, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0295, dtype=torch.float64) <<\n",
      "tensor(0.4001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch487 >> Training Loss: tensor(0.4001, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0294, dtype=torch.float64) <<\n",
      "tensor(0.3998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch488 >> Training Loss: tensor(0.3998, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0294, dtype=torch.float64) <<\n",
      "tensor(0.3995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch489 >> Training Loss: tensor(0.3995, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0294, dtype=torch.float64) <<\n",
      "tensor(0.3992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch490 >> Training Loss: tensor(0.3992, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0294, dtype=torch.float64) <<\n",
      "tensor(0.3989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch491 >> Training Loss: tensor(0.3989, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0293, dtype=torch.float64) <<\n",
      "tensor(0.3986, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch492 >> Training Loss: tensor(0.3986, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0293, dtype=torch.float64) <<\n",
      "tensor(0.3983, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch493 >> Training Loss: tensor(0.3983, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0293, dtype=torch.float64) <<\n",
      "tensor(0.3980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch494 >> Training Loss: tensor(0.3980, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0293, dtype=torch.float64) <<\n",
      "tensor(0.3977, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch495 >> Training Loss: tensor(0.3977, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0292, dtype=torch.float64) <<\n",
      "tensor(0.3973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch496 >> Training Loss: tensor(0.3973, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0292, dtype=torch.float64) <<\n",
      "tensor(0.3970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch497 >> Training Loss: tensor(0.3970, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0292, dtype=torch.float64) <<\n",
      "tensor(0.3967, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch498 >> Training Loss: tensor(0.3967, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0292, dtype=torch.float64) <<\n",
      "tensor(0.3963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch499 >> Training Loss: tensor(0.3963, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0291, dtype=torch.float64) <<\n"
     ]
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "TR_LOSS = []\n",
    "VAL_LOSS = []\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    #print(net.hidden[0].shape)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            tr_loss = tr_loss + criterion(outputs, labels)\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    c = 0\n",
    "    \n",
    "    #net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            val_loss = val_loss + criterion(outputs, labels)\n",
    "            #print(val_loss)\n",
    "    VAL_LOSS.append(val_loss)\n",
    "    print('Epoch'+str(i)+ ' >> Training Loss: '+str(tr_loss)+ ' Validation Loss: '+str(val_loss)+ ' <<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5dd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOSS = []\n",
    "for t in TR_LOSS:\n",
    "    cc = t.detach().numpy()\n",
    "    TRT_LOSS.append(cc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a52f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5178ff5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(TRT_LOSS)\n",
    "plt.plot(VAL_LOSS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5625396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4046, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "accumulated_test_loss = 0.0\n",
    "net.reset_hidden_states(bs=1)\n",
    "it_test = iter(data_loaded.test_dataloader)\n",
    "c=0\n",
    "while c < int(X_test.shape[0]/1):\n",
    "    #net.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        #print(c)\n",
    "        c = c+1\n",
    "        item = next(it_test)\n",
    "        outputs = net(item[0])\n",
    "        accumulated_test_loss = accumulated_test_loss + criterion(outputs, item[1])        \n",
    "print(accumulated_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f01f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "794fb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('RESULTS_LOSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b881426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_array = []\n",
    "for z in TR_LOSS:\n",
    "    tr_loss_array.append(z.tolist())\n",
    "training_loss = pd.DataFrame(tr_loss_array, columns=['TR_LOSS'])\n",
    "training_loss.to_csv('../RESULTS_LOSS/TR_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca05f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_array = []\n",
    "for z in VAL_LOSS:\n",
    "    val_loss_array.append(z.tolist())\n",
    "val_loss = pd.DataFrame(val_loss_array, columns=['VAL_LOSS'])\n",
    "val_loss.to_csv('../RESULTS_LOSS/VAL_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2b97b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_array = [accumulated_test_loss]\n",
    "\n",
    "test2_loss = pd.DataFrame(test_loss_array, columns=['TEST_LOSS'])\n",
    "test2_loss.to_csv('../RESULTS_LOSS/TEST_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95819c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f70b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../SCALER_DUMPS/min_max_scaler_lookahead10.save']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVE THE SCALER\n",
    "import joblib\n",
    "scaler_filename = \"../SCALER_DUMPS/min_max_scaler_lookahead\"+str(output_cardinality)+\".save\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "# And now to load...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f65d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e0f979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE MODEL\n",
    "model_ckp_path = \"../MODEL_CHECKPOINTS/model_lookahead\"+str(output_cardinality)+\".pth\" \n",
    "torch.save(net.state_dict(), model_ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268595d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c07726f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net2 = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "#net2.load_state_dict(torch.load(model_ckp_path))\n",
    "#net2.eval()\n",
    "#scaler = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab93fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b98c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
