{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e847bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cardinality=20\n",
    "output_cardinality=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0738c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection as sk\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81495b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning as pl\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ead879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b2e9fec90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940b1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc51aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2683d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pl.seed_everything(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac4eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.filenames = os.listdir(DATA_PATH)\n",
    "        self.filenames = [filename for filename in self.filenames if filename.endswith(\".csv\")]\n",
    "        self.len_filenames = len(self.filenames)\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.data = []\n",
    "        for i in range(len(self.filenames)):\n",
    "            self.data.append(self.read_data(i))\n",
    "        return self.data\n",
    "    \n",
    "    def read_data(self,i):\n",
    "        return pd.read_csv(self.data_path+\"/\"+self.filenames[i])\n",
    "    \n",
    "\n",
    "    def process_data(self, scaler, input_cardinality=20, output_cardinality=10, cols=['ThetaXHG']):\n",
    "        self.datasamples = self.get_data()\n",
    "        X = []\n",
    "        y = []\n",
    "        X_scaled = []\n",
    "        y_scaled = []\n",
    "        for i, datasample in enumerate(self.datasamples):            \n",
    "            #for j in range(datasample[cols].shape[0]-output_cardinality):\n",
    "            #    print('Input' +str(j)+ ' to '+str(j+input_cardinality) + 'Output: '+str(j+input_cardinality)+ ' to '+str(j+input_cardinality+output_cardinality))\n",
    "            #Append to X (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto input_cardinality]]\n",
    "            #Append to Y (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto output_cardinality]]\n",
    "            #start of Y will be ahead of end of X by 1\n",
    "            \n",
    "            for j in range(datasample.shape[0]-output_cardinality-input_cardinality+1):\n",
    "                X.append(datasample[cols].iloc[j:j+input_cardinality].to_numpy())\n",
    "                y.append(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy())\n",
    "                X_scaled.append(scaler.transform(datasample[cols].iloc[j:j+input_cardinality].to_numpy()))\n",
    "                y_scaled.append(scaler.transform(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy()))\n",
    "        \n",
    "        #return self.datasamples, X, y\n",
    "        #print(np.array(X).shape,np.array(y).shape,np.array(X_scaled).shape, np.array(y_scaled).shape)\n",
    "        return self.datasamples, np.array(X), np.array(y), np.array(X_scaled), np.array(y_scaled)\n",
    "    \n",
    "    \n",
    "    def train_test_split(self, X, y, split={'train':0.8,'val':0.2, 'test':0.5}):\n",
    "        #print(X)\n",
    "        self.X_train, self.X_val_test, self.y_train, self.y_val_test = sk.train_test_split(X, y, test_size=split['val'] , random_state=43)\n",
    "        self.X_test, self.X_val, self.y_test, self.y_val = sk.train_test_split(self.X_val_test, self.y_val_test, test_size=split['test'] , random_state=43)\n",
    "        \n",
    "        return (self.X_train, self.y_train), (self.X_val, self.X_test), (self.X_test, self.y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getscaler(self,cols):\n",
    "        self.datasamples = self.get_data()       \n",
    "        for i, sample in enumerate(self.datasamples):\n",
    "            if i ==0:\n",
    "                features = pd.DataFrame(sample[cols])\n",
    "            else:\n",
    "                features = pd.DataFrame.append(features, sample[cols]) #sample[cols]\n",
    "   \n",
    "        #scaling\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        scaler = scaler.fit(features)\n",
    "        features_scaled = pd.DataFrame(scaler.transform(features), index = features.index, columns = cols)\n",
    "        return scaler\n",
    "    \n",
    "        \"\"\"        \n",
    "        #convert back to datasamples\n",
    "        #save to X\n",
    "        #save to X_scaled\n",
    "        \n",
    "        \n",
    "        #Split treain test and validation\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            self.datasamples = []\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        return features, features_scaled\n",
    "        \"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5167df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329ba98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASET(Dataset):\n",
    "    #convert to pytorch dataset\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, output = self.X[idx], self.y[idx]\n",
    "        return (torch.from_numpy(sequence.reshape(-1)), torch.from_numpy(output.reshape(-1)))\n",
    "        #return dict(sequence=torch.tensor(sequence.reshape(-1),dtype=torch.float64), label=torch.tensor(output.reshape(-1),dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d8d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4722168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASETLOADER():\n",
    "    def __init__(self, data, batchsize, bs_val):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batchsize = batchsize\n",
    "        self.train_dataset = THETADATASET(self.data.X_train, self.data.y_train)\n",
    "        self.val_dataset = THETADATASET(self.data.X_val, self.data.y_val)\n",
    "        self.test_dataset = THETADATASET(self.data.X_test, self.data.y_test)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size = self.batchsize, shuffle= False, num_workers=0, worker_init_fn=seed_worker,generator=g)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size = bs_val, shuffle= False, num_workers=0,  worker_init_fn=seed_worker,generator=g)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size = 1, shuffle= False, num_workers=0,  worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c884b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1aa2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847ddc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446a3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders={}\n",
    "#dataloaders['train'], dataloaders['val'] = data_loaded.train_dataloader, data_loaded.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5411a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor f, o in tqdm(dataloaders['train']):\\n    #print(f[0].shape)\\n    break\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for f, o in tqdm(dataloaders['train']):\n",
    "    #print(f[0].shape)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3389745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(a,first['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5a811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea08fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc39c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(DATA_LOADED, model, criterion, optimizer, num_epochs=25):\n",
    "    #train_model(DATA_LOADED, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 0\n",
    "    dataloaders ={}\n",
    "    dataloaders['train'], dataloaders['val'] = DATA_LOADED.train_dataloader, DATA_LOADED.val_dataloader\n",
    "\n",
    "    #TR_ACCURACY=[]\n",
    "    TR_LOSS=[]\n",
    "    #VAL_ACCURACY=[]\n",
    "    VAL_LOSS=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        model.reset_hidden_states()\n",
    "        #model.reset_hidden_states()\n",
    "        #print(model.hidden)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            #running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #if phase == 'train':\n",
    "            #    scheduler.step()\n",
    "\n",
    "            #epoch_loss = running_loss / DATA_LOADED.dataset_sizes[phase]\n",
    "            epoch_loss = running_loss / dataset_size[phase]\n",
    "            \n",
    "            #epoch_acc = running_corrects.double() / DATA.dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "\n",
    "            if phase == 'train':\n",
    "                #  print(\"Training\")\n",
    "                #  TR_ACCURACY.append(epoch_acc)\n",
    "                TR_LOSS.append(epoch_loss)\n",
    "            else:\n",
    "                print(\"Valuation\")\n",
    "                #VAL_ACCURACY.append(epoch_acc)\n",
    "                VAL_LOSS.append(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return TR_LOSS, VAL_LOSS, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c88e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHALLEABLELSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def reset_hidden_states(self):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        #lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        #self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        #lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= ht[-1]\n",
    "        #lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin0_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63a3a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def reset_hidden_states(self,bs):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, bs, self.lstm_units).double(), torch.zeros(1*self.num_layers, bs, self.lstm_units).double())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= h.detach()[-1]\n",
    "        lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        #lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin1_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4360dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd04f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd86f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2414e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "batch_size = 16\n",
    "bs_val = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c141d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookahead10.ipynb  Lookahead2.ipynb  Lookahead6.ipynb  prediction.ipynb\r\n",
      "Lookahead15.ipynb  Lookahead3.ipynb  Lookahead7.ipynb  readme.md\r\n",
      "Lookahead1.ipynb   Lookahead4.ipynb  Lookahead8.ipynb\r\n",
      "Lookahead20.ipynb  Lookahead5.ipynb  Lookahead9.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3f936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../DATASET/TRANSORMER_DATA\"\n",
    "data = DATA(DATA_PATH)\n",
    "cols=['ThetaXHG']\n",
    "scaler = data.getscaler(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "957024ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, X, y, X_scaled, y_scaled = data.process_data(scaler, input_cardinality=input_cardinality, output_cardinality=output_cardinality, cols=['ThetaXHG'])\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = data.train_test_split(X_scaled,y_scaled,  split={'train':0.8,'val':0.2, 'test':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3a02b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 20, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "850ef43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toor/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "net = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "net = net.to(device)\n",
    "net = net.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebb4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "598b88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first['sequence'].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0cffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaded = THETADATASETLOADER(data, batchsize=batch_size, bs_val = bs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccc3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ffaa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size={}\n",
    "dataset_size['train'] =X_train.shape[0]\n",
    "dataset_size['val'] =X_val.shape[0]\n",
    "dataset_size['test'] =X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3db471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_LOSS = []\n",
    "VAL_LOSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d89481f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "722/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09609bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in range(epochs):\\n    net.reset_hidden_states(bs=batch_size)\\n    it = iter(data_loaded.train_dataloader)\\n    it_val = iter(data_loaded.train_dataloader)\\n    print('Epoch'+str(i))\\n    c = 0\\n    tr_loss = 0.0\\n    val_loss = 0.0\\n    \\n    while c < int(X_train.shape[0]/batch_size): \\n        with torch.set_grad_enabled(True):\\n            #print(c)\\n            c = c+1\\n            item = next(it)\\n            outputs = net(item[0])\\n            tr_loss = tr_loss + criterion(outputs, item[1])\\n    \\n    TR_LOSS.append(tr_loss)\\n    print(tr_loss)\\n    optimizer.zero_grad()\\n    tr_loss.backward()\\n    optimizer.step()\\n    \\n    net.reset_hidden_states(bs=bs_val)\\n    #print(net.hidden[0].shape)\\n    while c < int(X_val.shape[0]/bs_val):\\n        net.eval()\\n        with torch.set_grad_enabled(False):\\n            #print(c)\\n            c = c+1\\n            item = next(it_val)\\n            outputs = net(item[0])\\n            val_loss = val_loss + criterion(outputs, item[1])\\n    VAL_LOSS.append(val_loss)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "\"\"\"for i in range(epochs):\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    print('Epoch'+str(i))\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            outputs = net(item[0])\n",
    "            tr_loss = tr_loss + criterion(outputs, item[1])\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            outputs = net(item[0])\n",
    "            val_loss = val_loss + criterion(outputs, item[1])\n",
    "    VAL_LOSS.append(val_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "651ccb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0987, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch0 >> Training Loss: tensor(8.0987, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.5120, dtype=torch.float64) <<\n",
      "tensor(3.1558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch1 >> Training Loss: tensor(3.1558, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.5650, dtype=torch.float64) <<\n",
      "tensor(3.2737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch2 >> Training Loss: tensor(3.2737, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4588, dtype=torch.float64) <<\n",
      "tensor(2.6693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch3 >> Training Loss: tensor(2.6693, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4911, dtype=torch.float64) <<\n",
      "tensor(3.2678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch4 >> Training Loss: tensor(3.2678, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4307, dtype=torch.float64) <<\n",
      "tensor(2.5410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch5 >> Training Loss: tensor(2.5410, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4819, dtype=torch.float64) <<\n",
      "tensor(2.8122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch6 >> Training Loss: tensor(2.8122, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3785, dtype=torch.float64) <<\n",
      "tensor(2.1865, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch7 >> Training Loss: tensor(2.1865, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3712, dtype=torch.float64) <<\n",
      "tensor(2.3506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch8 >> Training Loss: tensor(2.3506, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3300, dtype=torch.float64) <<\n",
      "tensor(2.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch9 >> Training Loss: tensor(2.0027, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3227, dtype=torch.float64) <<\n",
      "tensor(1.8514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch10 >> Training Loss: tensor(1.8514, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3033, dtype=torch.float64) <<\n",
      "tensor(1.7704, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch11 >> Training Loss: tensor(1.7704, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2194, dtype=torch.float64) <<\n",
      "tensor(1.3178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch12 >> Training Loss: tensor(1.3178, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2041, dtype=torch.float64) <<\n",
      "tensor(1.3788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch13 >> Training Loss: tensor(1.3788, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1356, dtype=torch.float64) <<\n",
      "tensor(0.9419, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch14 >> Training Loss: tensor(0.9419, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1524, dtype=torch.float64) <<\n",
      "tensor(1.2897, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch15 >> Training Loss: tensor(1.2897, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1344, dtype=torch.float64) <<\n",
      "tensor(1.1430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch16 >> Training Loss: tensor(1.1430, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1245, dtype=torch.float64) <<\n",
      "tensor(1.0772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch17 >> Training Loss: tensor(1.0772, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1107, dtype=torch.float64) <<\n",
      "tensor(1.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch18 >> Training Loss: tensor(1.0033, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1106, dtype=torch.float64) <<\n",
      "tensor(0.9430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch19 >> Training Loss: tensor(0.9430, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1062, dtype=torch.float64) <<\n",
      "tensor(0.8485, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch20 >> Training Loss: tensor(0.8485, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1205, dtype=torch.float64) <<\n",
      "tensor(0.9350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch21 >> Training Loss: tensor(0.9350, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1114, dtype=torch.float64) <<\n",
      "tensor(0.8373, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch22 >> Training Loss: tensor(0.8373, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1205, dtype=torch.float64) <<\n",
      "tensor(0.8845, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch23 >> Training Loss: tensor(0.8845, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1198, dtype=torch.float64) <<\n",
      "tensor(0.8785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch24 >> Training Loss: tensor(0.8785, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1086, dtype=torch.float64) <<\n",
      "tensor(0.8148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch25 >> Training Loss: tensor(0.8148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1110, dtype=torch.float64) <<\n",
      "tensor(0.8607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch26 >> Training Loss: tensor(0.8607, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1023, dtype=torch.float64) <<\n",
      "tensor(0.8107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch27 >> Training Loss: tensor(0.8107, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0978, dtype=torch.float64) <<\n",
      "tensor(0.7961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch28 >> Training Loss: tensor(0.7961, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0963, dtype=torch.float64) <<\n",
      "tensor(0.8147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch29 >> Training Loss: tensor(0.8147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0867, dtype=torch.float64) <<\n",
      "tensor(0.7656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch30 >> Training Loss: tensor(0.7656, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0898, dtype=torch.float64) <<\n",
      "tensor(0.8043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch31 >> Training Loss: tensor(0.8043, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch32 >> Training Loss: tensor(0.7803, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0839, dtype=torch.float64) <<\n",
      "tensor(0.7906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch33 >> Training Loss: tensor(0.7906, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.7952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch34 >> Training Loss: tensor(0.7952, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0835, dtype=torch.float64) <<\n",
      "tensor(0.7709, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch35 >> Training Loss: tensor(0.7709, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0874, dtype=torch.float64) <<\n",
      "tensor(0.7856, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch36 >> Training Loss: tensor(0.7856, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.7530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch37 >> Training Loss: tensor(0.7530, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0879, dtype=torch.float64) <<\n",
      "tensor(0.7656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch38 >> Training Loss: tensor(0.7656, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0882, dtype=torch.float64) <<\n",
      "tensor(0.7505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch39 >> Training Loss: tensor(0.7505, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0903, dtype=torch.float64) <<\n",
      "tensor(0.7523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch40 >> Training Loss: tensor(0.7523, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0920, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7562, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch41 >> Training Loss: tensor(0.7562, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0914, dtype=torch.float64) <<\n",
      "tensor(0.7452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch42 >> Training Loss: tensor(0.7452, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0930, dtype=torch.float64) <<\n",
      "tensor(0.7546, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch43 >> Training Loss: tensor(0.7546, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0909, dtype=torch.float64) <<\n",
      "tensor(0.7400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch44 >> Training Loss: tensor(0.7400, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0909, dtype=torch.float64) <<\n",
      "tensor(0.7444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch45 >> Training Loss: tensor(0.7444, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0889, dtype=torch.float64) <<\n",
      "tensor(0.7354, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch46 >> Training Loss: tensor(0.7354, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0875, dtype=torch.float64) <<\n",
      "tensor(0.7341, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch47 >> Training Loss: tensor(0.7341, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0863, dtype=torch.float64) <<\n",
      "tensor(0.7335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch48 >> Training Loss: tensor(0.7335, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0854, dtype=torch.float64) <<\n",
      "tensor(0.7296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch49 >> Training Loss: tensor(0.7296, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0856, dtype=torch.float64) <<\n",
      "tensor(0.7340, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch50 >> Training Loss: tensor(0.7340, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0842, dtype=torch.float64) <<\n",
      "tensor(0.7289, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch51 >> Training Loss: tensor(0.7289, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch52 >> Training Loss: tensor(0.7333, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0849, dtype=torch.float64) <<\n",
      "tensor(0.7267, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch53 >> Training Loss: tensor(0.7267, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0863, dtype=torch.float64) <<\n",
      "tensor(0.7293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch54 >> Training Loss: tensor(0.7293, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0862, dtype=torch.float64) <<\n",
      "tensor(0.7232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch55 >> Training Loss: tensor(0.7232, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0873, dtype=torch.float64) <<\n",
      "tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch56 >> Training Loss: tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0880, dtype=torch.float64) <<\n",
      "tensor(0.7218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch57 >> Training Loss: tensor(0.7218, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0892, dtype=torch.float64) <<\n",
      "tensor(0.7243, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch58 >> Training Loss: tensor(0.7243, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0893, dtype=torch.float64) <<\n",
      "tensor(0.7218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch59 >> Training Loss: tensor(0.7218, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0895, dtype=torch.float64) <<\n",
      "tensor(0.7229, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch60 >> Training Loss: tensor(0.7229, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0892, dtype=torch.float64) <<\n",
      "tensor(0.7205, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch61 >> Training Loss: tensor(0.7205, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0890, dtype=torch.float64) <<\n",
      "tensor(0.7203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch62 >> Training Loss: tensor(0.7203, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0882, dtype=torch.float64) <<\n",
      "tensor(0.7184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch63 >> Training Loss: tensor(0.7184, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0874, dtype=torch.float64) <<\n",
      "tensor(0.7181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch64 >> Training Loss: tensor(0.7181, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0867, dtype=torch.float64) <<\n",
      "tensor(0.7170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch65 >> Training Loss: tensor(0.7170, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0865, dtype=torch.float64) <<\n",
      "tensor(0.7169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch66 >> Training Loss: tensor(0.7169, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0860, dtype=torch.float64) <<\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch67 >> Training Loss: tensor(0.7160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0855, dtype=torch.float64) <<\n",
      "tensor(0.7158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch68 >> Training Loss: tensor(0.7158, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0855, dtype=torch.float64) <<\n",
      "tensor(0.7148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch69 >> Training Loss: tensor(0.7148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0858, dtype=torch.float64) <<\n",
      "tensor(0.7145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch70 >> Training Loss: tensor(0.7145, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0859, dtype=torch.float64) <<\n",
      "tensor(0.7133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch71 >> Training Loss: tensor(0.7133, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0861, dtype=torch.float64) <<\n",
      "tensor(0.7131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch72 >> Training Loss: tensor(0.7131, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0864, dtype=torch.float64) <<\n",
      "tensor(0.7120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch73 >> Training Loss: tensor(0.7120, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0868, dtype=torch.float64) <<\n",
      "tensor(0.7119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch74 >> Training Loss: tensor(0.7119, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0868, dtype=torch.float64) <<\n",
      "tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch75 >> Training Loss: tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0868, dtype=torch.float64) <<\n",
      "tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch76 >> Training Loss: tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0867, dtype=torch.float64) <<\n",
      "tensor(0.7100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch77 >> Training Loss: tensor(0.7100, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0866, dtype=torch.float64) <<\n",
      "tensor(0.7099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch78 >> Training Loss: tensor(0.7099, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0862, dtype=torch.float64) <<\n",
      "tensor(0.7088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch79 >> Training Loss: tensor(0.7088, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0858, dtype=torch.float64) <<\n",
      "tensor(0.7086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch80 >> Training Loss: tensor(0.7086, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0855, dtype=torch.float64) <<\n",
      "tensor(0.7077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch81 >> Training Loss: tensor(0.7077, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0853, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch82 >> Training Loss: tensor(0.7075, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0849, dtype=torch.float64) <<\n",
      "tensor(0.7068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch83 >> Training Loss: tensor(0.7068, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch84 >> Training Loss: tensor(0.7065, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch85 >> Training Loss: tensor(0.7059, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch86 >> Training Loss: tensor(0.7053, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch87 >> Training Loss: tensor(0.7048, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0848, dtype=torch.float64) <<\n",
      "tensor(0.7042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch88 >> Training Loss: tensor(0.7042, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0851, dtype=torch.float64) <<\n",
      "tensor(0.7039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch89 >> Training Loss: tensor(0.7039, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0851, dtype=torch.float64) <<\n",
      "tensor(0.7032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch90 >> Training Loss: tensor(0.7032, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0851, dtype=torch.float64) <<\n",
      "tensor(0.7029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch91 >> Training Loss: tensor(0.7029, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0852, dtype=torch.float64) <<\n",
      "tensor(0.7023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch92 >> Training Loss: tensor(0.7023, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0852, dtype=torch.float64) <<\n",
      "tensor(0.7019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch93 >> Training Loss: tensor(0.7019, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0850, dtype=torch.float64) <<\n",
      "tensor(0.7013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch94 >> Training Loss: tensor(0.7013, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0848, dtype=torch.float64) <<\n",
      "tensor(0.7008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch95 >> Training Loss: tensor(0.7008, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch96 >> Training Loss: tensor(0.7003, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.6998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch97 >> Training Loss: tensor(0.6998, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.6993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch98 >> Training Loss: tensor(0.6993, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0843, dtype=torch.float64) <<\n",
      "tensor(0.6988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch99 >> Training Loss: tensor(0.6988, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0843, dtype=torch.float64) <<\n",
      "tensor(0.6983, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch100 >> Training Loss: tensor(0.6983, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0842, dtype=torch.float64) <<\n",
      "tensor(0.6978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch101 >> Training Loss: tensor(0.6978, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0843, dtype=torch.float64) <<\n",
      "tensor(0.6973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch102 >> Training Loss: tensor(0.6973, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.6968, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch103 >> Training Loss: tensor(0.6968, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.6963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch104 >> Training Loss: tensor(0.6963, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.6958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch105 >> Training Loss: tensor(0.6958, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.6952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch106 >> Training Loss: tensor(0.6952, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.6947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch107 >> Training Loss: tensor(0.6947, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.6942, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch108 >> Training Loss: tensor(0.6942, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.6937, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch109 >> Training Loss: tensor(0.6937, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.6931, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch110 >> Training Loss: tensor(0.6931, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0843, dtype=torch.float64) <<\n",
      "tensor(0.6926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch111 >> Training Loss: tensor(0.6926, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0842, dtype=torch.float64) <<\n",
      "tensor(0.6921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch112 >> Training Loss: tensor(0.6921, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0842, dtype=torch.float64) <<\n",
      "tensor(0.6915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch113 >> Training Loss: tensor(0.6915, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6909, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch114 >> Training Loss: tensor(0.6909, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6904, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch115 >> Training Loss: tensor(0.6904, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch116 >> Training Loss: tensor(0.6898, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch117 >> Training Loss: tensor(0.6892, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch118 >> Training Loss: tensor(0.6887, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6881, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch119 >> Training Loss: tensor(0.6881, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6875, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch120 >> Training Loss: tensor(0.6875, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6868, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch121 >> Training Loss: tensor(0.6868, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.6862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch122 >> Training Loss: tensor(0.6862, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6856, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch123 >> Training Loss: tensor(0.6856, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0840, dtype=torch.float64) <<\n",
      "tensor(0.6849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch124 >> Training Loss: tensor(0.6849, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0839, dtype=torch.float64) <<\n",
      "tensor(0.6842, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch125 >> Training Loss: tensor(0.6842, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0839, dtype=torch.float64) <<\n",
      "tensor(0.6836, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch126 >> Training Loss: tensor(0.6836, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0838, dtype=torch.float64) <<\n",
      "tensor(0.6828, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch127 >> Training Loss: tensor(0.6828, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0838, dtype=torch.float64) <<\n",
      "tensor(0.6821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch128 >> Training Loss: tensor(0.6821, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0838, dtype=torch.float64) <<\n",
      "tensor(0.6814, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch129 >> Training Loss: tensor(0.6814, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0837, dtype=torch.float64) <<\n",
      "tensor(0.6806, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch130 >> Training Loss: tensor(0.6806, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0837, dtype=torch.float64) <<\n",
      "tensor(0.6797, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch131 >> Training Loss: tensor(0.6797, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0837, dtype=torch.float64) <<\n",
      "tensor(0.6789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch132 >> Training Loss: tensor(0.6789, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0837, dtype=torch.float64) <<\n",
      "tensor(0.6780, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch133 >> Training Loss: tensor(0.6780, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0836, dtype=torch.float64) <<\n",
      "tensor(0.6770, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch134 >> Training Loss: tensor(0.6770, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0836, dtype=torch.float64) <<\n",
      "tensor(0.6759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch135 >> Training Loss: tensor(0.6759, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0835, dtype=torch.float64) <<\n",
      "tensor(0.6747, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch136 >> Training Loss: tensor(0.6747, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0834, dtype=torch.float64) <<\n",
      "tensor(0.6733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch137 >> Training Loss: tensor(0.6733, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0832, dtype=torch.float64) <<\n",
      "tensor(0.6715, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch138 >> Training Loss: tensor(0.6715, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0829, dtype=torch.float64) <<\n",
      "tensor(0.6692, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch139 >> Training Loss: tensor(0.6692, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0823, dtype=torch.float64) <<\n",
      "tensor(0.6656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch140 >> Training Loss: tensor(0.6656, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0811, dtype=torch.float64) <<\n",
      "tensor(0.6600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch141 >> Training Loss: tensor(0.6600, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0795, dtype=torch.float64) <<\n",
      "tensor(0.6567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch142 >> Training Loss: tensor(0.6567, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0832, dtype=torch.float64) <<\n",
      "tensor(0.6771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch143 >> Training Loss: tensor(0.6771, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1606, dtype=torch.float64) <<\n",
      "tensor(1.3180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch144 >> Training Loss: tensor(1.3180, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4330, dtype=torch.float64) <<\n",
      "tensor(3.3973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch145 >> Training Loss: tensor(3.3973, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1635, dtype=torch.float64) <<\n",
      "tensor(1.3501, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch146 >> Training Loss: tensor(1.3501, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2613, dtype=torch.float64) <<\n",
      "tensor(2.0632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch147 >> Training Loss: tensor(2.0632, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1243, dtype=torch.float64) <<\n",
      "tensor(0.9173, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch148 >> Training Loss: tensor(0.9173, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2647, dtype=torch.float64) <<\n",
      "tensor(2.0621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch149 >> Training Loss: tensor(2.0621, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1701, dtype=torch.float64) <<\n",
      "tensor(1.2360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch150 >> Training Loss: tensor(1.2360, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1784, dtype=torch.float64) <<\n",
      "tensor(1.1801, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch151 >> Training Loss: tensor(1.1801, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2510, dtype=torch.float64) <<\n",
      "tensor(1.6988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch152 >> Training Loss: tensor(1.6988, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2031, dtype=torch.float64) <<\n",
      "tensor(1.3548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch153 >> Training Loss: tensor(1.3548, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1513, dtype=torch.float64) <<\n",
      "tensor(1.0409, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch154 >> Training Loss: tensor(1.0409, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1721, dtype=torch.float64) <<\n",
      "tensor(1.2982, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch155 >> Training Loss: tensor(1.2982, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1706, dtype=torch.float64) <<\n",
      "tensor(1.3295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch156 >> Training Loss: tensor(1.3295, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1202, dtype=torch.float64) <<\n",
      "tensor(0.9390, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch157 >> Training Loss: tensor(0.9390, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1094, dtype=torch.float64) <<\n",
      "tensor(0.8805, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch158 >> Training Loss: tensor(0.8805, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1241, dtype=torch.float64) <<\n",
      "tensor(1.0638, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch159 >> Training Loss: tensor(1.0638, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0905, dtype=torch.float64) <<\n",
      "tensor(0.8600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch160 >> Training Loss: tensor(0.8600, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0862, dtype=torch.float64) <<\n",
      "tensor(0.8328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch161 >> Training Loss: tensor(0.8328, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1187, dtype=torch.float64) <<\n",
      "tensor(1.0735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch162 >> Training Loss: tensor(1.0735, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0848, dtype=torch.float64) <<\n",
      "tensor(0.8288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch163 >> Training Loss: tensor(0.8288, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0833, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8214, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch164 >> Training Loss: tensor(0.8214, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0988, dtype=torch.float64) <<\n",
      "tensor(0.9071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch165 >> Training Loss: tensor(0.9071, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0912, dtype=torch.float64) <<\n",
      "tensor(0.7997, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch166 >> Training Loss: tensor(0.7997, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0911, dtype=torch.float64) <<\n",
      "tensor(0.7666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch167 >> Training Loss: tensor(0.7666, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1043, dtype=torch.float64) <<\n",
      "tensor(0.8537, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch168 >> Training Loss: tensor(0.8537, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1074, dtype=torch.float64) <<\n",
      "tensor(0.8663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch169 >> Training Loss: tensor(0.8663, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0997, dtype=torch.float64) <<\n",
      "tensor(0.7955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch170 >> Training Loss: tensor(0.7955, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0971, dtype=torch.float64) <<\n",
      "tensor(0.7740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch171 >> Training Loss: tensor(0.7740, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0988, dtype=torch.float64) <<\n",
      "tensor(0.8035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch172 >> Training Loss: tensor(0.8035, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0930, dtype=torch.float64) <<\n",
      "tensor(0.7855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch173 >> Training Loss: tensor(0.7855, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0831, dtype=torch.float64) <<\n",
      "tensor(0.7355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch174 >> Training Loss: tensor(0.7355, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0834, dtype=torch.float64) <<\n",
      "tensor(0.7537, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch175 >> Training Loss: tensor(0.7537, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0869, dtype=torch.float64) <<\n",
      "tensor(0.7887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch176 >> Training Loss: tensor(0.7887, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0818, dtype=torch.float64) <<\n",
      "tensor(0.7554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch177 >> Training Loss: tensor(0.7554, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0798, dtype=torch.float64) <<\n",
      "tensor(0.7400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch178 >> Training Loss: tensor(0.7400, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0838, dtype=torch.float64) <<\n",
      "tensor(0.7603, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch179 >> Training Loss: tensor(0.7603, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.7475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch180 >> Training Loss: tensor(0.7475, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0840, dtype=torch.float64) <<\n",
      "tensor(0.7237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch181 >> Training Loss: tensor(0.7237, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0870, dtype=torch.float64) <<\n",
      "tensor(0.7332, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch182 >> Training Loss: tensor(0.7332, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0898, dtype=torch.float64) <<\n",
      "tensor(0.7468, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch183 >> Training Loss: tensor(0.7468, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0890, dtype=torch.float64) <<\n",
      "tensor(0.7360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch184 >> Training Loss: tensor(0.7360, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0880, dtype=torch.float64) <<\n",
      "tensor(0.7266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch185 >> Training Loss: tensor(0.7266, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0881, dtype=torch.float64) <<\n",
      "tensor(0.7326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch186 >> Training Loss: tensor(0.7326, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0865, dtype=torch.float64) <<\n",
      "tensor(0.7310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch187 >> Training Loss: tensor(0.7310, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0831, dtype=torch.float64) <<\n",
      "tensor(0.7176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch188 >> Training Loss: tensor(0.7176, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0816, dtype=torch.float64) <<\n",
      "tensor(0.7167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch189 >> Training Loss: tensor(0.7167, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0818, dtype=torch.float64) <<\n",
      "tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch190 >> Training Loss: tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0809, dtype=torch.float64) <<\n",
      "tensor(0.7220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch191 >> Training Loss: tensor(0.7220, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0801, dtype=torch.float64) <<\n",
      "tensor(0.7161, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch192 >> Training Loss: tensor(0.7161, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0810, dtype=torch.float64) <<\n",
      "tensor(0.7195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch193 >> Training Loss: tensor(0.7195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0818, dtype=torch.float64) <<\n",
      "tensor(0.7182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch194 >> Training Loss: tensor(0.7182, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0819, dtype=torch.float64) <<\n",
      "tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch195 >> Training Loss: tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0827, dtype=torch.float64) <<\n",
      "tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch196 >> Training Loss: tensor(0.7110, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0839, dtype=torch.float64) <<\n",
      "tensor(0.7146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch197 >> Training Loss: tensor(0.7146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.7123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch198 >> Training Loss: tensor(0.7123, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0840, dtype=torch.float64) <<\n",
      "tensor(0.7098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch199 >> Training Loss: tensor(0.7098, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.7112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch200 >> Training Loss: tensor(0.7112, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0835, dtype=torch.float64) <<\n",
      "tensor(0.7104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch201 >> Training Loss: tensor(0.7104, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0823, dtype=torch.float64) <<\n",
      "tensor(0.7067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch202 >> Training Loss: tensor(0.7067, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0815, dtype=torch.float64) <<\n",
      "tensor(0.7064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch203 >> Training Loss: tensor(0.7064, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0811, dtype=torch.float64) <<\n",
      "tensor(0.7078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch204 >> Training Loss: tensor(0.7078, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0805, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch205 >> Training Loss: tensor(0.7062, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0802, dtype=torch.float64) <<\n",
      "tensor(0.7050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch206 >> Training Loss: tensor(0.7050, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.7058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch207 >> Training Loss: tensor(0.7058, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0805, dtype=torch.float64) <<\n",
      "tensor(0.7049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch208 >> Training Loss: tensor(0.7049, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.7030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch209 >> Training Loss: tensor(0.7030, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0811, dtype=torch.float64) <<\n",
      "tensor(0.7030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch210 >> Training Loss: tensor(0.7030, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0816, dtype=torch.float64) <<\n",
      "tensor(0.7031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch211 >> Training Loss: tensor(0.7031, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0819, dtype=torch.float64) <<\n",
      "tensor(0.7018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch212 >> Training Loss: tensor(0.7018, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0821, dtype=torch.float64) <<\n",
      "tensor(0.7015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch213 >> Training Loss: tensor(0.7015, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0823, dtype=torch.float64) <<\n",
      "tensor(0.7018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch214 >> Training Loss: tensor(0.7018, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0821, dtype=torch.float64) <<\n",
      "tensor(0.7009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch215 >> Training Loss: tensor(0.7009, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0818, dtype=torch.float64) <<\n",
      "tensor(0.7000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch216 >> Training Loss: tensor(0.7000, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0815, dtype=torch.float64) <<\n",
      "tensor(0.6999, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch217 >> Training Loss: tensor(0.6999, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0811, dtype=torch.float64) <<\n",
      "tensor(0.6994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch218 >> Training Loss: tensor(0.6994, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0808, dtype=torch.float64) <<\n",
      "tensor(0.6986, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch219 >> Training Loss: tensor(0.6986, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.6985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch220 >> Training Loss: tensor(0.6985, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6983, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch221 >> Training Loss: tensor(0.6983, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0805, dtype=torch.float64) <<\n",
      "tensor(0.6976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch222 >> Training Loss: tensor(0.6976, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch223 >> Training Loss: tensor(0.6972, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.6970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch224 >> Training Loss: tensor(0.6970, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0809, dtype=torch.float64) <<\n",
      "tensor(0.6963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch225 >> Training Loss: tensor(0.6963, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0811, dtype=torch.float64) <<\n",
      "tensor(0.6959, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch226 >> Training Loss: tensor(0.6959, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0813, dtype=torch.float64) <<\n",
      "tensor(0.6957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch227 >> Training Loss: tensor(0.6957, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0814, dtype=torch.float64) <<\n",
      "tensor(0.6953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch228 >> Training Loss: tensor(0.6953, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0814, dtype=torch.float64) <<\n",
      "tensor(0.6948, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch229 >> Training Loss: tensor(0.6948, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0813, dtype=torch.float64) <<\n",
      "tensor(0.6946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch230 >> Training Loss: tensor(0.6946, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0812, dtype=torch.float64) <<\n",
      "tensor(0.6941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch231 >> Training Loss: tensor(0.6941, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0811, dtype=torch.float64) <<\n",
      "tensor(0.6936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch232 >> Training Loss: tensor(0.6936, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0810, dtype=torch.float64) <<\n",
      "tensor(0.6933, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch233 >> Training Loss: tensor(0.6933, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0808, dtype=torch.float64) <<\n",
      "tensor(0.6930, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch234 >> Training Loss: tensor(0.6930, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.6926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch235 >> Training Loss: tensor(0.6926, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6922, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch236 >> Training Loss: tensor(0.6922, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch237 >> Training Loss: tensor(0.6919, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.6915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch238 >> Training Loss: tensor(0.6915, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.6911, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch239 >> Training Loss: tensor(0.6911, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0808, dtype=torch.float64) <<\n",
      "tensor(0.6908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch240 >> Training Loss: tensor(0.6908, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0809, dtype=torch.float64) <<\n",
      "tensor(0.6904, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch241 >> Training Loss: tensor(0.6904, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0809, dtype=torch.float64) <<\n",
      "tensor(0.6900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch242 >> Training Loss: tensor(0.6900, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0810, dtype=torch.float64) <<\n",
      "tensor(0.6897, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch243 >> Training Loss: tensor(0.6897, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0810, dtype=torch.float64) <<\n",
      "tensor(0.6893, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch244 >> Training Loss: tensor(0.6893, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0809, dtype=torch.float64) <<\n",
      "tensor(0.6889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch245 >> Training Loss: tensor(0.6889, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0809, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6886, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch246 >> Training Loss: tensor(0.6886, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0808, dtype=torch.float64) <<\n",
      "tensor(0.6882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch247 >> Training Loss: tensor(0.6882, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.6878, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch248 >> Training Loss: tensor(0.6878, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.6875, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch249 >> Training Loss: tensor(0.6875, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6871, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch250 >> Training Loss: tensor(0.6871, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6867, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch251 >> Training Loss: tensor(0.6867, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch252 >> Training Loss: tensor(0.6864, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6860, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch253 >> Training Loss: tensor(0.6860, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6856, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch254 >> Training Loss: tensor(0.6856, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch255 >> Training Loss: tensor(0.6852, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch256 >> Training Loss: tensor(0.6848, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6844, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch257 >> Training Loss: tensor(0.6844, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6841, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch258 >> Training Loss: tensor(0.6841, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6837, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch259 >> Training Loss: tensor(0.6837, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0806, dtype=torch.float64) <<\n",
      "tensor(0.6833, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch260 >> Training Loss: tensor(0.6833, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0805, dtype=torch.float64) <<\n",
      "tensor(0.6829, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch261 >> Training Loss: tensor(0.6829, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0805, dtype=torch.float64) <<\n",
      "tensor(0.6825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch262 >> Training Loss: tensor(0.6825, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0804, dtype=torch.float64) <<\n",
      "tensor(0.6821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch263 >> Training Loss: tensor(0.6821, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0804, dtype=torch.float64) <<\n",
      "tensor(0.6817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch264 >> Training Loss: tensor(0.6817, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0804, dtype=torch.float64) <<\n",
      "tensor(0.6813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch265 >> Training Loss: tensor(0.6813, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch266 >> Training Loss: tensor(0.6808, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch267 >> Training Loss: tensor(0.6804, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch268 >> Training Loss: tensor(0.6800, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch269 >> Training Loss: tensor(0.6796, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch270 >> Training Loss: tensor(0.6791, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6787, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch271 >> Training Loss: tensor(0.6787, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch272 >> Training Loss: tensor(0.6783, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0803, dtype=torch.float64) <<\n",
      "tensor(0.6778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch273 >> Training Loss: tensor(0.6778, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0802, dtype=torch.float64) <<\n",
      "tensor(0.6773, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch274 >> Training Loss: tensor(0.6773, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0802, dtype=torch.float64) <<\n",
      "tensor(0.6769, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch275 >> Training Loss: tensor(0.6769, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0802, dtype=torch.float64) <<\n",
      "tensor(0.6764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch276 >> Training Loss: tensor(0.6764, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0801, dtype=torch.float64) <<\n",
      "tensor(0.6759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch277 >> Training Loss: tensor(0.6759, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0801, dtype=torch.float64) <<\n",
      "tensor(0.6755, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch278 >> Training Loss: tensor(0.6755, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0801, dtype=torch.float64) <<\n",
      "tensor(0.6750, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch279 >> Training Loss: tensor(0.6750, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0800, dtype=torch.float64) <<\n",
      "tensor(0.6745, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch280 >> Training Loss: tensor(0.6745, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0800, dtype=torch.float64) <<\n",
      "tensor(0.6740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch281 >> Training Loss: tensor(0.6740, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0800, dtype=torch.float64) <<\n",
      "tensor(0.6734, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch282 >> Training Loss: tensor(0.6734, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0799, dtype=torch.float64) <<\n",
      "tensor(0.6729, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch283 >> Training Loss: tensor(0.6729, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0799, dtype=torch.float64) <<\n",
      "tensor(0.6724, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch284 >> Training Loss: tensor(0.6724, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0799, dtype=torch.float64) <<\n",
      "tensor(0.6718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch285 >> Training Loss: tensor(0.6718, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0799, dtype=torch.float64) <<\n",
      "tensor(0.6713, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch286 >> Training Loss: tensor(0.6713, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0798, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch287 >> Training Loss: tensor(0.6707, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0798, dtype=torch.float64) <<\n",
      "tensor(0.6701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch288 >> Training Loss: tensor(0.6701, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0798, dtype=torch.float64) <<\n",
      "tensor(0.6695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch289 >> Training Loss: tensor(0.6695, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0797, dtype=torch.float64) <<\n",
      "tensor(0.6689, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch290 >> Training Loss: tensor(0.6689, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0797, dtype=torch.float64) <<\n",
      "tensor(0.6683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch291 >> Training Loss: tensor(0.6683, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0796, dtype=torch.float64) <<\n",
      "tensor(0.6677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch292 >> Training Loss: tensor(0.6677, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0796, dtype=torch.float64) <<\n",
      "tensor(0.6670, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch293 >> Training Loss: tensor(0.6670, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0795, dtype=torch.float64) <<\n",
      "tensor(0.6663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch294 >> Training Loss: tensor(0.6663, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0795, dtype=torch.float64) <<\n",
      "tensor(0.6656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch295 >> Training Loss: tensor(0.6656, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0794, dtype=torch.float64) <<\n",
      "tensor(0.6649, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch296 >> Training Loss: tensor(0.6649, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0794, dtype=torch.float64) <<\n",
      "tensor(0.6642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch297 >> Training Loss: tensor(0.6642, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0793, dtype=torch.float64) <<\n",
      "tensor(0.6635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch298 >> Training Loss: tensor(0.6635, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0793, dtype=torch.float64) <<\n",
      "tensor(0.6627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch299 >> Training Loss: tensor(0.6627, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0792, dtype=torch.float64) <<\n",
      "tensor(0.6619, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch300 >> Training Loss: tensor(0.6619, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0792, dtype=torch.float64) <<\n",
      "tensor(0.6611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch301 >> Training Loss: tensor(0.6611, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0791, dtype=torch.float64) <<\n",
      "tensor(0.6602, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch302 >> Training Loss: tensor(0.6602, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0791, dtype=torch.float64) <<\n",
      "tensor(0.6593, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch303 >> Training Loss: tensor(0.6593, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0790, dtype=torch.float64) <<\n",
      "tensor(0.6584, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch304 >> Training Loss: tensor(0.6584, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0789, dtype=torch.float64) <<\n",
      "tensor(0.6575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch305 >> Training Loss: tensor(0.6575, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0788, dtype=torch.float64) <<\n",
      "tensor(0.6565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch306 >> Training Loss: tensor(0.6565, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0788, dtype=torch.float64) <<\n",
      "tensor(0.6555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch307 >> Training Loss: tensor(0.6555, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0787, dtype=torch.float64) <<\n",
      "tensor(0.6545, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch308 >> Training Loss: tensor(0.6545, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0786, dtype=torch.float64) <<\n",
      "tensor(0.6534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch309 >> Training Loss: tensor(0.6534, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0785, dtype=torch.float64) <<\n",
      "tensor(0.6523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch310 >> Training Loss: tensor(0.6523, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0784, dtype=torch.float64) <<\n",
      "tensor(0.6511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch311 >> Training Loss: tensor(0.6511, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0783, dtype=torch.float64) <<\n",
      "tensor(0.6499, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch312 >> Training Loss: tensor(0.6499, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0782, dtype=torch.float64) <<\n",
      "tensor(0.6486, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch313 >> Training Loss: tensor(0.6486, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0781, dtype=torch.float64) <<\n",
      "tensor(0.6473, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch314 >> Training Loss: tensor(0.6473, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0780, dtype=torch.float64) <<\n",
      "tensor(0.6459, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch315 >> Training Loss: tensor(0.6459, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0779, dtype=torch.float64) <<\n",
      "tensor(0.6445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch316 >> Training Loss: tensor(0.6445, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0777, dtype=torch.float64) <<\n",
      "tensor(0.6430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch317 >> Training Loss: tensor(0.6430, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0776, dtype=torch.float64) <<\n",
      "tensor(0.6414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch318 >> Training Loss: tensor(0.6414, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0774, dtype=torch.float64) <<\n",
      "tensor(0.6398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch319 >> Training Loss: tensor(0.6398, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0773, dtype=torch.float64) <<\n",
      "tensor(0.6381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch320 >> Training Loss: tensor(0.6381, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0771, dtype=torch.float64) <<\n",
      "tensor(0.6364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch321 >> Training Loss: tensor(0.6364, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0769, dtype=torch.float64) <<\n",
      "tensor(0.6345, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch322 >> Training Loss: tensor(0.6345, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0768, dtype=torch.float64) <<\n",
      "tensor(0.6327, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch323 >> Training Loss: tensor(0.6327, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0766, dtype=torch.float64) <<\n",
      "tensor(0.6307, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch324 >> Training Loss: tensor(0.6307, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0764, dtype=torch.float64) <<\n",
      "tensor(0.6287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch325 >> Training Loss: tensor(0.6287, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0761, dtype=torch.float64) <<\n",
      "tensor(0.6266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch326 >> Training Loss: tensor(0.6266, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0759, dtype=torch.float64) <<\n",
      "tensor(0.6244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch327 >> Training Loss: tensor(0.6244, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0757, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch328 >> Training Loss: tensor(0.6222, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0754, dtype=torch.float64) <<\n",
      "tensor(0.6199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch329 >> Training Loss: tensor(0.6199, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0751, dtype=torch.float64) <<\n",
      "tensor(0.6175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch330 >> Training Loss: tensor(0.6175, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0748, dtype=torch.float64) <<\n",
      "tensor(0.6151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch331 >> Training Loss: tensor(0.6151, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0745, dtype=torch.float64) <<\n",
      "tensor(0.6126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch332 >> Training Loss: tensor(0.6126, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0742, dtype=torch.float64) <<\n",
      "tensor(0.6101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch333 >> Training Loss: tensor(0.6101, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0739, dtype=torch.float64) <<\n",
      "tensor(0.6075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch334 >> Training Loss: tensor(0.6075, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0736, dtype=torch.float64) <<\n",
      "tensor(0.6049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch335 >> Training Loss: tensor(0.6049, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0732, dtype=torch.float64) <<\n",
      "tensor(0.6022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch336 >> Training Loss: tensor(0.6022, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0728, dtype=torch.float64) <<\n",
      "tensor(0.5994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch337 >> Training Loss: tensor(0.5994, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0724, dtype=torch.float64) <<\n",
      "tensor(0.5966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch338 >> Training Loss: tensor(0.5966, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0720, dtype=torch.float64) <<\n",
      "tensor(0.5936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch339 >> Training Loss: tensor(0.5936, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0716, dtype=torch.float64) <<\n",
      "tensor(0.5906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch340 >> Training Loss: tensor(0.5906, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0712, dtype=torch.float64) <<\n",
      "tensor(0.5873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch341 >> Training Loss: tensor(0.5873, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0707, dtype=torch.float64) <<\n",
      "tensor(0.5839, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch342 >> Training Loss: tensor(0.5839, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0702, dtype=torch.float64) <<\n",
      "tensor(0.5804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch343 >> Training Loss: tensor(0.5804, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0697, dtype=torch.float64) <<\n",
      "tensor(0.5766, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch344 >> Training Loss: tensor(0.5766, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0692, dtype=torch.float64) <<\n",
      "tensor(0.5732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch345 >> Training Loss: tensor(0.5732, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0694, dtype=torch.float64) <<\n",
      "tensor(0.5747, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch346 >> Training Loss: tensor(0.5747, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0772, dtype=torch.float64) <<\n",
      "tensor(0.6388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch347 >> Training Loss: tensor(0.6388, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1487, dtype=torch.float64) <<\n",
      "tensor(1.2147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch348 >> Training Loss: tensor(1.2147, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2113, dtype=torch.float64) <<\n",
      "tensor(1.7123, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch349 >> Training Loss: tensor(1.7123, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0721, dtype=torch.float64) <<\n",
      "tensor(0.5817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch350 >> Training Loss: tensor(0.5817, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1463, dtype=torch.float64) <<\n",
      "tensor(1.1966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch351 >> Training Loss: tensor(1.1966, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0794, dtype=torch.float64) <<\n",
      "tensor(0.6066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch352 >> Training Loss: tensor(0.6066, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1242, dtype=torch.float64) <<\n",
      "tensor(0.9449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch353 >> Training Loss: tensor(0.9449, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0974, dtype=torch.float64) <<\n",
      "tensor(0.7750, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch354 >> Training Loss: tensor(0.7750, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0823, dtype=torch.float64) <<\n",
      "tensor(0.7069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch355 >> Training Loss: tensor(0.7069, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1035, dtype=torch.float64) <<\n",
      "tensor(0.8723, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch356 >> Training Loss: tensor(0.8723, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0866, dtype=torch.float64) <<\n",
      "tensor(0.6728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch357 >> Training Loss: tensor(0.6728, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1058, dtype=torch.float64) <<\n",
      "tensor(0.7461, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch358 >> Training Loss: tensor(0.7461, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1176, dtype=torch.float64) <<\n",
      "tensor(0.8184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch359 >> Training Loss: tensor(0.8184, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0987, dtype=torch.float64) <<\n",
      "tensor(0.7146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch360 >> Training Loss: tensor(0.7146, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0825, dtype=torch.float64) <<\n",
      "tensor(0.6614, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch361 >> Training Loss: tensor(0.6614, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0844, dtype=torch.float64) <<\n",
      "tensor(0.7318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch362 >> Training Loss: tensor(0.7318, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0750, dtype=torch.float64) <<\n",
      "tensor(0.6824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch363 >> Training Loss: tensor(0.6824, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0692, dtype=torch.float64) <<\n",
      "tensor(0.6425, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch364 >> Training Loss: tensor(0.6425, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0765, dtype=torch.float64) <<\n",
      "tensor(0.6939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch365 >> Training Loss: tensor(0.6939, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0745, dtype=torch.float64) <<\n",
      "tensor(0.6621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch366 >> Training Loss: tensor(0.6621, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0719, dtype=torch.float64) <<\n",
      "tensor(0.6288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch367 >> Training Loss: tensor(0.6288, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0776, dtype=torch.float64) <<\n",
      "tensor(0.6702, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch368 >> Training Loss: tensor(0.6702, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0767, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch369 >> Training Loss: tensor(0.6647, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0700, dtype=torch.float64) <<\n",
      "tensor(0.6220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch370 >> Training Loss: tensor(0.6220, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0710, dtype=torch.float64) <<\n",
      "tensor(0.6451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch371 >> Training Loss: tensor(0.6451, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0715, dtype=torch.float64) <<\n",
      "tensor(0.6548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch372 >> Training Loss: tensor(0.6548, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0686, dtype=torch.float64) <<\n",
      "tensor(0.6208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch373 >> Training Loss: tensor(0.6208, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0724, dtype=torch.float64) <<\n",
      "tensor(0.6358, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch374 >> Training Loss: tensor(0.6358, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0741, dtype=torch.float64) <<\n",
      "tensor(0.6427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch375 >> Training Loss: tensor(0.6427, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0704, dtype=torch.float64) <<\n",
      "tensor(0.6150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch376 >> Training Loss: tensor(0.6150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0712, dtype=torch.float64) <<\n",
      "tensor(0.6242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch377 >> Training Loss: tensor(0.6242, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0726, dtype=torch.float64) <<\n",
      "tensor(0.6305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch378 >> Training Loss: tensor(0.6305, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0718, dtype=torch.float64) <<\n",
      "tensor(0.6114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch379 >> Training Loss: tensor(0.6114, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0737, dtype=torch.float64) <<\n",
      "tensor(0.6166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch380 >> Training Loss: tensor(0.6166, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0741, dtype=torch.float64) <<\n",
      "tensor(0.6196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch381 >> Training Loss: tensor(0.6196, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0709, dtype=torch.float64) <<\n",
      "tensor(0.6037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch382 >> Training Loss: tensor(0.6037, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0699, dtype=torch.float64) <<\n",
      "tensor(0.6085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch383 >> Training Loss: tensor(0.6085, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0697, dtype=torch.float64) <<\n",
      "tensor(0.6093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch384 >> Training Loss: tensor(0.6093, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0693, dtype=torch.float64) <<\n",
      "tensor(0.5969, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch385 >> Training Loss: tensor(0.5969, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0711, dtype=torch.float64) <<\n",
      "tensor(0.6013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch386 >> Training Loss: tensor(0.6013, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0711, dtype=torch.float64) <<\n",
      "tensor(0.5978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch387 >> Training Loss: tensor(0.5978, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0695, dtype=torch.float64) <<\n",
      "tensor(0.5889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch388 >> Training Loss: tensor(0.5889, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0695, dtype=torch.float64) <<\n",
      "tensor(0.5927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch389 >> Training Loss: tensor(0.5927, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0690, dtype=torch.float64) <<\n",
      "tensor(0.5867, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch390 >> Training Loss: tensor(0.5867, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0688, dtype=torch.float64) <<\n",
      "tensor(0.5818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch391 >> Training Loss: tensor(0.5818, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0689, dtype=torch.float64) <<\n",
      "tensor(0.5835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch392 >> Training Loss: tensor(0.5835, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0669, dtype=torch.float64) <<\n",
      "tensor(0.5760, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch393 >> Training Loss: tensor(0.5760, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0658, dtype=torch.float64) <<\n",
      "tensor(0.5756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch394 >> Training Loss: tensor(0.5756, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0654, dtype=torch.float64) <<\n",
      "tensor(0.5736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch395 >> Training Loss: tensor(0.5736, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0656, dtype=torch.float64) <<\n",
      "tensor(0.5682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch396 >> Training Loss: tensor(0.5682, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0665, dtype=torch.float64) <<\n",
      "tensor(0.5694, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch397 >> Training Loss: tensor(0.5694, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0658, dtype=torch.float64) <<\n",
      "tensor(0.5634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch398 >> Training Loss: tensor(0.5634, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0655, dtype=torch.float64) <<\n",
      "tensor(0.5630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch399 >> Training Loss: tensor(0.5630, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0655, dtype=torch.float64) <<\n",
      "tensor(0.5595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch400 >> Training Loss: tensor(0.5595, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0660, dtype=torch.float64) <<\n",
      "tensor(0.5570, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch401 >> Training Loss: tensor(0.5570, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0659, dtype=torch.float64) <<\n",
      "tensor(0.5552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch402 >> Training Loss: tensor(0.5552, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0647, dtype=torch.float64) <<\n",
      "tensor(0.5519, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch403 >> Training Loss: tensor(0.5519, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0646, dtype=torch.float64) <<\n",
      "tensor(0.5510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch404 >> Training Loss: tensor(0.5510, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0657, dtype=torch.float64) <<\n",
      "tensor(0.5484, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch405 >> Training Loss: tensor(0.5484, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0662, dtype=torch.float64) <<\n",
      "tensor(0.5476, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch406 >> Training Loss: tensor(0.5476, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0654, dtype=torch.float64) <<\n",
      "tensor(0.5458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch407 >> Training Loss: tensor(0.5458, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0655, dtype=torch.float64) <<\n",
      "tensor(0.5438, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch408 >> Training Loss: tensor(0.5438, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0665, dtype=torch.float64) <<\n",
      "tensor(0.5434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch409 >> Training Loss: tensor(0.5434, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0650, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch410 >> Training Loss: tensor(0.5397, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0639, dtype=torch.float64) <<\n",
      "tensor(0.5391, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch411 >> Training Loss: tensor(0.5391, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0643, dtype=torch.float64) <<\n",
      "tensor(0.5364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch412 >> Training Loss: tensor(0.5364, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0635, dtype=torch.float64) <<\n",
      "tensor(0.5333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch413 >> Training Loss: tensor(0.5333, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0627, dtype=torch.float64) <<\n",
      "tensor(0.5318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch414 >> Training Loss: tensor(0.5318, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0637, dtype=torch.float64) <<\n",
      "tensor(0.5283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch415 >> Training Loss: tensor(0.5283, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0633, dtype=torch.float64) <<\n",
      "tensor(0.5244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch416 >> Training Loss: tensor(0.5244, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0630, dtype=torch.float64) <<\n",
      "tensor(0.5221, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch417 >> Training Loss: tensor(0.5221, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0640, dtype=torch.float64) <<\n",
      "tensor(0.5199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch418 >> Training Loss: tensor(0.5199, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0625, dtype=torch.float64) <<\n",
      "tensor(0.5169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch419 >> Training Loss: tensor(0.5169, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0627, dtype=torch.float64) <<\n",
      "tensor(0.5134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch420 >> Training Loss: tensor(0.5134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0615, dtype=torch.float64) <<\n",
      "tensor(0.5101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch421 >> Training Loss: tensor(0.5101, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0611, dtype=torch.float64) <<\n",
      "tensor(0.5071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch422 >> Training Loss: tensor(0.5071, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0607, dtype=torch.float64) <<\n",
      "tensor(0.5042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch423 >> Training Loss: tensor(0.5042, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0602, dtype=torch.float64) <<\n",
      "tensor(0.5013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch424 >> Training Loss: tensor(0.5013, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0610, dtype=torch.float64) <<\n",
      "tensor(0.4995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch425 >> Training Loss: tensor(0.4995, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0602, dtype=torch.float64) <<\n",
      "tensor(0.5042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch426 >> Training Loss: tensor(0.5042, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0690, dtype=torch.float64) <<\n",
      "tensor(0.5466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch427 >> Training Loss: tensor(0.5466, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0957, dtype=torch.float64) <<\n",
      "tensor(0.8659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch428 >> Training Loss: tensor(0.8659, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2231, dtype=torch.float64) <<\n",
      "tensor(1.8574, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch429 >> Training Loss: tensor(1.8574, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3352, dtype=torch.float64) <<\n",
      "tensor(2.9929, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch430 >> Training Loss: tensor(2.9929, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0966, dtype=torch.float64) <<\n",
      "tensor(0.8709, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch431 >> Training Loss: tensor(0.8709, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1292, dtype=torch.float64) <<\n",
      "tensor(1.0558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch432 >> Training Loss: tensor(1.0558, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1952, dtype=torch.float64) <<\n",
      "tensor(1.4916, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch433 >> Training Loss: tensor(1.4916, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2080, dtype=torch.float64) <<\n",
      "tensor(1.3838, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch434 >> Training Loss: tensor(1.3838, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2413, dtype=torch.float64) <<\n",
      "tensor(1.4503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch435 >> Training Loss: tensor(1.4503, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2473, dtype=torch.float64) <<\n",
      "tensor(1.4595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch436 >> Training Loss: tensor(1.4595, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1944, dtype=torch.float64) <<\n",
      "tensor(1.1880, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch437 >> Training Loss: tensor(1.1880, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1247, dtype=torch.float64) <<\n",
      "tensor(0.8640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch438 >> Training Loss: tensor(0.8640, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0954, dtype=torch.float64) <<\n",
      "tensor(0.7955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch439 >> Training Loss: tensor(0.7955, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.7878, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch440 >> Training Loss: tensor(0.7878, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0859, dtype=torch.float64) <<\n",
      "tensor(0.8535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch441 >> Training Loss: tensor(0.8535, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0994, dtype=torch.float64) <<\n",
      "tensor(0.9785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch442 >> Training Loss: tensor(0.9785, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0833, dtype=torch.float64) <<\n",
      "tensor(0.8142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch443 >> Training Loss: tensor(0.8142, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0845, dtype=torch.float64) <<\n",
      "tensor(0.7729, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch444 >> Training Loss: tensor(0.7729, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0929, dtype=torch.float64) <<\n",
      "tensor(0.8255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch445 >> Training Loss: tensor(0.8255, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0857, dtype=torch.float64) <<\n",
      "tensor(0.7600, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch446 >> Training Loss: tensor(0.7600, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0981, dtype=torch.float64) <<\n",
      "tensor(0.8286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch447 >> Training Loss: tensor(0.8286, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1031, dtype=torch.float64) <<\n",
      "tensor(0.8537, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch448 >> Training Loss: tensor(0.8537, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0911, dtype=torch.float64) <<\n",
      "tensor(0.7657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch449 >> Training Loss: tensor(0.7657, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0908, dtype=torch.float64) <<\n",
      "tensor(0.7671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch450 >> Training Loss: tensor(0.7671, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0889, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7513, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch451 >> Training Loss: tensor(0.7513, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0805, dtype=torch.float64) <<\n",
      "tensor(0.7014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch452 >> Training Loss: tensor(0.7014, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0841, dtype=torch.float64) <<\n",
      "tensor(0.7525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch453 >> Training Loss: tensor(0.7525, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0810, dtype=torch.float64) <<\n",
      "tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch454 >> Training Loss: tensor(0.7256, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0821, dtype=torch.float64) <<\n",
      "tensor(0.7139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch455 >> Training Loss: tensor(0.7139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0819, dtype=torch.float64) <<\n",
      "tensor(0.7122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch456 >> Training Loss: tensor(0.7122, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0737, dtype=torch.float64) <<\n",
      "tensor(0.6617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch457 >> Training Loss: tensor(0.6617, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0772, dtype=torch.float64) <<\n",
      "tensor(0.6815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch458 >> Training Loss: tensor(0.6815, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0791, dtype=torch.float64) <<\n",
      "tensor(0.6737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch459 >> Training Loss: tensor(0.6737, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0814, dtype=torch.float64) <<\n",
      "tensor(0.6738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch460 >> Training Loss: tensor(0.6738, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0847, dtype=torch.float64) <<\n",
      "tensor(0.6919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch461 >> Training Loss: tensor(0.6919, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0816, dtype=torch.float64) <<\n",
      "tensor(0.6669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch462 >> Training Loss: tensor(0.6669, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0805, dtype=torch.float64) <<\n",
      "tensor(0.6671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch463 >> Training Loss: tensor(0.6671, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0778, dtype=torch.float64) <<\n",
      "tensor(0.6612, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch464 >> Training Loss: tensor(0.6612, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0750, dtype=torch.float64) <<\n",
      "tensor(0.6510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch465 >> Training Loss: tensor(0.6510, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0763, dtype=torch.float64) <<\n",
      "tensor(0.6666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch466 >> Training Loss: tensor(0.6666, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0741, dtype=torch.float64) <<\n",
      "tensor(0.6579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch467 >> Training Loss: tensor(0.6579, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0742, dtype=torch.float64) <<\n",
      "tensor(0.6631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch468 >> Training Loss: tensor(0.6631, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0745, dtype=torch.float64) <<\n",
      "tensor(0.6595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch469 >> Training Loss: tensor(0.6595, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0750, dtype=torch.float64) <<\n",
      "tensor(0.6494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch470 >> Training Loss: tensor(0.6494, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0770, dtype=torch.float64) <<\n",
      "tensor(0.6532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch471 >> Training Loss: tensor(0.6532, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0771, dtype=torch.float64) <<\n",
      "tensor(0.6460, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch472 >> Training Loss: tensor(0.6460, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0783, dtype=torch.float64) <<\n",
      "tensor(0.6500, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch473 >> Training Loss: tensor(0.6500, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0784, dtype=torch.float64) <<\n",
      "tensor(0.6506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch474 >> Training Loss: tensor(0.6506, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0769, dtype=torch.float64) <<\n",
      "tensor(0.6453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch475 >> Training Loss: tensor(0.6453, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0759, dtype=torch.float64) <<\n",
      "tensor(0.6463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch476 >> Training Loss: tensor(0.6463, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0737, dtype=torch.float64) <<\n",
      "tensor(0.6402, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch477 >> Training Loss: tensor(0.6402, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0722, dtype=torch.float64) <<\n",
      "tensor(0.6399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch478 >> Training Loss: tensor(0.6399, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0713, dtype=torch.float64) <<\n",
      "tensor(0.6412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch479 >> Training Loss: tensor(0.6412, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0707, dtype=torch.float64) <<\n",
      "tensor(0.6388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch480 >> Training Loss: tensor(0.6388, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0710, dtype=torch.float64) <<\n",
      "tensor(0.6406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch481 >> Training Loss: tensor(0.6406, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0707, dtype=torch.float64) <<\n",
      "tensor(0.6376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch482 >> Training Loss: tensor(0.6376, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0709, dtype=torch.float64) <<\n",
      "tensor(0.6364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch483 >> Training Loss: tensor(0.6364, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0715, dtype=torch.float64) <<\n",
      "tensor(0.6365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch484 >> Training Loss: tensor(0.6365, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0719, dtype=torch.float64) <<\n",
      "tensor(0.6347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch485 >> Training Loss: tensor(0.6347, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0724, dtype=torch.float64) <<\n",
      "tensor(0.6360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch486 >> Training Loss: tensor(0.6360, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0724, dtype=torch.float64) <<\n",
      "tensor(0.6353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch487 >> Training Loss: tensor(0.6353, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0720, dtype=torch.float64) <<\n",
      "tensor(0.6343, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch488 >> Training Loss: tensor(0.6343, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0715, dtype=torch.float64) <<\n",
      "tensor(0.6343, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch489 >> Training Loss: tensor(0.6343, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0707, dtype=torch.float64) <<\n",
      "tensor(0.6326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch490 >> Training Loss: tensor(0.6326, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0702, dtype=torch.float64) <<\n",
      "tensor(0.6326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch491 >> Training Loss: tensor(0.6326, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0698, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch492 >> Training Loss: tensor(0.6325, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0694, dtype=torch.float64) <<\n",
      "tensor(0.6319, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch493 >> Training Loss: tensor(0.6319, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0693, dtype=torch.float64) <<\n",
      "tensor(0.6322, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch494 >> Training Loss: tensor(0.6322, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0694, dtype=torch.float64) <<\n",
      "tensor(0.6309, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch495 >> Training Loss: tensor(0.6309, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0697, dtype=torch.float64) <<\n",
      "tensor(0.6304, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch496 >> Training Loss: tensor(0.6304, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0701, dtype=torch.float64) <<\n",
      "tensor(0.6300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch497 >> Training Loss: tensor(0.6300, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0704, dtype=torch.float64) <<\n",
      "tensor(0.6293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch498 >> Training Loss: tensor(0.6293, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0707, dtype=torch.float64) <<\n",
      "tensor(0.6295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch499 >> Training Loss: tensor(0.6295, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0708, dtype=torch.float64) <<\n"
     ]
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "TR_LOSS = []\n",
    "VAL_LOSS = []\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    #print(net.hidden[0].shape)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            tr_loss = tr_loss + criterion(outputs, labels)\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    c = 0\n",
    "    \n",
    "    #net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            val_loss = val_loss + criterion(outputs, labels)\n",
    "            #print(val_loss)\n",
    "    VAL_LOSS.append(val_loss)\n",
    "    print('Epoch'+str(i)+ ' >> Training Loss: '+str(tr_loss)+ ' Validation Loss: '+str(val_loss)+ ' <<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5dd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOSS = []\n",
    "for t in TR_LOSS:\n",
    "    cc = t.detach().numpy()\n",
    "    TRT_LOSS.append(cc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a52f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5178ff5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(TRT_LOSS)\n",
    "plt.plot(VAL_LOSS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5625396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0165, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "accumulated_test_loss = 0.0\n",
    "net.reset_hidden_states(bs=1)\n",
    "it_test = iter(data_loaded.test_dataloader)\n",
    "c=0\n",
    "while c < int(X_test.shape[0]/1):\n",
    "    #net.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        #print(c)\n",
    "        c = c+1\n",
    "        item = next(it_test)\n",
    "        outputs = net(item[0])\n",
    "        accumulated_test_loss = accumulated_test_loss + criterion(outputs, item[1])        \n",
    "print(accumulated_test_loss/int(len(data_loaded.test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f01f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "794fb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('RESULTS_LOSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b881426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_array = []\n",
    "for z in TR_LOSS:\n",
    "    tr_loss_array.append(z.tolist())\n",
    "training_loss = pd.DataFrame(tr_loss_array, columns=['TR_LOSS'])\n",
    "training_loss.to_csv('../RESULTS_LOSS/TR_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca05f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_array = []\n",
    "for z in VAL_LOSS:\n",
    "    val_loss_array.append(z.tolist())\n",
    "val_loss = pd.DataFrame(val_loss_array, columns=['VAL_LOSS'])\n",
    "val_loss.to_csv('../RESULTS_LOSS/VAL_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2b97b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_array = [accumulated_test_loss]\n",
    "\n",
    "test2_loss = pd.DataFrame(test_loss_array, columns=['TEST_LOSS'])\n",
    "test2_loss.to_csv('../RESULTS_LOSS/TEST_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95819c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f70b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../SCALER_DUMPS/min_max_scaler_lookahead20.save']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVE THE SCALER\n",
    "import joblib\n",
    "scaler_filename = \"../SCALER_DUMPS/min_max_scaler_lookahead\"+str(output_cardinality)+\".save\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "# And now to load...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f65d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e0f979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE MODEL\n",
    "model_ckp_path = \"../MODEL_CHECKPOINTS/model_lookahead\"+str(output_cardinality)+\".pth\" \n",
    "torch.save(net.state_dict(), model_ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268595d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c07726f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net2 = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "#net2.load_state_dict(torch.load(model_ckp_path))\n",
    "#net2.eval()\n",
    "#scaler = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab93fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b98c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
