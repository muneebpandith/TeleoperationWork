{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e847bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cardinality=20\n",
    "output_cardinality=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0738c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.model_selection as sk\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81495b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning as pl\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ead879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa4384d9d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940b1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc51aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2683d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pl.seed_everything(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac4eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.filenames = os.listdir(DATA_PATH)\n",
    "        self.filenames = [filename for filename in self.filenames if filename.endswith(\".csv\")]\n",
    "        self.len_filenames = len(self.filenames)\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.data = []\n",
    "        for i in range(len(self.filenames)):\n",
    "            self.data.append(self.read_data(i))\n",
    "        return self.data\n",
    "    \n",
    "    def read_data(self,i):\n",
    "        return pd.read_csv(self.data_path+\"/\"+self.filenames[i])\n",
    "    \n",
    "\n",
    "    def process_data(self, scaler, input_cardinality=20, output_cardinality=10, cols=['ThetaXHG']):\n",
    "        self.datasamples = self.get_data()\n",
    "        X = []\n",
    "        y = []\n",
    "        X_scaled = []\n",
    "        y_scaled = []\n",
    "        for i, datasample in enumerate(self.datasamples):            \n",
    "            #for j in range(datasample[cols].shape[0]-output_cardinality):\n",
    "            #    print('Input' +str(j)+ ' to '+str(j+input_cardinality) + 'Output: '+str(j+input_cardinality)+ ' to '+str(j+input_cardinality+output_cardinality))\n",
    "            #Append to X (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto input_cardinality]]\n",
    "            #Append to Y (TO MAKE A LIST OF LISTS), [[0.019,1.02,...., upto output_cardinality]]\n",
    "            #start of Y will be ahead of end of X by 1\n",
    "            \n",
    "            for j in range(datasample.shape[0]-output_cardinality-input_cardinality+1):\n",
    "                X.append(datasample[cols].iloc[j:j+input_cardinality].to_numpy())\n",
    "                y.append(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy())\n",
    "                X_scaled.append(scaler.transform(datasample[cols].iloc[j:j+input_cardinality].to_numpy()))\n",
    "                y_scaled.append(scaler.transform(datasample[cols].iloc[j+input_cardinality: j+input_cardinality+output_cardinality].to_numpy()))\n",
    "        \n",
    "        #return self.datasamples, X, y\n",
    "        #print(np.array(X).shape,np.array(y).shape,np.array(X_scaled).shape, np.array(y_scaled).shape)\n",
    "        return self.datasamples, np.array(X), np.array(y), np.array(X_scaled), np.array(y_scaled)\n",
    "    \n",
    "    \n",
    "    def train_test_split(self, X, y, split={'train':0.8,'val':0.2, 'test':0.5}):\n",
    "        #print(X)\n",
    "        self.X_train, self.X_val_test, self.y_train, self.y_val_test = sk.train_test_split(X, y, test_size=split['val'] , random_state=43)\n",
    "        self.X_test, self.X_val, self.y_test, self.y_val = sk.train_test_split(self.X_val_test, self.y_val_test, test_size=split['test'] , random_state=43)\n",
    "        \n",
    "        return (self.X_train, self.y_train), (self.X_val, self.X_test), (self.X_test, self.y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getscaler(self,cols):\n",
    "        self.datasamples = self.get_data()       \n",
    "        for i, sample in enumerate(self.datasamples):\n",
    "            if i ==0:\n",
    "                features = pd.DataFrame(sample[cols])\n",
    "            else:\n",
    "                features = pd.DataFrame.append(features, sample[cols]) #sample[cols]\n",
    "   \n",
    "        #scaling\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        scaler = scaler.fit(features)\n",
    "        features_scaled = pd.DataFrame(scaler.transform(features), index = features.index, columns = cols)\n",
    "        return scaler\n",
    "    \n",
    "        \"\"\"        \n",
    "        #convert back to datasamples\n",
    "        #save to X\n",
    "        #save to X_scaled\n",
    "        \n",
    "        \n",
    "        #Split treain test and validation\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            self.datasamples = []\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        return features, features_scaled\n",
    "        \"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5167df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329ba98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASET(Dataset):\n",
    "    #convert to pytorch dataset\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, output = self.X[idx], self.y[idx]\n",
    "        return (torch.from_numpy(sequence.reshape(-1)), torch.from_numpy(output.reshape(-1)))\n",
    "        #return dict(sequence=torch.tensor(sequence.reshape(-1),dtype=torch.float64), label=torch.tensor(output.reshape(-1),dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d8d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4722168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class THETADATASETLOADER():\n",
    "    def __init__(self, data, batchsize, bs_val):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batchsize = batchsize\n",
    "        self.train_dataset = THETADATASET(self.data.X_train, self.data.y_train)\n",
    "        self.val_dataset = THETADATASET(self.data.X_val, self.data.y_val)\n",
    "        self.test_dataset = THETADATASET(self.data.X_test, self.data.y_test)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size = self.batchsize, shuffle= False, num_workers=0, worker_init_fn=seed_worker,generator=g)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size = bs_val, shuffle= False, num_workers=0,  worker_init_fn=seed_worker,generator=g)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size = 1, shuffle= False, num_workers=0,  worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c884b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1aa2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847ddc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446a3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders={}\n",
    "#dataloaders['train'], dataloaders['val'] = data_loaded.train_dataloader, data_loaded.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5411a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor f, o in tqdm(dataloaders['train']):\\n    #print(f[0].shape)\\n    break\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for f, o in tqdm(dataloaders['train']):\n",
    "    #print(f[0].shape)\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3389745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn(a,first['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a5a811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea08fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc39c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(DATA_LOADED, model, criterion, optimizer, num_epochs=25):\n",
    "    #train_model(DATA_LOADED, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    start = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 0\n",
    "    dataloaders ={}\n",
    "    dataloaders['train'], dataloaders['val'] = DATA_LOADED.train_dataloader, DATA_LOADED.val_dataloader\n",
    "\n",
    "    #TR_ACCURACY=[]\n",
    "    TR_LOSS=[]\n",
    "    #VAL_ACCURACY=[]\n",
    "    VAL_LOSS=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        model.reset_hidden_states()\n",
    "        #model.reset_hidden_states()\n",
    "        #print(model.hidden)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            #running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            #if phase == 'train':\n",
    "            #    scheduler.step()\n",
    "\n",
    "            #epoch_loss = running_loss / DATA_LOADED.dataset_sizes[phase]\n",
    "            epoch_loss = running_loss / dataset_size[phase]\n",
    "            \n",
    "            #epoch_acc = running_corrects.double() / DATA.dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "\n",
    "            if phase == 'train':\n",
    "                #  print(\"Training\")\n",
    "                #  TR_ACCURACY.append(epoch_acc)\n",
    "                TR_LOSS.append(epoch_loss)\n",
    "            else:\n",
    "                print(\"Valuation\")\n",
    "                #VAL_ACCURACY.append(epoch_acc)\n",
    "                VAL_LOSS.append(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return TR_LOSS, VAL_LOSS, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c88e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHALLEABLELSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).to(device).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def reset_hidden_states(self):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        #lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        #self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        #lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= ht[-1]\n",
    "        #lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin0_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63a3a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNET(nn.Module):\n",
    "    def __init__(self, batch_size, input_len, output_len, lstm_units = 4, num_layers=1):\n",
    "        super(LSTMNET, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #input_size = no of features = 1\n",
    "        #hidden_size = no of lstm units in the layer\n",
    "        #num_layers = no of lstm layers\n",
    "        self.lstm_units = lstm_units\n",
    "        self.lstm1 = nn.LSTM(input_size= 1, hidden_size= lstm_units, num_layers=num_layers,batch_first=True, dropout=0.6)\n",
    "        \n",
    "        self.linear0 = nn.Linear(in_features= 20, out_features=10)\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features= lstm_units, out_features=10)\n",
    "        self.linear2 = nn.Linear(in_features= 10, out_features=10)\n",
    "        self.ll = nn.Linear(in_features= 10, out_features=output_len)\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double(), torch.zeros(1*self.num_layers, self.batch_size, self.lstm_units).double())\n",
    "        #print(self.hidden[0].device)\n",
    "        #print(self.hidden.shape)\n",
    "        #self.hidden[0]= self.hidden[0].to(device)\n",
    "        #self.hidden[1] = self.hidden[1].to(device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def reset_hidden_states(self,bs):\n",
    "        self.hidden = (torch.zeros(1*self.num_layers, bs, self.lstm_units).double(), torch.zeros(1*self.num_layers, bs, self.lstm_units).double())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #print(x.unsqueeze(-1).shape)\n",
    "        #print(self.hidden.shape)\n",
    "        #print(self.hidden)\n",
    "        lstm_out, (h,c) = self.lstm1(x.unsqueeze(-1), self.hidden)\n",
    "        self.hidden= (h.detach(),c.detach())\n",
    "        #c.detach_()\n",
    "        #h.detach_()\n",
    "        #self.hidden = (h.detach(), c.detach())\n",
    "        #print(ht.shape)\n",
    "        #ht=ht.to(device)\n",
    "        #ct=ct.to(device)\n",
    "        \n",
    "        lstm_out = lstm_out[:,-1,:]\n",
    "        #print(ht.shape)\n",
    "        #either lstm_out goes to next or ht goes\n",
    "        #lstm_out= h.detach()[-1]\n",
    "        lin1_out = self.linear1(lstm_out)\n",
    "        #Add RELU\n",
    "        #lin0_out = F.relu(self.linear0(x))\n",
    "        ll_out = self.ll(lin1_out)\n",
    "        #x = self.linear0(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #Add RELU\n",
    "        return ll_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4360dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd04f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd86f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2414e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "batch_size = 16\n",
    "bs_val = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c141d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookahead10.ipynb  Lookahead2.ipynb  Lookahead6.ipynb  prediction.ipynb\r\n",
      "Lookahead15.ipynb  Lookahead3.ipynb  Lookahead7.ipynb  readme.md\r\n",
      "Lookahead1.ipynb   Lookahead4.ipynb  Lookahead8.ipynb\r\n",
      "Lookahead20.ipynb  Lookahead5.ipynb  Lookahead9.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3f936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../DATASET/TRANSORMER_DATA\"\n",
    "data = DATA(DATA_PATH)\n",
    "cols=['ThetaXHG']\n",
    "scaler = data.getscaler(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "957024ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, X, y, X_scaled, y_scaled = data.process_data(scaler, input_cardinality=input_cardinality, output_cardinality=output_cardinality, cols=['ThetaXHG'])\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = data.train_test_split(X_scaled,y_scaled,  split={'train':0.8,'val':0.2, 'test':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef3a02b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1012, 8, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "850ef43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toor/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "net = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "net = net.to(device)\n",
    "net = net.double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebb4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "598b88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first['sequence'].double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0cffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaded = THETADATASETLOADER(data, batchsize=batch_size, bs_val = bs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccc3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ffaa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size={}\n",
    "dataset_size['train'] =X_train.shape[0]\n",
    "dataset_size['val'] =X_val.shape[0]\n",
    "dataset_size['test'] =X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3db471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_LOSS = []\n",
    "VAL_LOSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d89481f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "722/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09609bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in range(epochs):\\n    net.reset_hidden_states(bs=batch_size)\\n    it = iter(data_loaded.train_dataloader)\\n    it_val = iter(data_loaded.train_dataloader)\\n    print('Epoch'+str(i))\\n    c = 0\\n    tr_loss = 0.0\\n    val_loss = 0.0\\n    \\n    while c < int(X_train.shape[0]/batch_size): \\n        with torch.set_grad_enabled(True):\\n            #print(c)\\n            c = c+1\\n            item = next(it)\\n            outputs = net(item[0])\\n            tr_loss = tr_loss + criterion(outputs, item[1])\\n    \\n    TR_LOSS.append(tr_loss)\\n    print(tr_loss)\\n    optimizer.zero_grad()\\n    tr_loss.backward()\\n    optimizer.step()\\n    \\n    net.reset_hidden_states(bs=bs_val)\\n    #print(net.hidden[0].shape)\\n    while c < int(X_val.shape[0]/bs_val):\\n        net.eval()\\n        with torch.set_grad_enabled(False):\\n            #print(c)\\n            c = c+1\\n            item = next(it_val)\\n            outputs = net(item[0])\\n            val_loss = val_loss + criterion(outputs, item[1])\\n    VAL_LOSS.append(val_loss)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "\"\"\"for i in range(epochs):\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    print('Epoch'+str(i))\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            outputs = net(item[0])\n",
    "            tr_loss = tr_loss + criterion(outputs, item[1])\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            outputs = net(item[0])\n",
    "            val_loss = val_loss + criterion(outputs, item[1])\n",
    "    VAL_LOSS.append(val_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "651ccb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.5074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch0 >> Training Loss: tensor(13.5074, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(1.2862, dtype=torch.float64) <<\n",
      "tensor(9.3787, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch1 >> Training Loss: tensor(9.3787, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.9574, dtype=torch.float64) <<\n",
      "tensor(6.7490, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch2 >> Training Loss: tensor(6.7490, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.8373, dtype=torch.float64) <<\n",
      "tensor(6.0178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch3 >> Training Loss: tensor(6.0178, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.8507, dtype=torch.float64) <<\n",
      "tensor(6.2707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch4 >> Training Loss: tensor(6.2707, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.8065, dtype=torch.float64) <<\n",
      "tensor(5.8663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch5 >> Training Loss: tensor(5.8663, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.7882, dtype=torch.float64) <<\n",
      "tensor(5.6271, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch6 >> Training Loss: tensor(5.6271, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.7554, dtype=torch.float64) <<\n",
      "tensor(5.3571, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch7 >> Training Loss: tensor(5.3571, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.6723, dtype=torch.float64) <<\n",
      "tensor(4.7796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch8 >> Training Loss: tensor(4.7796, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.6016, dtype=torch.float64) <<\n",
      "tensor(4.3508, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch9 >> Training Loss: tensor(4.3508, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.5326, dtype=torch.float64) <<\n",
      "tensor(3.9095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch10 >> Training Loss: tensor(3.9095, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.4387, dtype=torch.float64) <<\n",
      "tensor(3.2288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch11 >> Training Loss: tensor(3.2288, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3435, dtype=torch.float64) <<\n",
      "tensor(2.5449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch12 >> Training Loss: tensor(2.5449, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2384, dtype=torch.float64) <<\n",
      "tensor(1.7610, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch13 >> Training Loss: tensor(1.7610, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1841, dtype=torch.float64) <<\n",
      "tensor(1.2974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch14 >> Training Loss: tensor(1.2974, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1861, dtype=torch.float64) <<\n",
      "tensor(1.2020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch15 >> Training Loss: tensor(1.2020, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2504, dtype=torch.float64) <<\n",
      "tensor(1.6393, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch16 >> Training Loss: tensor(1.6393, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2578, dtype=torch.float64) <<\n",
      "tensor(1.6910, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch17 >> Training Loss: tensor(1.6910, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1781, dtype=torch.float64) <<\n",
      "tensor(1.1523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch18 >> Training Loss: tensor(1.1523, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1173, dtype=torch.float64) <<\n",
      "tensor(0.7487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch19 >> Training Loss: tensor(0.7487, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0883, dtype=torch.float64) <<\n",
      "tensor(0.5779, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch20 >> Training Loss: tensor(0.5779, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0843, dtype=torch.float64) <<\n",
      "tensor(0.5756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch21 >> Training Loss: tensor(0.5756, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0939, dtype=torch.float64) <<\n",
      "tensor(0.6457, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch22 >> Training Loss: tensor(0.6457, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1016, dtype=torch.float64) <<\n",
      "tensor(0.7024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch23 >> Training Loss: tensor(0.7024, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0987, dtype=torch.float64) <<\n",
      "tensor(0.6907, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch24 >> Training Loss: tensor(0.6907, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0987, dtype=torch.float64) <<\n",
      "tensor(0.6947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch25 >> Training Loss: tensor(0.6947, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0913, dtype=torch.float64) <<\n",
      "tensor(0.6188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch26 >> Training Loss: tensor(0.6188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0905, dtype=torch.float64) <<\n",
      "tensor(0.5993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch27 >> Training Loss: tensor(0.5993, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0835, dtype=torch.float64) <<\n",
      "tensor(0.5314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch28 >> Training Loss: tensor(0.5314, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0891, dtype=torch.float64) <<\n",
      "tensor(0.5523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch29 >> Training Loss: tensor(0.5523, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0919, dtype=torch.float64) <<\n",
      "tensor(0.5420, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch30 >> Training Loss: tensor(0.5420, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1014, dtype=torch.float64) <<\n",
      "tensor(0.5939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch31 >> Training Loss: tensor(0.5939, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1024, dtype=torch.float64) <<\n",
      "tensor(0.5825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch32 >> Training Loss: tensor(0.5825, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1007, dtype=torch.float64) <<\n",
      "tensor(0.5699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch33 >> Training Loss: tensor(0.5699, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0912, dtype=torch.float64) <<\n",
      "tensor(0.5150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch34 >> Training Loss: tensor(0.5150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0807, dtype=torch.float64) <<\n",
      "tensor(0.4560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch35 >> Training Loss: tensor(0.4560, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0736, dtype=torch.float64) <<\n",
      "tensor(0.4253, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch36 >> Training Loss: tensor(0.4253, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0674, dtype=torch.float64) <<\n",
      "tensor(0.3977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch37 >> Training Loss: tensor(0.3977, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0675, dtype=torch.float64) <<\n",
      "tensor(0.4134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch38 >> Training Loss: tensor(0.4134, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0657, dtype=torch.float64) <<\n",
      "tensor(0.4115, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch39 >> Training Loss: tensor(0.4115, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0667, dtype=torch.float64) <<\n",
      "tensor(0.4285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch40 >> Training Loss: tensor(0.4285, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0659, dtype=torch.float64) <<\n",
      "tensor(0.4185, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch41 >> Training Loss: tensor(0.4185, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0652, dtype=torch.float64) <<\n",
      "tensor(0.4107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch42 >> Training Loss: tensor(0.4107, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0634, dtype=torch.float64) <<\n",
      "tensor(0.3944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch43 >> Training Loss: tensor(0.3944, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0625, dtype=torch.float64) <<\n",
      "tensor(0.3775, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch44 >> Training Loss: tensor(0.3775, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0643, dtype=torch.float64) <<\n",
      "tensor(0.3789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch45 >> Training Loss: tensor(0.3789, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0653, dtype=torch.float64) <<\n",
      "tensor(0.3761, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch46 >> Training Loss: tensor(0.3761, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0677, dtype=torch.float64) <<\n",
      "tensor(0.3862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch47 >> Training Loss: tensor(0.3862, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0692, dtype=torch.float64) <<\n",
      "tensor(0.3927, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch48 >> Training Loss: tensor(0.3927, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0681, dtype=torch.float64) <<\n",
      "tensor(0.3845, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch49 >> Training Loss: tensor(0.3845, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0667, dtype=torch.float64) <<\n",
      "tensor(0.3787, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch50 >> Training Loss: tensor(0.3787, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0648, dtype=torch.float64) <<\n",
      "tensor(0.3697, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch51 >> Training Loss: tensor(0.3697, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0622, dtype=torch.float64) <<\n",
      "tensor(0.3580, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch52 >> Training Loss: tensor(0.3580, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0612, dtype=torch.float64) <<\n",
      "tensor(0.3577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch53 >> Training Loss: tensor(0.3577, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0613, dtype=torch.float64) <<\n",
      "tensor(0.3611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch54 >> Training Loss: tensor(0.3611, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0607, dtype=torch.float64) <<\n",
      "tensor(0.3610, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch55 >> Training Loss: tensor(0.3610, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0609, dtype=torch.float64) <<\n",
      "tensor(0.3633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch56 >> Training Loss: tensor(0.3633, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0614, dtype=torch.float64) <<\n",
      "tensor(0.3635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch57 >> Training Loss: tensor(0.3635, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0608, dtype=torch.float64) <<\n",
      "tensor(0.3576, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch58 >> Training Loss: tensor(0.3576, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0608, dtype=torch.float64) <<\n",
      "tensor(0.3530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch59 >> Training Loss: tensor(0.3530, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0616, dtype=torch.float64) <<\n",
      "tensor(0.3523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch60 >> Training Loss: tensor(0.3523, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0618, dtype=torch.float64) <<\n",
      "tensor(0.3515, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch61 >> Training Loss: tensor(0.3515, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0623, dtype=torch.float64) <<\n",
      "tensor(0.3505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch62 >> Training Loss: tensor(0.3505, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0627, dtype=torch.float64) <<\n",
      "tensor(0.3510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch63 >> Training Loss: tensor(0.3510, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0625, dtype=torch.float64) <<\n",
      "tensor(0.3515, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch64 >> Training Loss: tensor(0.3515, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0622, dtype=torch.float64) <<\n",
      "tensor(0.3495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch65 >> Training Loss: tensor(0.3495, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0611, dtype=torch.float64) <<\n",
      "tensor(0.3461, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch66 >> Training Loss: tensor(0.3461, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0604, dtype=torch.float64) <<\n",
      "tensor(0.3439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch67 >> Training Loss: tensor(0.3439, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0600, dtype=torch.float64) <<\n",
      "tensor(0.3439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch68 >> Training Loss: tensor(0.3439, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0595, dtype=torch.float64) <<\n",
      "tensor(0.3445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch69 >> Training Loss: tensor(0.3445, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0595, dtype=torch.float64) <<\n",
      "tensor(0.3443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch70 >> Training Loss: tensor(0.3443, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0593, dtype=torch.float64) <<\n",
      "tensor(0.3437, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch71 >> Training Loss: tensor(0.3437, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0593, dtype=torch.float64) <<\n",
      "tensor(0.3432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch72 >> Training Loss: tensor(0.3432, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0596, dtype=torch.float64) <<\n",
      "tensor(0.3428, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch73 >> Training Loss: tensor(0.3428, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0597, dtype=torch.float64) <<\n",
      "tensor(0.3424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch74 >> Training Loss: tensor(0.3424, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0600, dtype=torch.float64) <<\n",
      "tensor(0.3417, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch75 >> Training Loss: tensor(0.3417, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0601, dtype=torch.float64) <<\n",
      "tensor(0.3411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch76 >> Training Loss: tensor(0.3411, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0602, dtype=torch.float64) <<\n",
      "tensor(0.3407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch77 >> Training Loss: tensor(0.3407, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0602, dtype=torch.float64) <<\n",
      "tensor(0.3403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch78 >> Training Loss: tensor(0.3403, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0600, dtype=torch.float64) <<\n",
      "tensor(0.3399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch79 >> Training Loss: tensor(0.3399, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0598, dtype=torch.float64) <<\n",
      "tensor(0.3395, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch80 >> Training Loss: tensor(0.3395, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0594, dtype=torch.float64) <<\n",
      "tensor(0.3391, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch81 >> Training Loss: tensor(0.3391, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0594, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch82 >> Training Loss: tensor(0.3389, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0590, dtype=torch.float64) <<\n",
      "tensor(0.3387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch83 >> Training Loss: tensor(0.3387, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0591, dtype=torch.float64) <<\n",
      "tensor(0.3386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch84 >> Training Loss: tensor(0.3386, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0589, dtype=torch.float64) <<\n",
      "tensor(0.3384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch85 >> Training Loss: tensor(0.3384, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0592, dtype=torch.float64) <<\n",
      "tensor(0.3382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch86 >> Training Loss: tensor(0.3382, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0590, dtype=torch.float64) <<\n",
      "tensor(0.3382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch87 >> Training Loss: tensor(0.3382, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0596, dtype=torch.float64) <<\n",
      "tensor(0.3388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch88 >> Training Loss: tensor(0.3388, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0596, dtype=torch.float64) <<\n",
      "tensor(0.3404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch89 >> Training Loss: tensor(0.3404, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0607, dtype=torch.float64) <<\n",
      "tensor(0.3444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch90 >> Training Loss: tensor(0.3444, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0611, dtype=torch.float64) <<\n",
      "tensor(0.3532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch91 >> Training Loss: tensor(0.3532, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0645, dtype=torch.float64) <<\n",
      "tensor(0.3738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch92 >> Training Loss: tensor(0.3738, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0678, dtype=torch.float64) <<\n",
      "tensor(0.4144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch93 >> Training Loss: tensor(0.4144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0784, dtype=torch.float64) <<\n",
      "tensor(0.4857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch94 >> Training Loss: tensor(0.4857, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0808, dtype=torch.float64) <<\n",
      "tensor(0.5303, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch95 >> Training Loss: tensor(0.5303, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0780, dtype=torch.float64) <<\n",
      "tensor(0.4847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch96 >> Training Loss: tensor(0.4847, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0608, dtype=torch.float64) <<\n",
      "tensor(0.3596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch97 >> Training Loss: tensor(0.3596, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0599, dtype=torch.float64) <<\n",
      "tensor(0.3525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch98 >> Training Loss: tensor(0.3525, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0716, dtype=torch.float64) <<\n",
      "tensor(0.4356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch99 >> Training Loss: tensor(0.4356, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0670, dtype=torch.float64) <<\n",
      "tensor(0.4148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch100 >> Training Loss: tensor(0.4148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0594, dtype=torch.float64) <<\n",
      "tensor(0.3392, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch101 >> Training Loss: tensor(0.3392, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0625, dtype=torch.float64) <<\n",
      "tensor(0.3611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch102 >> Training Loss: tensor(0.3611, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0664, dtype=torch.float64) <<\n",
      "tensor(0.4033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch103 >> Training Loss: tensor(0.4033, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0630, dtype=torch.float64) <<\n",
      "tensor(0.3621, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch104 >> Training Loss: tensor(0.3621, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0595, dtype=torch.float64) <<\n",
      "tensor(0.3349, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch105 >> Training Loss: tensor(0.3349, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0635, dtype=torch.float64) <<\n",
      "tensor(0.3740, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch106 >> Training Loss: tensor(0.3740, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0643, dtype=torch.float64) <<\n",
      "tensor(0.3719, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch107 >> Training Loss: tensor(0.3719, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0590, dtype=torch.float64) <<\n",
      "tensor(0.3341, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch108 >> Training Loss: tensor(0.3341, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0605, dtype=torch.float64) <<\n",
      "tensor(0.3506, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch109 >> Training Loss: tensor(0.3506, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0633, dtype=torch.float64) <<\n",
      "tensor(0.3667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch110 >> Training Loss: tensor(0.3667, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0589, dtype=torch.float64) <<\n",
      "tensor(0.3385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch111 >> Training Loss: tensor(0.3385, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0588, dtype=torch.float64) <<\n",
      "tensor(0.3383, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch112 >> Training Loss: tensor(0.3383, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0619, dtype=torch.float64) <<\n",
      "tensor(0.3573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch113 >> Training Loss: tensor(0.3573, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0591, dtype=torch.float64) <<\n",
      "tensor(0.3417, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch114 >> Training Loss: tensor(0.3417, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0583, dtype=torch.float64) <<\n",
      "tensor(0.3324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch115 >> Training Loss: tensor(0.3324, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0607, dtype=torch.float64) <<\n",
      "tensor(0.3472, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch116 >> Training Loss: tensor(0.3472, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0594, dtype=torch.float64) <<\n",
      "tensor(0.3430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch117 >> Training Loss: tensor(0.3430, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0584, dtype=torch.float64) <<\n",
      "tensor(0.3306, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch118 >> Training Loss: tensor(0.3306, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0597, dtype=torch.float64) <<\n",
      "tensor(0.3381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch119 >> Training Loss: tensor(0.3381, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0595, dtype=torch.float64) <<\n",
      "tensor(0.3423, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch120 >> Training Loss: tensor(0.3423, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0588, dtype=torch.float64) <<\n",
      "tensor(0.3323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch121 >> Training Loss: tensor(0.3323, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0586, dtype=torch.float64) <<\n",
      "tensor(0.3315, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch122 >> Training Loss: tensor(0.3315, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0589, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3383, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch123 >> Training Loss: tensor(0.3383, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0590, dtype=torch.float64) <<\n",
      "tensor(0.3347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch124 >> Training Loss: tensor(0.3347, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0579, dtype=torch.float64) <<\n",
      "tensor(0.3290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch125 >> Training Loss: tensor(0.3290, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0581, dtype=torch.float64) <<\n",
      "tensor(0.3329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch126 >> Training Loss: tensor(0.3329, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0589, dtype=torch.float64) <<\n",
      "tensor(0.3351, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch127 >> Training Loss: tensor(0.3351, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0578, dtype=torch.float64) <<\n",
      "tensor(0.3301, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch128 >> Training Loss: tensor(0.3301, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0577, dtype=torch.float64) <<\n",
      "tensor(0.3288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch129 >> Training Loss: tensor(0.3288, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0585, dtype=torch.float64) <<\n",
      "tensor(0.3322, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch130 >> Training Loss: tensor(0.3322, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0580, dtype=torch.float64) <<\n",
      "tensor(0.3316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch131 >> Training Loss: tensor(0.3316, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0579, dtype=torch.float64) <<\n",
      "tensor(0.3280, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch132 >> Training Loss: tensor(0.3280, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0580, dtype=torch.float64) <<\n",
      "tensor(0.3282, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch133 >> Training Loss: tensor(0.3282, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0579, dtype=torch.float64) <<\n",
      "tensor(0.3304, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch134 >> Training Loss: tensor(0.3304, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0582, dtype=torch.float64) <<\n",
      "tensor(0.3295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch135 >> Training Loss: tensor(0.3295, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0576, dtype=torch.float64) <<\n",
      "tensor(0.3270, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch136 >> Training Loss: tensor(0.3270, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0576, dtype=torch.float64) <<\n",
      "tensor(0.3271, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch137 >> Training Loss: tensor(0.3271, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0580, dtype=torch.float64) <<\n",
      "tensor(0.3285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch138 >> Training Loss: tensor(0.3285, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0575, dtype=torch.float64) <<\n",
      "tensor(0.3280, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch139 >> Training Loss: tensor(0.3280, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0575, dtype=torch.float64) <<\n",
      "tensor(0.3263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch140 >> Training Loss: tensor(0.3263, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0575, dtype=torch.float64) <<\n",
      "tensor(0.3259, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch141 >> Training Loss: tensor(0.3259, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0574, dtype=torch.float64) <<\n",
      "tensor(0.3268, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch142 >> Training Loss: tensor(0.3268, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0577, dtype=torch.float64) <<\n",
      "tensor(0.3269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch143 >> Training Loss: tensor(0.3269, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0573, dtype=torch.float64) <<\n",
      "tensor(0.3258, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch144 >> Training Loss: tensor(0.3258, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0573, dtype=torch.float64) <<\n",
      "tensor(0.3249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch145 >> Training Loss: tensor(0.3249, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0574, dtype=torch.float64) <<\n",
      "tensor(0.3250, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch146 >> Training Loss: tensor(0.3250, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0573, dtype=torch.float64) <<\n",
      "tensor(0.3254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch147 >> Training Loss: tensor(0.3254, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0575, dtype=torch.float64) <<\n",
      "tensor(0.3251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch148 >> Training Loss: tensor(0.3251, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0572, dtype=torch.float64) <<\n",
      "tensor(0.3243, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch149 >> Training Loss: tensor(0.3243, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0572, dtype=torch.float64) <<\n",
      "tensor(0.3237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch150 >> Training Loss: tensor(0.3237, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0572, dtype=torch.float64) <<\n",
      "tensor(0.3236, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch151 >> Training Loss: tensor(0.3236, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0570, dtype=torch.float64) <<\n",
      "tensor(0.3238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch152 >> Training Loss: tensor(0.3238, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0572, dtype=torch.float64) <<\n",
      "tensor(0.3237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch153 >> Training Loss: tensor(0.3237, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0570, dtype=torch.float64) <<\n",
      "tensor(0.3232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch154 >> Training Loss: tensor(0.3232, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0570, dtype=torch.float64) <<\n",
      "tensor(0.3226, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch155 >> Training Loss: tensor(0.3226, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0569, dtype=torch.float64) <<\n",
      "tensor(0.3222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch156 >> Training Loss: tensor(0.3222, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0569, dtype=torch.float64) <<\n",
      "tensor(0.3221, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch157 >> Training Loss: tensor(0.3221, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0570, dtype=torch.float64) <<\n",
      "tensor(0.3220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch158 >> Training Loss: tensor(0.3220, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0568, dtype=torch.float64) <<\n",
      "tensor(0.3219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch159 >> Training Loss: tensor(0.3219, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0570, dtype=torch.float64) <<\n",
      "tensor(0.3216, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch160 >> Training Loss: tensor(0.3216, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0567, dtype=torch.float64) <<\n",
      "tensor(0.3212, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch161 >> Training Loss: tensor(0.3212, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0568, dtype=torch.float64) <<\n",
      "tensor(0.3208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch162 >> Training Loss: tensor(0.3208, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch163 >> Training Loss: tensor(0.3204, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch164 >> Training Loss: tensor(0.3200, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch165 >> Training Loss: tensor(0.3197, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0565, dtype=torch.float64) <<\n",
      "tensor(0.3195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch166 >> Training Loss: tensor(0.3195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0565, dtype=torch.float64) <<\n",
      "tensor(0.3192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch167 >> Training Loss: tensor(0.3192, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0564, dtype=torch.float64) <<\n",
      "tensor(0.3190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch168 >> Training Loss: tensor(0.3190, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0565, dtype=torch.float64) <<\n",
      "tensor(0.3189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch169 >> Training Loss: tensor(0.3189, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0563, dtype=torch.float64) <<\n",
      "tensor(0.3188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch170 >> Training Loss: tensor(0.3188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch171 >> Training Loss: tensor(0.3189, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0562, dtype=torch.float64) <<\n",
      "tensor(0.3193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch172 >> Training Loss: tensor(0.3193, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0569, dtype=torch.float64) <<\n",
      "tensor(0.3206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch173 >> Training Loss: tensor(0.3206, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0565, dtype=torch.float64) <<\n",
      "tensor(0.3236, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch174 >> Training Loss: tensor(0.3236, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0584, dtype=torch.float64) <<\n",
      "tensor(0.3311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch175 >> Training Loss: tensor(0.3311, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0589, dtype=torch.float64) <<\n",
      "tensor(0.3487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch176 >> Training Loss: tensor(0.3487, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0666, dtype=torch.float64) <<\n",
      "tensor(0.3924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch177 >> Training Loss: tensor(0.3924, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0739, dtype=torch.float64) <<\n",
      "tensor(0.4849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch178 >> Training Loss: tensor(0.4849, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1013, dtype=torch.float64) <<\n",
      "tensor(0.6681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch179 >> Training Loss: tensor(0.6681, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1088, dtype=torch.float64) <<\n",
      "tensor(0.7912, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch180 >> Training Loss: tensor(0.7912, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1021, dtype=torch.float64) <<\n",
      "tensor(0.6758, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch181 >> Training Loss: tensor(0.6758, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0581, dtype=torch.float64) <<\n",
      "tensor(0.3495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch182 >> Training Loss: tensor(0.3495, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0660, dtype=torch.float64) <<\n",
      "tensor(0.4222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch183 >> Training Loss: tensor(0.4222, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0899, dtype=torch.float64) <<\n",
      "tensor(0.5808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch184 >> Training Loss: tensor(0.5808, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0602, dtype=torch.float64) <<\n",
      "tensor(0.3659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch185 >> Training Loss: tensor(0.3659, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0621, dtype=torch.float64) <<\n",
      "tensor(0.3812, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch186 >> Training Loss: tensor(0.3812, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0787, dtype=torch.float64) <<\n",
      "tensor(0.4887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch187 >> Training Loss: tensor(0.4887, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0572, dtype=torch.float64) <<\n",
      "tensor(0.3286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch188 >> Training Loss: tensor(0.3286, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0653, dtype=torch.float64) <<\n",
      "tensor(0.4006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch189 >> Training Loss: tensor(0.4006, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0695, dtype=torch.float64) <<\n",
      "tensor(0.4129, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch190 >> Training Loss: tensor(0.4129, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0571, dtype=torch.float64) <<\n",
      "tensor(0.3201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch191 >> Training Loss: tensor(0.3201, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0665, dtype=torch.float64) <<\n",
      "tensor(0.4120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch192 >> Training Loss: tensor(0.4120, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0596, dtype=torch.float64) <<\n",
      "tensor(0.3395, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch193 >> Training Loss: tensor(0.3395, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0613, dtype=torch.float64) <<\n",
      "tensor(0.3534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch194 >> Training Loss: tensor(0.3534, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0613, dtype=torch.float64) <<\n",
      "tensor(0.3714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch195 >> Training Loss: tensor(0.3714, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0559, dtype=torch.float64) <<\n",
      "tensor(0.3203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch196 >> Training Loss: tensor(0.3203, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0634, dtype=torch.float64) <<\n",
      "tensor(0.3706, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch197 >> Training Loss: tensor(0.3706, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0560, dtype=torch.float64) <<\n",
      "tensor(0.3212, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch198 >> Training Loss: tensor(0.3212, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0588, dtype=torch.float64) <<\n",
      "tensor(0.3489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch199 >> Training Loss: tensor(0.3489, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0589, dtype=torch.float64) <<\n",
      "tensor(0.3356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch200 >> Training Loss: tensor(0.3356, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0577, dtype=torch.float64) <<\n",
      "tensor(0.3261, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch201 >> Training Loss: tensor(0.3261, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0586, dtype=torch.float64) <<\n",
      "tensor(0.3445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch202 >> Training Loss: tensor(0.3445, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0560, dtype=torch.float64) <<\n",
      "tensor(0.3159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch203 >> Training Loss: tensor(0.3159, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0597, dtype=torch.float64) <<\n",
      "tensor(0.3400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch204 >> Training Loss: tensor(0.3400, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0559, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch205 >> Training Loss: tensor(0.3185, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3280, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch206 >> Training Loss: tensor(0.3280, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0574, dtype=torch.float64) <<\n",
      "tensor(0.3253, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch207 >> Training Loss: tensor(0.3253, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0562, dtype=torch.float64) <<\n",
      "tensor(0.3176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch208 >> Training Loss: tensor(0.3176, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0563, dtype=torch.float64) <<\n",
      "tensor(0.3283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch209 >> Training Loss: tensor(0.3283, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0553, dtype=torch.float64) <<\n",
      "tensor(0.3137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch210 >> Training Loss: tensor(0.3137, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0573, dtype=torch.float64) <<\n",
      "tensor(0.3249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch211 >> Training Loss: tensor(0.3249, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0552, dtype=torch.float64) <<\n",
      "tensor(0.3156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch212 >> Training Loss: tensor(0.3156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0554, dtype=torch.float64) <<\n",
      "tensor(0.3182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch213 >> Training Loss: tensor(0.3182, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch214 >> Training Loss: tensor(0.3190, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0555, dtype=torch.float64) <<\n",
      "tensor(0.3130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch215 >> Training Loss: tensor(0.3130, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0556, dtype=torch.float64) <<\n",
      "tensor(0.3195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch216 >> Training Loss: tensor(0.3195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0553, dtype=torch.float64) <<\n",
      "tensor(0.3121, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch217 >> Training Loss: tensor(0.3121, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0561, dtype=torch.float64) <<\n",
      "tensor(0.3160, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch218 >> Training Loss: tensor(0.3160, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0550, dtype=torch.float64) <<\n",
      "tensor(0.3140, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch219 >> Training Loss: tensor(0.3140, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch220 >> Training Loss: tensor(0.3118, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0559, dtype=torch.float64) <<\n",
      "tensor(0.3148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch221 >> Training Loss: tensor(0.3148, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch222 >> Training Loss: tensor(0.3102, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3129, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch223 >> Training Loss: tensor(0.3129, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0552, dtype=torch.float64) <<\n",
      "tensor(0.3109, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch224 >> Training Loss: tensor(0.3109, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0550, dtype=torch.float64) <<\n",
      "tensor(0.3098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch225 >> Training Loss: tensor(0.3098, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0547, dtype=torch.float64) <<\n",
      "tensor(0.3113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch226 >> Training Loss: tensor(0.3113, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch227 >> Training Loss: tensor(0.3083, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0552, dtype=torch.float64) <<\n",
      "tensor(0.3096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch228 >> Training Loss: tensor(0.3096, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0545, dtype=torch.float64) <<\n",
      "tensor(0.3085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch229 >> Training Loss: tensor(0.3085, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0545, dtype=torch.float64) <<\n",
      "tensor(0.3073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch230 >> Training Loss: tensor(0.3073, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0550, dtype=torch.float64) <<\n",
      "tensor(0.3083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch231 >> Training Loss: tensor(0.3083, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0544, dtype=torch.float64) <<\n",
      "tensor(0.3063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch232 >> Training Loss: tensor(0.3063, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0543, dtype=torch.float64) <<\n",
      "tensor(0.3065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch233 >> Training Loss: tensor(0.3065, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0547, dtype=torch.float64) <<\n",
      "tensor(0.3063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch234 >> Training Loss: tensor(0.3063, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0543, dtype=torch.float64) <<\n",
      "tensor(0.3049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch235 >> Training Loss: tensor(0.3049, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0541, dtype=torch.float64) <<\n",
      "tensor(0.3053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch236 >> Training Loss: tensor(0.3053, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0544, dtype=torch.float64) <<\n",
      "tensor(0.3044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch237 >> Training Loss: tensor(0.3044, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n",
      "tensor(0.3036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch238 >> Training Loss: tensor(0.3036, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0540, dtype=torch.float64) <<\n",
      "tensor(0.3038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch239 >> Training Loss: tensor(0.3038, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n",
      "tensor(0.3027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch240 >> Training Loss: tensor(0.3027, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0541, dtype=torch.float64) <<\n",
      "tensor(0.3022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch241 >> Training Loss: tensor(0.3022, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0538, dtype=torch.float64) <<\n",
      "tensor(0.3021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch242 >> Training Loss: tensor(0.3021, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0539, dtype=torch.float64) <<\n",
      "tensor(0.3010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch243 >> Training Loss: tensor(0.3010, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0539, dtype=torch.float64) <<\n",
      "tensor(0.3006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch244 >> Training Loss: tensor(0.3006, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0536, dtype=torch.float64) <<\n",
      "tensor(0.3003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch245 >> Training Loss: tensor(0.3003, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0537, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch246 >> Training Loss: tensor(0.2993, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0536, dtype=torch.float64) <<\n",
      "tensor(0.2988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch247 >> Training Loss: tensor(0.2988, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0534, dtype=torch.float64) <<\n",
      "tensor(0.2984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch248 >> Training Loss: tensor(0.2984, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0535, dtype=torch.float64) <<\n",
      "tensor(0.2976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch249 >> Training Loss: tensor(0.2976, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0534, dtype=torch.float64) <<\n",
      "tensor(0.2968, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch250 >> Training Loss: tensor(0.2968, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0532, dtype=torch.float64) <<\n",
      "tensor(0.2964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch251 >> Training Loss: tensor(0.2964, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0533, dtype=torch.float64) <<\n",
      "tensor(0.2957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch252 >> Training Loss: tensor(0.2957, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0531, dtype=torch.float64) <<\n",
      "tensor(0.2948, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch253 >> Training Loss: tensor(0.2948, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0529, dtype=torch.float64) <<\n",
      "tensor(0.2941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch254 >> Training Loss: tensor(0.2941, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0530, dtype=torch.float64) <<\n",
      "tensor(0.2935, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch255 >> Training Loss: tensor(0.2935, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0527, dtype=torch.float64) <<\n",
      "tensor(0.2926, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch256 >> Training Loss: tensor(0.2926, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0527, dtype=torch.float64) <<\n",
      "tensor(0.2917, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch257 >> Training Loss: tensor(0.2917, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0527, dtype=torch.float64) <<\n",
      "tensor(0.2909, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch258 >> Training Loss: tensor(0.2909, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0524, dtype=torch.float64) <<\n",
      "tensor(0.2902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch259 >> Training Loss: tensor(0.2902, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0525, dtype=torch.float64) <<\n",
      "tensor(0.2893, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch260 >> Training Loss: tensor(0.2893, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0522, dtype=torch.float64) <<\n",
      "tensor(0.2883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch261 >> Training Loss: tensor(0.2883, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0521, dtype=torch.float64) <<\n",
      "tensor(0.2873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch262 >> Training Loss: tensor(0.2873, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0520, dtype=torch.float64) <<\n",
      "tensor(0.2863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch263 >> Training Loss: tensor(0.2863, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0518, dtype=torch.float64) <<\n",
      "tensor(0.2854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch264 >> Training Loss: tensor(0.2854, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0518, dtype=torch.float64) <<\n",
      "tensor(0.2844, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch265 >> Training Loss: tensor(0.2844, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0515, dtype=torch.float64) <<\n",
      "tensor(0.2834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch266 >> Training Loss: tensor(0.2834, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0516, dtype=torch.float64) <<\n",
      "tensor(0.2824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch267 >> Training Loss: tensor(0.2824, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0511, dtype=torch.float64) <<\n",
      "tensor(0.2815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch268 >> Training Loss: tensor(0.2815, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0515, dtype=torch.float64) <<\n",
      "tensor(0.2810, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch269 >> Training Loss: tensor(0.2810, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0508, dtype=torch.float64) <<\n",
      "tensor(0.2819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch270 >> Training Loss: tensor(0.2819, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0528, dtype=torch.float64) <<\n",
      "tensor(0.2879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch271 >> Training Loss: tensor(0.2879, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0535, dtype=torch.float64) <<\n",
      "tensor(0.3141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch272 >> Training Loss: tensor(0.3141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0723, dtype=torch.float64) <<\n",
      "tensor(0.4296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch273 >> Training Loss: tensor(0.4296, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1193, dtype=torch.float64) <<\n",
      "tensor(0.9051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch274 >> Training Loss: tensor(0.9051, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3208, dtype=torch.float64) <<\n",
      "tensor(2.4189, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch275 >> Training Loss: tensor(2.4189, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3215, dtype=torch.float64) <<\n",
      "tensor(2.6314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch276 >> Training Loss: tensor(2.6314, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0621, dtype=torch.float64) <<\n",
      "tensor(0.3547, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch277 >> Training Loss: tensor(0.3547, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2046, dtype=torch.float64) <<\n",
      "tensor(1.4748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch278 >> Training Loss: tensor(1.4748, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0606, dtype=torch.float64) <<\n",
      "tensor(0.3689, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch279 >> Training Loss: tensor(0.3689, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1461, dtype=torch.float64) <<\n",
      "tensor(1.1439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch280 >> Training Loss: tensor(1.1439, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0804, dtype=torch.float64) <<\n",
      "tensor(0.5653, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch281 >> Training Loss: tensor(0.5653, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0916, dtype=torch.float64) <<\n",
      "tensor(0.6050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch282 >> Training Loss: tensor(0.6050, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1244, dtype=torch.float64) <<\n",
      "tensor(0.8687, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch283 >> Training Loss: tensor(0.8687, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0679, dtype=torch.float64) <<\n",
      "tensor(0.4255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch284 >> Training Loss: tensor(0.4255, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0816, dtype=torch.float64) <<\n",
      "tensor(0.5552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch285 >> Training Loss: tensor(0.5552, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0994, dtype=torch.float64) <<\n",
      "tensor(0.6850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch286 >> Training Loss: tensor(0.6850, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0666, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch287 >> Training Loss: tensor(0.3847, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0878, dtype=torch.float64) <<\n",
      "tensor(0.5434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch288 >> Training Loss: tensor(0.5434, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0923, dtype=torch.float64) <<\n",
      "tensor(0.5812, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch289 >> Training Loss: tensor(0.5812, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0654, dtype=torch.float64) <<\n",
      "tensor(0.3713, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch290 >> Training Loss: tensor(0.3713, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0773, dtype=torch.float64) <<\n",
      "tensor(0.4871, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch291 >> Training Loss: tensor(0.4871, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0800, dtype=torch.float64) <<\n",
      "tensor(0.5203, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch292 >> Training Loss: tensor(0.5203, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0636, dtype=torch.float64) <<\n",
      "tensor(0.3817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch293 >> Training Loss: tensor(0.3817, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0708, dtype=torch.float64) <<\n",
      "tensor(0.4321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch294 >> Training Loss: tensor(0.4321, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0766, dtype=torch.float64) <<\n",
      "tensor(0.4767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch295 >> Training Loss: tensor(0.4767, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0634, dtype=torch.float64) <<\n",
      "tensor(0.3718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch296 >> Training Loss: tensor(0.3718, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0638, dtype=torch.float64) <<\n",
      "tensor(0.3812, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch297 >> Training Loss: tensor(0.3812, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0705, dtype=torch.float64) <<\n",
      "tensor(0.4397, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch298 >> Training Loss: tensor(0.4397, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0620, dtype=torch.float64) <<\n",
      "tensor(0.3670, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch299 >> Training Loss: tensor(0.3670, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0613, dtype=torch.float64) <<\n",
      "tensor(0.3554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch300 >> Training Loss: tensor(0.3554, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0670, dtype=torch.float64) <<\n",
      "tensor(0.4015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch301 >> Training Loss: tensor(0.4015, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0611, dtype=torch.float64) <<\n",
      "tensor(0.3592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch302 >> Training Loss: tensor(0.3592, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0575, dtype=torch.float64) <<\n",
      "tensor(0.3410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch303 >> Training Loss: tensor(0.3410, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0611, dtype=torch.float64) <<\n",
      "tensor(0.3779, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch304 >> Training Loss: tensor(0.3779, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0586, dtype=torch.float64) <<\n",
      "tensor(0.3545, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch305 >> Training Loss: tensor(0.3545, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0569, dtype=torch.float64) <<\n",
      "tensor(0.3296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch306 >> Training Loss: tensor(0.3296, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0613, dtype=torch.float64) <<\n",
      "tensor(0.3577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch307 >> Training Loss: tensor(0.3577, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0603, dtype=torch.float64) <<\n",
      "tensor(0.3503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch308 >> Training Loss: tensor(0.3503, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0567, dtype=torch.float64) <<\n",
      "tensor(0.3285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch309 >> Training Loss: tensor(0.3285, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0578, dtype=torch.float64) <<\n",
      "tensor(0.3466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch310 >> Training Loss: tensor(0.3466, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0572, dtype=torch.float64) <<\n",
      "tensor(0.3430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch311 >> Training Loss: tensor(0.3430, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0560, dtype=torch.float64) <<\n",
      "tensor(0.3261, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch312 >> Training Loss: tensor(0.3261, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0584, dtype=torch.float64) <<\n",
      "tensor(0.3395, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch313 >> Training Loss: tensor(0.3395, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0587, dtype=torch.float64) <<\n",
      "tensor(0.3404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch314 >> Training Loss: tensor(0.3404, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0562, dtype=torch.float64) <<\n",
      "tensor(0.3257, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch315 >> Training Loss: tensor(0.3257, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0564, dtype=torch.float64) <<\n",
      "tensor(0.3328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch316 >> Training Loss: tensor(0.3328, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0567, dtype=torch.float64) <<\n",
      "tensor(0.3354, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch317 >> Training Loss: tensor(0.3354, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0562, dtype=torch.float64) <<\n",
      "tensor(0.3249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch318 >> Training Loss: tensor(0.3249, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0574, dtype=torch.float64) <<\n",
      "tensor(0.3298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch319 >> Training Loss: tensor(0.3298, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0576, dtype=torch.float64) <<\n",
      "tensor(0.3313, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch320 >> Training Loss: tensor(0.3313, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0560, dtype=torch.float64) <<\n",
      "tensor(0.3228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch321 >> Training Loss: tensor(0.3228, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0557, dtype=torch.float64) <<\n",
      "tensor(0.3260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch322 >> Training Loss: tensor(0.3260, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0558, dtype=torch.float64) <<\n",
      "tensor(0.3279, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch323 >> Training Loss: tensor(0.3279, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0555, dtype=torch.float64) <<\n",
      "tensor(0.3215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch324 >> Training Loss: tensor(0.3215, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0563, dtype=torch.float64) <<\n",
      "tensor(0.3229, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch325 >> Training Loss: tensor(0.3229, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch326 >> Training Loss: tensor(0.3242, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0558, dtype=torch.float64) <<\n",
      "tensor(0.3195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch327 >> Training Loss: tensor(0.3195, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0555, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch328 >> Training Loss: tensor(0.3206, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0555, dtype=torch.float64) <<\n",
      "tensor(0.3215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch329 >> Training Loss: tensor(0.3215, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0553, dtype=torch.float64) <<\n",
      "tensor(0.3178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch330 >> Training Loss: tensor(0.3178, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0557, dtype=torch.float64) <<\n",
      "tensor(0.3183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch331 >> Training Loss: tensor(0.3183, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0558, dtype=torch.float64) <<\n",
      "tensor(0.3191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch332 >> Training Loss: tensor(0.3191, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0552, dtype=torch.float64) <<\n",
      "tensor(0.3164, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch333 >> Training Loss: tensor(0.3164, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0550, dtype=torch.float64) <<\n",
      "tensor(0.3167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch334 >> Training Loss: tensor(0.3167, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0550, dtype=torch.float64) <<\n",
      "tensor(0.3171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch335 >> Training Loss: tensor(0.3171, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0551, dtype=torch.float64) <<\n",
      "tensor(0.3150, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch336 >> Training Loss: tensor(0.3150, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0555, dtype=torch.float64) <<\n",
      "tensor(0.3153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch337 >> Training Loss: tensor(0.3153, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0556, dtype=torch.float64) <<\n",
      "tensor(0.3156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch338 >> Training Loss: tensor(0.3156, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0552, dtype=torch.float64) <<\n",
      "tensor(0.3139, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch339 >> Training Loss: tensor(0.3139, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0549, dtype=torch.float64) <<\n",
      "tensor(0.3141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch340 >> Training Loss: tensor(0.3141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0549, dtype=torch.float64) <<\n",
      "tensor(0.3141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch341 >> Training Loss: tensor(0.3141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0549, dtype=torch.float64) <<\n",
      "tensor(0.3130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch342 >> Training Loss: tensor(0.3130, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0551, dtype=torch.float64) <<\n",
      "tensor(0.3131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch343 >> Training Loss: tensor(0.3131, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0551, dtype=torch.float64) <<\n",
      "tensor(0.3129, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch344 >> Training Loss: tensor(0.3129, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0549, dtype=torch.float64) <<\n",
      "tensor(0.3120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch345 >> Training Loss: tensor(0.3120, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch346 >> Training Loss: tensor(0.3120, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch347 >> Training Loss: tensor(0.3119, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0549, dtype=torch.float64) <<\n",
      "tensor(0.3111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch348 >> Training Loss: tensor(0.3111, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0550, dtype=torch.float64) <<\n",
      "tensor(0.3111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch349 >> Training Loss: tensor(0.3111, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0549, dtype=torch.float64) <<\n",
      "tensor(0.3108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch350 >> Training Loss: tensor(0.3108, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0547, dtype=torch.float64) <<\n",
      "tensor(0.3102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch351 >> Training Loss: tensor(0.3102, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0546, dtype=torch.float64) <<\n",
      "tensor(0.3102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch352 >> Training Loss: tensor(0.3102, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0546, dtype=torch.float64) <<\n",
      "tensor(0.3099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch353 >> Training Loss: tensor(0.3099, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0547, dtype=torch.float64) <<\n",
      "tensor(0.3094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch354 >> Training Loss: tensor(0.3094, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch355 >> Training Loss: tensor(0.3094, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0547, dtype=torch.float64) <<\n",
      "tensor(0.3090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch356 >> Training Loss: tensor(0.3090, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0546, dtype=torch.float64) <<\n",
      "tensor(0.3087, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch357 >> Training Loss: tensor(0.3087, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0545, dtype=torch.float64) <<\n",
      "tensor(0.3086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch358 >> Training Loss: tensor(0.3086, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0545, dtype=torch.float64) <<\n",
      "tensor(0.3082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch359 >> Training Loss: tensor(0.3082, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0545, dtype=torch.float64) <<\n",
      "tensor(0.3079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch360 >> Training Loss: tensor(0.3079, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0546, dtype=torch.float64) <<\n",
      "tensor(0.3078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch361 >> Training Loss: tensor(0.3078, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0545, dtype=torch.float64) <<\n",
      "tensor(0.3075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch362 >> Training Loss: tensor(0.3075, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0544, dtype=torch.float64) <<\n",
      "tensor(0.3072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch363 >> Training Loss: tensor(0.3072, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0543, dtype=torch.float64) <<\n",
      "tensor(0.3070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch364 >> Training Loss: tensor(0.3070, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0544, dtype=torch.float64) <<\n",
      "tensor(0.3067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch365 >> Training Loss: tensor(0.3067, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0544, dtype=torch.float64) <<\n",
      "tensor(0.3065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch366 >> Training Loss: tensor(0.3065, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0544, dtype=torch.float64) <<\n",
      "tensor(0.3063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch367 >> Training Loss: tensor(0.3063, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0543, dtype=torch.float64) <<\n",
      "tensor(0.3060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch368 >> Training Loss: tensor(0.3060, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch369 >> Training Loss: tensor(0.3058, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n",
      "tensor(0.3055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch370 >> Training Loss: tensor(0.3055, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n",
      "tensor(0.3052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch371 >> Training Loss: tensor(0.3052, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n",
      "tensor(0.3050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch372 >> Training Loss: tensor(0.3050, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n",
      "tensor(0.3047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch373 >> Training Loss: tensor(0.3047, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0541, dtype=torch.float64) <<\n",
      "tensor(0.3045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch374 >> Training Loss: tensor(0.3045, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0541, dtype=torch.float64) <<\n",
      "tensor(0.3042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch375 >> Training Loss: tensor(0.3042, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0541, dtype=torch.float64) <<\n",
      "tensor(0.3039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch376 >> Training Loss: tensor(0.3039, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0541, dtype=torch.float64) <<\n",
      "tensor(0.3037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch377 >> Training Loss: tensor(0.3037, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0540, dtype=torch.float64) <<\n",
      "tensor(0.3034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch378 >> Training Loss: tensor(0.3034, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0540, dtype=torch.float64) <<\n",
      "tensor(0.3031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch379 >> Training Loss: tensor(0.3031, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0539, dtype=torch.float64) <<\n",
      "tensor(0.3029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch380 >> Training Loss: tensor(0.3029, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0539, dtype=torch.float64) <<\n",
      "tensor(0.3026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch381 >> Training Loss: tensor(0.3026, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0539, dtype=torch.float64) <<\n",
      "tensor(0.3023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch382 >> Training Loss: tensor(0.3023, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0539, dtype=torch.float64) <<\n",
      "tensor(0.3020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch383 >> Training Loss: tensor(0.3020, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0538, dtype=torch.float64) <<\n",
      "tensor(0.3017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch384 >> Training Loss: tensor(0.3017, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0538, dtype=torch.float64) <<\n",
      "tensor(0.3014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch385 >> Training Loss: tensor(0.3014, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0537, dtype=torch.float64) <<\n",
      "tensor(0.3011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch386 >> Training Loss: tensor(0.3011, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0537, dtype=torch.float64) <<\n",
      "tensor(0.3008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch387 >> Training Loss: tensor(0.3008, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0537, dtype=torch.float64) <<\n",
      "tensor(0.3005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch388 >> Training Loss: tensor(0.3005, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0536, dtype=torch.float64) <<\n",
      "tensor(0.3002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch389 >> Training Loss: tensor(0.3002, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0536, dtype=torch.float64) <<\n",
      "tensor(0.2999, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch390 >> Training Loss: tensor(0.2999, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0535, dtype=torch.float64) <<\n",
      "tensor(0.2995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch391 >> Training Loss: tensor(0.2995, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0535, dtype=torch.float64) <<\n",
      "tensor(0.2992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch392 >> Training Loss: tensor(0.2992, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0535, dtype=torch.float64) <<\n",
      "tensor(0.2988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch393 >> Training Loss: tensor(0.2988, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0534, dtype=torch.float64) <<\n",
      "tensor(0.2985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch394 >> Training Loss: tensor(0.2985, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0534, dtype=torch.float64) <<\n",
      "tensor(0.2981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch395 >> Training Loss: tensor(0.2981, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0533, dtype=torch.float64) <<\n",
      "tensor(0.2977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch396 >> Training Loss: tensor(0.2977, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0533, dtype=torch.float64) <<\n",
      "tensor(0.2974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch397 >> Training Loss: tensor(0.2974, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0532, dtype=torch.float64) <<\n",
      "tensor(0.2970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch398 >> Training Loss: tensor(0.2970, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0532, dtype=torch.float64) <<\n",
      "tensor(0.2966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch399 >> Training Loss: tensor(0.2966, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0531, dtype=torch.float64) <<\n",
      "tensor(0.2961, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch400 >> Training Loss: tensor(0.2961, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0530, dtype=torch.float64) <<\n",
      "tensor(0.2957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch401 >> Training Loss: tensor(0.2957, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0530, dtype=torch.float64) <<\n",
      "tensor(0.2953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch402 >> Training Loss: tensor(0.2953, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0529, dtype=torch.float64) <<\n",
      "tensor(0.2948, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch403 >> Training Loss: tensor(0.2948, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0529, dtype=torch.float64) <<\n",
      "tensor(0.2943, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch404 >> Training Loss: tensor(0.2943, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0528, dtype=torch.float64) <<\n",
      "tensor(0.2939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch405 >> Training Loss: tensor(0.2939, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0527, dtype=torch.float64) <<\n",
      "tensor(0.2934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch406 >> Training Loss: tensor(0.2934, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0527, dtype=torch.float64) <<\n",
      "tensor(0.2928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch407 >> Training Loss: tensor(0.2928, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0526, dtype=torch.float64) <<\n",
      "tensor(0.2923, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch408 >> Training Loss: tensor(0.2923, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0525, dtype=torch.float64) <<\n",
      "tensor(0.2918, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch409 >> Training Loss: tensor(0.2918, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0525, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2912, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch410 >> Training Loss: tensor(0.2912, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0524, dtype=torch.float64) <<\n",
      "tensor(0.2906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch411 >> Training Loss: tensor(0.2906, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0523, dtype=torch.float64) <<\n",
      "tensor(0.2900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch412 >> Training Loss: tensor(0.2900, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0523, dtype=torch.float64) <<\n",
      "tensor(0.2894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch413 >> Training Loss: tensor(0.2894, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0522, dtype=torch.float64) <<\n",
      "tensor(0.2887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch414 >> Training Loss: tensor(0.2887, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0521, dtype=torch.float64) <<\n",
      "tensor(0.2880, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch415 >> Training Loss: tensor(0.2880, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0520, dtype=torch.float64) <<\n",
      "tensor(0.2873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch416 >> Training Loss: tensor(0.2873, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0519, dtype=torch.float64) <<\n",
      "tensor(0.2866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch417 >> Training Loss: tensor(0.2866, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0518, dtype=torch.float64) <<\n",
      "tensor(0.2859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch418 >> Training Loss: tensor(0.2859, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0517, dtype=torch.float64) <<\n",
      "tensor(0.2851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch419 >> Training Loss: tensor(0.2851, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0516, dtype=torch.float64) <<\n",
      "tensor(0.2843, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch420 >> Training Loss: tensor(0.2843, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0515, dtype=torch.float64) <<\n",
      "tensor(0.2834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch421 >> Training Loss: tensor(0.2834, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0514, dtype=torch.float64) <<\n",
      "tensor(0.2826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch422 >> Training Loss: tensor(0.2826, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0513, dtype=torch.float64) <<\n",
      "tensor(0.2817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch423 >> Training Loss: tensor(0.2817, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0512, dtype=torch.float64) <<\n",
      "tensor(0.2808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch424 >> Training Loss: tensor(0.2808, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0511, dtype=torch.float64) <<\n",
      "tensor(0.2798, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch425 >> Training Loss: tensor(0.2798, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0510, dtype=torch.float64) <<\n",
      "tensor(0.2789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch426 >> Training Loss: tensor(0.2789, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0509, dtype=torch.float64) <<\n",
      "tensor(0.2779, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch427 >> Training Loss: tensor(0.2779, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0508, dtype=torch.float64) <<\n",
      "tensor(0.2769, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch428 >> Training Loss: tensor(0.2769, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0507, dtype=torch.float64) <<\n",
      "tensor(0.2759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch429 >> Training Loss: tensor(0.2759, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0506, dtype=torch.float64) <<\n",
      "tensor(0.2748, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch430 >> Training Loss: tensor(0.2748, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0505, dtype=torch.float64) <<\n",
      "tensor(0.2738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch431 >> Training Loss: tensor(0.2738, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0504, dtype=torch.float64) <<\n",
      "tensor(0.2727, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch432 >> Training Loss: tensor(0.2727, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0503, dtype=torch.float64) <<\n",
      "tensor(0.2717, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch433 >> Training Loss: tensor(0.2717, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0501, dtype=torch.float64) <<\n",
      "tensor(0.2706, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch434 >> Training Loss: tensor(0.2706, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0501, dtype=torch.float64) <<\n",
      "tensor(0.2695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch435 >> Training Loss: tensor(0.2695, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0499, dtype=torch.float64) <<\n",
      "tensor(0.2684, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch436 >> Training Loss: tensor(0.2684, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0499, dtype=torch.float64) <<\n",
      "tensor(0.2672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch437 >> Training Loss: tensor(0.2672, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0496, dtype=torch.float64) <<\n",
      "tensor(0.2661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch438 >> Training Loss: tensor(0.2661, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0500, dtype=torch.float64) <<\n",
      "tensor(0.2656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch439 >> Training Loss: tensor(0.2656, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0494, dtype=torch.float64) <<\n",
      "tensor(0.2739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch440 >> Training Loss: tensor(0.2739, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0688, dtype=torch.float64) <<\n",
      "tensor(0.3916, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch441 >> Training Loss: tensor(0.3916, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1634, dtype=torch.float64) <<\n",
      "tensor(1.3266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch442 >> Training Loss: tensor(1.3266, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.3579, dtype=torch.float64) <<\n",
      "tensor(2.7283, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch443 >> Training Loss: tensor(2.7283, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0899, dtype=torch.float64) <<\n",
      "tensor(0.5808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch444 >> Training Loss: tensor(0.5808, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1420, dtype=torch.float64) <<\n",
      "tensor(1.1109, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch445 >> Training Loss: tensor(1.1109, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.2183, dtype=torch.float64) <<\n",
      "tensor(1.7771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch446 >> Training Loss: tensor(1.7771, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1432, dtype=torch.float64) <<\n",
      "tensor(1.1044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch447 >> Training Loss: tensor(1.1044, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1009, dtype=torch.float64) <<\n",
      "tensor(0.6950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch448 >> Training Loss: tensor(0.6950, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1394, dtype=torch.float64) <<\n",
      "tensor(0.9911, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch449 >> Training Loss: tensor(0.9911, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1231, dtype=torch.float64) <<\n",
      "tensor(0.8356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch450 >> Training Loss: tensor(0.8356, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0933, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch451 >> Training Loss: tensor(0.5295, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1612, dtype=torch.float64) <<\n",
      "tensor(1.0626, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch452 >> Training Loss: tensor(1.0626, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0908, dtype=torch.float64) <<\n",
      "tensor(0.5507, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch453 >> Training Loss: tensor(0.5507, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0825, dtype=torch.float64) <<\n",
      "tensor(0.5135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch454 >> Training Loss: tensor(0.5135, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.1224, dtype=torch.float64) <<\n",
      "tensor(0.8336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch455 >> Training Loss: tensor(0.8336, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0987, dtype=torch.float64) <<\n",
      "tensor(0.6584, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch456 >> Training Loss: tensor(0.6584, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0654, dtype=torch.float64) <<\n",
      "tensor(0.4237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch457 >> Training Loss: tensor(0.4237, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0871, dtype=torch.float64) <<\n",
      "tensor(0.6074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch458 >> Training Loss: tensor(0.6074, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0902, dtype=torch.float64) <<\n",
      "tensor(0.6058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch459 >> Training Loss: tensor(0.6058, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0687, dtype=torch.float64) <<\n",
      "tensor(0.4105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch460 >> Training Loss: tensor(0.4105, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0852, dtype=torch.float64) <<\n",
      "tensor(0.5414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch461 >> Training Loss: tensor(0.5414, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0836, dtype=torch.float64) <<\n",
      "tensor(0.5291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch462 >> Training Loss: tensor(0.5291, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0635, dtype=torch.float64) <<\n",
      "tensor(0.3728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch463 >> Training Loss: tensor(0.3728, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0717, dtype=torch.float64) <<\n",
      "tensor(0.4556, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch464 >> Training Loss: tensor(0.4556, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0765, dtype=torch.float64) <<\n",
      "tensor(0.5054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch465 >> Training Loss: tensor(0.5054, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0625, dtype=torch.float64) <<\n",
      "tensor(0.3850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch466 >> Training Loss: tensor(0.3850, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0651, dtype=torch.float64) <<\n",
      "tensor(0.3939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch467 >> Training Loss: tensor(0.3939, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0725, dtype=torch.float64) <<\n",
      "tensor(0.4491, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch468 >> Training Loss: tensor(0.4491, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0644, dtype=torch.float64) <<\n",
      "tensor(0.3859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch469 >> Training Loss: tensor(0.3859, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0606, dtype=torch.float64) <<\n",
      "tensor(0.3607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch470 >> Training Loss: tensor(0.3607, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0663, dtype=torch.float64) <<\n",
      "tensor(0.4141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch471 >> Training Loss: tensor(0.4141, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0607, dtype=torch.float64) <<\n",
      "tensor(0.3698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch472 >> Training Loss: tensor(0.3698, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0589, dtype=torch.float64) <<\n",
      "tensor(0.3479, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch473 >> Training Loss: tensor(0.3479, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0646, dtype=torch.float64) <<\n",
      "tensor(0.3869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch474 >> Training Loss: tensor(0.3869, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0617, dtype=torch.float64) <<\n",
      "tensor(0.3645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch475 >> Training Loss: tensor(0.3645, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3315, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch476 >> Training Loss: tensor(0.3315, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0597, dtype=torch.float64) <<\n",
      "tensor(0.3622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch477 >> Training Loss: tensor(0.3622, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0595, dtype=torch.float64) <<\n",
      "tensor(0.3590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch478 >> Training Loss: tensor(0.3590, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0566, dtype=torch.float64) <<\n",
      "tensor(0.3285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch479 >> Training Loss: tensor(0.3285, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0591, dtype=torch.float64) <<\n",
      "tensor(0.3442, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch480 >> Training Loss: tensor(0.3442, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0591, dtype=torch.float64) <<\n",
      "tensor(0.3451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch481 >> Training Loss: tensor(0.3451, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0560, dtype=torch.float64) <<\n",
      "tensor(0.3264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch482 >> Training Loss: tensor(0.3264, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0564, dtype=torch.float64) <<\n",
      "tensor(0.3373, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch483 >> Training Loss: tensor(0.3373, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0564, dtype=torch.float64) <<\n",
      "tensor(0.3384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch484 >> Training Loss: tensor(0.3384, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3199, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch485 >> Training Loss: tensor(0.3199, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0565, dtype=torch.float64) <<\n",
      "tensor(0.3267, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch486 >> Training Loss: tensor(0.3267, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0577, dtype=torch.float64) <<\n",
      "tensor(0.3335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch487 >> Training Loss: tensor(0.3335, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0558, dtype=torch.float64) <<\n",
      "tensor(0.3209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch488 >> Training Loss: tensor(0.3209, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0552, dtype=torch.float64) <<\n",
      "tensor(0.3209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch489 >> Training Loss: tensor(0.3209, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0555, dtype=torch.float64) <<\n",
      "tensor(0.3250, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch490 >> Training Loss: tensor(0.3250, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0549, dtype=torch.float64) <<\n",
      "tensor(0.3171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch491 >> Training Loss: tensor(0.3171, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0556, dtype=torch.float64) <<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch492 >> Training Loss: tensor(0.3188, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0561, dtype=torch.float64) <<\n",
      "tensor(0.3221, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch493 >> Training Loss: tensor(0.3221, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0548, dtype=torch.float64) <<\n",
      "tensor(0.3144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch494 >> Training Loss: tensor(0.3144, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0542, dtype=torch.float64) <<\n",
      "tensor(0.3136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch495 >> Training Loss: tensor(0.3136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0546, dtype=torch.float64) <<\n",
      "tensor(0.3178, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch496 >> Training Loss: tensor(0.3178, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0544, dtype=torch.float64) <<\n",
      "tensor(0.3136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch497 >> Training Loss: tensor(0.3136, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0547, dtype=torch.float64) <<\n",
      "tensor(0.3122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch498 >> Training Loss: tensor(0.3122, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0551, dtype=torch.float64) <<\n",
      "tensor(0.3138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Epoch499 >> Training Loss: tensor(0.3138, dtype=torch.float64, grad_fn=<AddBackward0>) Validation Loss: tensor(0.0545, dtype=torch.float64) <<\n"
     ]
    }
   ],
   "source": [
    "#Do separately Val loss and train loss\n",
    "TR_LOSS = []\n",
    "VAL_LOSS = []\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    net.reset_hidden_states(bs=batch_size)\n",
    "    #print(net.hidden[0].shape)\n",
    "    it = iter(data_loaded.train_dataloader)\n",
    "    it_val = iter(data_loaded.train_dataloader)\n",
    "    c = 0\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    while c < int(X_train.shape[0]/batch_size): \n",
    "        with torch.set_grad_enabled(True):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            tr_loss = tr_loss + criterion(outputs, labels)\n",
    "    \n",
    "    TR_LOSS.append(tr_loss)\n",
    "    print(tr_loss)\n",
    "    optimizer.zero_grad()\n",
    "    tr_loss.backward()\n",
    "    optimizer.step()\n",
    "    c = 0\n",
    "    \n",
    "    #net.reset_hidden_states(bs=bs_val)\n",
    "    #print(net.hidden[0].shape)\n",
    "    #print(net.hidden[0].shape)\n",
    "    while c < int(X_val.shape[0]/bs_val):\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #print(c)\n",
    "            c = c+1\n",
    "            item = next(it_val)\n",
    "            inputs = item[0].to(device)\n",
    "            labels = item[1].to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            val_loss = val_loss + criterion(outputs, labels)\n",
    "            #print(val_loss)\n",
    "    VAL_LOSS.append(val_loss)\n",
    "    print('Epoch'+str(i)+ ' >> Training Loss: '+str(tr_loss)+ ' Validation Loss: '+str(val_loss)+ ' <<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5dd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOSS = []\n",
    "for t in TR_LOSS:\n",
    "    cc = t.detach().numpy()\n",
    "    TRT_LOSS.append(cc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a52f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5178ff5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(TRT_LOSS)\n",
    "plt.plot(VAL_LOSS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5625396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0054, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "accumulated_test_loss = 0.0\n",
    "net.reset_hidden_states(bs=1)\n",
    "it_test = iter(data_loaded.test_dataloader)\n",
    "c=0\n",
    "while c < int(X_test.shape[0]/1):\n",
    "    #net.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        #print(c)\n",
    "        c = c+1\n",
    "        item = next(it_test)\n",
    "        outputs = net(item[0])\n",
    "        accumulated_test_loss = accumulated_test_loss + criterion(outputs, item[1])        \n",
    "print(accumulated_test_loss/int(len(data_loaded.test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f01f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "794fb47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.mkdir('RESULTS_LOSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b881426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_array = []\n",
    "for z in TR_LOSS:\n",
    "    tr_loss_array.append(z.tolist())\n",
    "training_loss = pd.DataFrame(tr_loss_array, columns=['TR_LOSS'])\n",
    "training_loss.to_csv('../RESULTS_LOSS/TR_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca05f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_array = []\n",
    "for z in VAL_LOSS:\n",
    "    val_loss_array.append(z.tolist())\n",
    "val_loss = pd.DataFrame(val_loss_array, columns=['VAL_LOSS'])\n",
    "val_loss.to_csv('../RESULTS_LOSS/VAL_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2b97b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_array = [accumulated_test_loss]\n",
    "\n",
    "test2_loss = pd.DataFrame(test_loss_array, columns=['TEST_LOSS'])\n",
    "test2_loss.to_csv('../RESULTS_LOSS/TEST_LOSS_'+str(output_cardinality)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95819c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f70b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../SCALER_DUMPS/min_max_scaler_lookahead8.save']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVE THE SCALER\n",
    "import joblib\n",
    "scaler_filename = \"../SCALER_DUMPS/min_max_scaler_lookahead\"+str(output_cardinality)+\".save\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "# And now to load...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f65d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e0f979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE MODEL\n",
    "model_ckp_path = \"../MODEL_CHECKPOINTS/model_lookahead\"+str(output_cardinality)+\".pth\" \n",
    "torch.save(net.state_dict(), model_ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268595d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c07726f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net2 = LSTMNET(batch_size=batch_size,input_len=input_cardinality,output_len=output_cardinality)\n",
    "#net2.load_state_dict(torch.load(model_ckp_path))\n",
    "#net2.eval()\n",
    "#scaler = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab93fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b98c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
